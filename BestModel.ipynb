{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malayshikhar/anaconda3/envs/TDL/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5290 - loss: 0.8066 - val_accuracy: 0.6828 - val_loss: 0.6343\n",
      "Epoch 2/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7122 - loss: 0.5877 - val_accuracy: 0.7950 - val_loss: 0.4881\n",
      "Epoch 3/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7898 - loss: 0.4979 - val_accuracy: 0.8039 - val_loss: 0.4604\n",
      "Epoch 4/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8008 - loss: 0.4742 - val_accuracy: 0.7706 - val_loss: 0.4951\n",
      "Epoch 5/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8118 - loss: 0.4565 - val_accuracy: 0.7905 - val_loss: 0.4686\n",
      "Epoch 6/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8084 - loss: 0.4584 - val_accuracy: 0.8106 - val_loss: 0.4474\n",
      "Epoch 7/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8180 - loss: 0.4378 - val_accuracy: 0.8101 - val_loss: 0.4575\n",
      "Epoch 8/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8184 - loss: 0.4293 - val_accuracy: 0.7936 - val_loss: 0.4618\n",
      "Epoch 9/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8234 - loss: 0.4229 - val_accuracy: 0.8006 - val_loss: 0.4414\n",
      "Epoch 10/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8255 - loss: 0.4183 - val_accuracy: 0.7915 - val_loss: 0.4670\n",
      "Epoch 11/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8283 - loss: 0.4101 - val_accuracy: 0.8066 - val_loss: 0.4384\n",
      "Epoch 12/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8283 - loss: 0.4084 - val_accuracy: 0.8064 - val_loss: 0.4484\n",
      "Epoch 13/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8318 - loss: 0.4032 - val_accuracy: 0.8139 - val_loss: 0.4451\n",
      "Epoch 14/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8352 - loss: 0.3940 - val_accuracy: 0.8118 - val_loss: 0.4408\n",
      "Epoch 15/15\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8420 - loss: 0.3920 - val_accuracy: 0.8145 - val_loss: 0.4379\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.8122 - loss: 0.4453\n",
      "Test Loss: 0.43740859627723694, Test Accuracy: 0.8144927620887756\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"amazon_reviews_features.txt\", sep=\"\\t\")\n",
    "\n",
    "# Preprocessing\n",
    "# Combine REVIEW_TITLE and REVIEW_TEXT into a single column\n",
    "df['REVIEW'] = df['REVIEW_TITLE'] + ' ' + df['REVIEW_TEXT']\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove special characters and punctuation\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to REVIEW_TEXT column\n",
    "df['REVIEW_TEXT'] = df['REVIEW_TEXT'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['REVIEW'])\n",
    "tfidf_weights = tfidf_matrix.toarray()\n",
    "\n",
    "# Select top 2000 n-grams based on their tf-idf scores\n",
    "total_tfidf_scores = np.sum(tfidf_weights, axis=0)\n",
    "top_indices = total_tfidf_scores.argsort()[-2000:][::-1]\n",
    "top_ngrams = [tfidf_vectorizer.get_feature_names_out()[i] for i in top_indices]\n",
    "\n",
    "# Dimensionality reduction for n-grams\n",
    "X_ngrams_selected = SelectKBest(score_func=f_classif, k=2000).fit_transform(tfidf_weights, df['TARGET'])\n",
    "X_ngrams_lsa = TruncatedSVD(n_components=1500, random_state=42).fit_transform(X_ngrams_selected)\n",
    "\n",
    "\n",
    "\n",
    "# Word Embeddings\n",
    "content_review_tokens = [text.split() for text in df['REVIEW']]\n",
    "skipgram_model = Word2Vec(sentences=content_review_tokens, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Average Word Embedding\n",
    "avg_embedding_reviews = []\n",
    "for tokens in content_review_tokens:\n",
    "    embeddings = [skipgram_model.wv[word] for word in tokens if word in skipgram_model.wv]\n",
    "    avg_embedding = np.mean(embeddings, axis=0) if embeddings else np.zeros(100)\n",
    "    avg_embedding_reviews.append(avg_embedding)\n",
    "\n",
    "# Emotion Features\n",
    "emotion_X = df[['VERIFIED_PURCHASE', 'OPI_FIN_POS', 'OPI_FIN_NEG', 'BL_POS', 'BL_NEG', 'AFINN_POS',\n",
    "                'AFINN_NEG', 'S140_POS', 'S140_NEG', 'SWN_POS', 'SWN_NEG',\n",
    "                'NRC_HASH_POS', 'NRC_HASH_NEG', 'EMOTICON_POS', 'EMOTICON_NEG',\n",
    "                'NRC_ANGER', 'NRC_ANTICIPATION', 'NRC_DISGUST', 'NRC_FEAR', 'NRC_JOY',\n",
    "                'NRC_SADNESS', 'NRC_SURPRISE', 'NRC_TRUST', 'NRC_EXP_ANGER',\n",
    "                'NRC_EXP_ANTICIPATION', 'NRC_EXP_DISGUST', 'NRC_EXP_FEAR',\n",
    "                'NRC_EXP_JOY', 'NRC_EXP_SADNESS', 'NRC_EXP_SURPRISE', 'NRC_EXP_TRUST']].values.tolist()\n",
    "\n",
    "# Combine features\n",
    "X = np.concatenate((X_ngrams_lsa, avg_embedding_reviews, emotion_X), axis=1)\n",
    "\n",
    "# Train-test split\n",
    "# X_train = np.concatenate((X[:8400], X[10500:18900]))\n",
    "# X_test = np.concatenate((X[8400:10500], X[18900:]))\n",
    "# y_train = np.concatenate((df['TARGET'].values[:8400], df['TARGET'].values[10500:18900]))\n",
    "# y_test = np.concatenate((df['TARGET'].values[8400:10500], df['TARGET'].values[18900:]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['TARGET'].values, test_size=0.23, random_state=42)\n",
    "\n",
    "\n",
    "# Define the DFFNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
