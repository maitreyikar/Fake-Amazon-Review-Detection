{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5v7BSveQWJz9",
   "metadata": {
    "id": "5v7BSveQWJz9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing:\n",
    "# Collect a dataset of consumer reviews labeled as fake or truthful.\n",
    "# Preprocess the text data by removing stop words, special symbols, and lowercasing the text.\n",
    "# Extract emotion features from the reviews using lexicon-based methods.\n",
    "# Tokenize the text into unigrams, bigrams, and trigrams.\n",
    "# Calculate tf.idf weights for the n-grams.\n",
    "# Pre-train word embeddings using the Skip-Gram model on a large corpus of text data (e.g., Amazon reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efe4279",
   "metadata": {
    "id": "2efe4279",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from unidecode import unidecode\n",
    "from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c9ac51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "a7c9ac51",
    "outputId": "caec03aa-6e88-429f-92e7-8e3f2585e859",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21000 entries, 0 to 20999\n",
      "Data columns (total 24 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DOC_ID             21000 non-null  int64  \n",
      " 1   LABEL              21000 non-null  object \n",
      " 2   RATING             21000 non-null  int64  \n",
      " 3   VERIFIED_PURCHASE  21000 non-null  object \n",
      " 4   PRODUCT_CATEGORY   21000 non-null  object \n",
      " 5   PRODUCT_ID         21000 non-null  object \n",
      " 6   PRODUCT_TITLE      21000 non-null  object \n",
      " 7   REVIEW_TITLE       21000 non-null  object \n",
      " 8   REVIEW_TEXT        21000 non-null  object \n",
      " 9   target             21000 non-null  int64  \n",
      " 10  OPI_FIN_POS        21000 non-null  int64  \n",
      " 11  OPI_FIN_NEG        21000 non-null  int64  \n",
      " 12  BL_POS             21000 non-null  int64  \n",
      " 13  BL_NEG             21000 non-null  int64  \n",
      " 14  AFINN_POS          21000 non-null  float64\n",
      " 15  AFINN_NEG          21000 non-null  float64\n",
      " 16  S140_POS           21000 non-null  float64\n",
      " 17  S140_NEG           21000 non-null  float64\n",
      " 18  SWN_POS            21000 non-null  float64\n",
      " 19  SWN_NEG            21000 non-null  float64\n",
      " 20  NRC_HASH_POS       21000 non-null  float64\n",
      " 21  NRC_HASH_NEG       21000 non-null  float64\n",
      " 22  NRC_EMOTICON_POS   21000 non-null  float64\n",
      " 23  NRC_EMOTICON_NEG   21000 non-null  float64\n",
      "dtypes: float64(10), int64(7), object(7)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# The reviews are labelled as fake or real (in the dataset theyâ€™re mapped fake (label1) or real (label2)).\n",
    "# https://medium.com/@lievgarcia/deception-on-amazon-c1e30d977cfd\n",
    "\n",
    "df = pd.read_csv(\"amazon_reviews_features.txt\", sep = \"\\t\")   \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2671c0eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "2671c0eb",
    "outputId": "8c0c3610-05ec-4318-860d-4ccb97f1a9a7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>RATING</th>\n",
       "      <th>VERIFIED_PURCHASE</th>\n",
       "      <th>PRODUCT_CATEGORY</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODUCT_TITLE</th>\n",
       "      <th>REVIEW_TITLE</th>\n",
       "      <th>REVIEW_TEXT</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>AFINN_POS</th>\n",
       "      <th>AFINN_NEG</th>\n",
       "      <th>S140_POS</th>\n",
       "      <th>S140_NEG</th>\n",
       "      <th>SWN_POS</th>\n",
       "      <th>SWN_NEG</th>\n",
       "      <th>NRC_HASH_POS</th>\n",
       "      <th>NRC_HASH_NEG</th>\n",
       "      <th>NRC_EMOTICON_POS</th>\n",
       "      <th>NRC_EMOTICON_NEG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>PC</td>\n",
       "      <td>B00008NG7N</td>\n",
       "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
       "      <td>useful</td>\n",
       "      <td>When least you think so, this product will sav...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21302</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01281</td>\n",
       "      <td>0.21302</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>B00LH0Y3NM</td>\n",
       "      <td>Note 3 Battery : Stalion Strength Replacement ...</td>\n",
       "      <td>New era for batteries</td>\n",
       "      <td>Lithium batteries are something new introduced...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14293</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.05619</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Baby</td>\n",
       "      <td>B000I5UZ1Q</td>\n",
       "      <td>Fisher-Price Papasan Cradle Swing, Starlight</td>\n",
       "      <td>doesn't swing very well.</td>\n",
       "      <td>I purchased this swing for my baby. She is 6 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06157</td>\n",
       "      <td>0.107558</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.10604</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>B003822IRA</td>\n",
       "      <td>Casio MS-80B Standard Function Desktop Calculator</td>\n",
       "      <td>Great computing!</td>\n",
       "      <td>I was looking for an inexpensive desk calcolat...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20638</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.06995</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00PWSAXAM</td>\n",
       "      <td>Shine Whitening - Zero Peroxide Teeth Whitenin...</td>\n",
       "      <td>Only use twice a week</td>\n",
       "      <td>I only use it twice a week and the results are...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20035</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.02408</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00686HNUK</td>\n",
       "      <td>Tobacco Pipe Stand - Fold-away Portable - Ligh...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>I'm not sure what this is supposed to be but I...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23168</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.12150</td>\n",
       "      <td>0.23168</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Toys</td>\n",
       "      <td>B00NUG865W</td>\n",
       "      <td>ESPN 2-Piece Table Tennis</td>\n",
       "      <td>PING PONG TABLE GREAT FOR YOUTHS AND FAMILY</td>\n",
       "      <td>Pleased with ping pong table. 11 year old and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11041</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.01396</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00QUL8VX6</td>\n",
       "      <td>Abundant Health 25% Vitamin C Serum with Vitam...</td>\n",
       "      <td>Great vitamin C serum</td>\n",
       "      <td>Great vitamin C serum... I really like the oil...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.38425</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.14375</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.38425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B004YHKVCM</td>\n",
       "      <td>PODS Spring Meadow HE Turbo Laundry Detergent ...</td>\n",
       "      <td>wonderful detergent.</td>\n",
       "      <td>I've used tide pods laundry detergent for many...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32881</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.22687</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.32881</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00H4IBD0M</td>\n",
       "      <td>Sheer TEST, Best Testosterone Booster Suppleme...</td>\n",
       "      <td>WARNING: do not waste your money on this</td>\n",
       "      <td>Everybody wants to fall for their promises. Bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.10118</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.30493</td>\n",
       "      <td>0.10118</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00VQNLQL2</td>\n",
       "      <td>#1 Most Potent Male Performance Enhancement Su...</td>\n",
       "      <td>Unfortunately they didn't work for me.</td>\n",
       "      <td>Unfortunately they didn't work for me. They ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72399</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34957</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00IWOJS9A</td>\n",
       "      <td>Proctor Silex K2070YA Electric Kettle, 1-Liter</td>\n",
       "      <td>She said that it is easy to use</td>\n",
       "      <td>This kettle is a gift for my daughter. She sai...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30214</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.00454</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.30214</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00JEGSZCQ</td>\n",
       "      <td>Stainless Steel Tumbler with Straw- Hot and Co...</td>\n",
       "      <td>Works great</td>\n",
       "      <td>I love this tumbler! Keeps drinks warm or cold...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02778</td>\n",
       "      <td>0.084239</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.15218</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B000TD0PXU</td>\n",
       "      <td>Now Foods Mood Support With St Johns Wort Veg-...</td>\n",
       "      <td>Just OK</td>\n",
       "      <td>Only giving this 3 stars because it is so chea...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25092</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.086364</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04647</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>B0019QITHK</td>\n",
       "      <td>Boss Caressoft Medical Stool, black</td>\n",
       "      <td>Color Is Accurate</td>\n",
       "      <td>I bought 2 of these in brown for my island. Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03157</td>\n",
       "      <td>0.057292</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04059</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>B004WODP20</td>\n",
       "      <td>Sony MDRZX100  ZX Series Stereo Headphones (Bl...</td>\n",
       "      <td>Price is right!</td>\n",
       "      <td>wonderful headphones had them now for 5 months...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.048913</td>\n",
       "      <td>0.31048</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.11444</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Camera</td>\n",
       "      <td>B00E56WY18</td>\n",
       "      <td>Falcon Zero F360 HD DVR Dual Dash Cam, Rear Vi...</td>\n",
       "      <td>Good design, but a recommendation</td>\n",
       "      <td>Video quality if superb, fits just fine, looks...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17288</td>\n",
       "      <td>0.094298</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04051</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Sports</td>\n",
       "      <td>B000HE8NK8</td>\n",
       "      <td>Barnett Diablo Slingshot</td>\n",
       "      <td>good enough</td>\n",
       "      <td>As sling shots go, this is good enough for my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07757</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05340</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.07757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00FJKEGRY</td>\n",
       "      <td>Cuisavour's Deluxe Spiral Slicer with Japanese...</td>\n",
       "      <td>I just wish that there were guides and manuals...</td>\n",
       "      <td>just to be fair, i really believe that this is...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.30057</td>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.034639</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.39315</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.30057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00KNXIDH6</td>\n",
       "      <td>100 Tablet CleanGuard Nightguard Cleaner</td>\n",
       "      <td>and he was satisfied with this</td>\n",
       "      <td>These tablets are especially helpful if you us...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04280</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31013</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Home</td>\n",
       "      <td>B00LWRZFAA</td>\n",
       "      <td>Birds Flying Black Tree Branches Wall Sticker ...</td>\n",
       "      <td>best of money value</td>\n",
       "      <td>Looking decent as same shown in photos Thank Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03220</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.05727</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03220</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00B2YGB9M</td>\n",
       "      <td>Garcinia Cambogia Pure Extract Supplement, 80%...</td>\n",
       "      <td>It's harder to lose weight the older you get</td>\n",
       "      <td>I find that the older I get, the harder it is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.37945</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>0.042339</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03519</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.37945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Camera</td>\n",
       "      <td>B004TJ6JH6</td>\n",
       "      <td>NEEWERÂ® 160 LED CN-160 Dimmable Ultra High Pow...</td>\n",
       "      <td>So easy to use!!</td>\n",
       "      <td>I have had my camera for about 2 weeks now. Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01583</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01583</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00OBDRLVS</td>\n",
       "      <td>NatureWise Garcinia Cambogia Extract (Not Synt...</td>\n",
       "      <td>Stay Away And Don't Buy It</td>\n",
       "      <td>It is highly recommended not to buy this produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.35857</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09905</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.35857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>PC</td>\n",
       "      <td>B002RWJGHM</td>\n",
       "      <td>Thermaltake Power Supply 240-Pin 600 Power Sup...</td>\n",
       "      <td>good supply</td>\n",
       "      <td>The actual power supply is good, but the cable...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.35269</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15294</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.35269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B0018KL0YG</td>\n",
       "      <td>Oregano Oil CERTIFIED ORGANIC- Joy of the Moun...</td>\n",
       "      <td>Did you know? Oregano delivers more antioxidan...</td>\n",
       "      <td>My personal physician recommended it and I bou...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10952</td>\n",
       "      <td>0.054974</td>\n",
       "      <td>0.043194</td>\n",
       "      <td>0.11001</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00004OCO6</td>\n",
       "      <td>OXO Good Grips 6-Piece Measuring Cup Set</td>\n",
       "      <td>Products are great</td>\n",
       "      <td>I love the fact that you can easily read the m...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02574</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.064655</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.19030</td>\n",
       "      <td>0.02574</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B009LO31HK</td>\n",
       "      <td>SVETOLÂ® Green Coffee Bean Extract 50%  Acid - ...</td>\n",
       "      <td>One of the better ones</td>\n",
       "      <td>I have bought several products from this selle...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02455</td>\n",
       "      <td>0.068548</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Home</td>\n",
       "      <td>B001L1R3SO</td>\n",
       "      <td>Soft Heat Luxury Micro-Fleece Low-Voltage Elec...</td>\n",
       "      <td>Better than expected</td>\n",
       "      <td>This is an extremely well made electric blanke...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25617</td>\n",
       "      <td>0.086310</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.08774</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00PYF1J24</td>\n",
       "      <td>Cafe Deluxe French Press Coffee Maker &amp; Coffee...</td>\n",
       "      <td>great product for coffee lover that want littl...</td>\n",
       "      <td>great product for coffee lover that want littl...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36279</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.01722</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36279</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    DOC_ID       LABEL  RATING VERIFIED_PURCHASE        PRODUCT_CATEGORY  \\\n",
       "0        1  __label1__       4                 N                      PC   \n",
       "1        2  __label1__       4                 Y                Wireless   \n",
       "2        3  __label1__       3                 N                    Baby   \n",
       "3        4  __label1__       4                 N         Office Products   \n",
       "4        5  __label1__       4                 N                  Beauty   \n",
       "5        6  __label1__       3                 N  Health & Personal Care   \n",
       "6        7  __label1__       4                 N                    Toys   \n",
       "7        8  __label1__       4                 Y                  Beauty   \n",
       "8        9  __label1__       4                 N  Health & Personal Care   \n",
       "9       10  __label1__       1                 N  Health & Personal Care   \n",
       "10      11  __label1__       1                 N  Health & Personal Care   \n",
       "11      12  __label1__       4                 N                 Kitchen   \n",
       "12      13  __label1__       4                 N                 Kitchen   \n",
       "13      14  __label1__       4                 N  Health & Personal Care   \n",
       "14      15  __label1__       4                 N               Furniture   \n",
       "15      16  __label1__       4                 N             Electronics   \n",
       "16      17  __label1__       4                 Y                  Camera   \n",
       "17      18  __label1__       4                 N                  Sports   \n",
       "18      19  __label1__       4                 Y                 Kitchen   \n",
       "19      20  __label1__       4                 Y  Health & Personal Care   \n",
       "20      21  __label1__       4                 N                    Home   \n",
       "21      22  __label1__       4                 Y  Health & Personal Care   \n",
       "22      23  __label1__       3                 N                  Camera   \n",
       "23      24  __label1__       2                 Y  Health & Personal Care   \n",
       "24      25  __label1__       3                 N                      PC   \n",
       "25      26  __label1__       4                 N  Health & Personal Care   \n",
       "26      27  __label1__       4                 N                 Kitchen   \n",
       "27      28  __label1__       4                 Y  Health & Personal Care   \n",
       "28      29  __label1__       4                 N                    Home   \n",
       "29      30  __label1__       4                 Y                 Kitchen   \n",
       "\n",
       "    PRODUCT_ID                                      PRODUCT_TITLE  \\\n",
       "0   B00008NG7N        Targus PAUK10U Ultra Mini USB Keypad, Black   \n",
       "1   B00LH0Y3NM  Note 3 Battery : Stalion Strength Replacement ...   \n",
       "2   B000I5UZ1Q       Fisher-Price Papasan Cradle Swing, Starlight   \n",
       "3   B003822IRA  Casio MS-80B Standard Function Desktop Calculator   \n",
       "4   B00PWSAXAM  Shine Whitening - Zero Peroxide Teeth Whitenin...   \n",
       "5   B00686HNUK  Tobacco Pipe Stand - Fold-away Portable - Ligh...   \n",
       "6   B00NUG865W                          ESPN 2-Piece Table Tennis   \n",
       "7   B00QUL8VX6  Abundant Health 25% Vitamin C Serum with Vitam...   \n",
       "8   B004YHKVCM  PODS Spring Meadow HE Turbo Laundry Detergent ...   \n",
       "9   B00H4IBD0M  Sheer TEST, Best Testosterone Booster Suppleme...   \n",
       "10  B00VQNLQL2  #1 Most Potent Male Performance Enhancement Su...   \n",
       "11  B00IWOJS9A     Proctor Silex K2070YA Electric Kettle, 1-Liter   \n",
       "12  B00JEGSZCQ  Stainless Steel Tumbler with Straw- Hot and Co...   \n",
       "13  B000TD0PXU  Now Foods Mood Support With St Johns Wort Veg-...   \n",
       "14  B0019QITHK                Boss Caressoft Medical Stool, black   \n",
       "15  B004WODP20  Sony MDRZX100  ZX Series Stereo Headphones (Bl...   \n",
       "16  B00E56WY18  Falcon Zero F360 HD DVR Dual Dash Cam, Rear Vi...   \n",
       "17  B000HE8NK8                           Barnett Diablo Slingshot   \n",
       "18  B00FJKEGRY  Cuisavour's Deluxe Spiral Slicer with Japanese...   \n",
       "19  B00KNXIDH6           100 Tablet CleanGuard Nightguard Cleaner   \n",
       "20  B00LWRZFAA  Birds Flying Black Tree Branches Wall Sticker ...   \n",
       "21  B00B2YGB9M  Garcinia Cambogia Pure Extract Supplement, 80%...   \n",
       "22  B004TJ6JH6  NEEWERÂ® 160 LED CN-160 Dimmable Ultra High Pow...   \n",
       "23  B00OBDRLVS  NatureWise Garcinia Cambogia Extract (Not Synt...   \n",
       "24  B002RWJGHM  Thermaltake Power Supply 240-Pin 600 Power Sup...   \n",
       "25  B0018KL0YG  Oregano Oil CERTIFIED ORGANIC- Joy of the Moun...   \n",
       "26  B00004OCO6           OXO Good Grips 6-Piece Measuring Cup Set   \n",
       "27  B009LO31HK  SVETOLÂ® Green Coffee Bean Extract 50%  Acid - ...   \n",
       "28  B001L1R3SO  Soft Heat Luxury Micro-Fleece Low-Voltage Elec...   \n",
       "29  B00PYF1J24  Cafe Deluxe French Press Coffee Maker & Coffee...   \n",
       "\n",
       "                                         REVIEW_TITLE  \\\n",
       "0                                              useful   \n",
       "1                               New era for batteries   \n",
       "2                            doesn't swing very well.   \n",
       "3                                    Great computing!   \n",
       "4                               Only use twice a week   \n",
       "5                                            not sure   \n",
       "6         PING PONG TABLE GREAT FOR YOUTHS AND FAMILY   \n",
       "7                               Great vitamin C serum   \n",
       "8                                wonderful detergent.   \n",
       "9            WARNING: do not waste your money on this   \n",
       "10             Unfortunately they didn't work for me.   \n",
       "11                    She said that it is easy to use   \n",
       "12                                        Works great   \n",
       "13                                            Just OK   \n",
       "14                                  Color Is Accurate   \n",
       "15                                    Price is right!   \n",
       "16                  Good design, but a recommendation   \n",
       "17                                        good enough   \n",
       "18  I just wish that there were guides and manuals...   \n",
       "19                     and he was satisfied with this   \n",
       "20                                best of money value   \n",
       "21       It's harder to lose weight the older you get   \n",
       "22                                   So easy to use!!   \n",
       "23                         Stay Away And Don't Buy It   \n",
       "24                                        good supply   \n",
       "25  Did you know? Oregano delivers more antioxidan...   \n",
       "26                                 Products are great   \n",
       "27                             One of the better ones   \n",
       "28                               Better than expected   \n",
       "29  great product for coffee lover that want littl...   \n",
       "\n",
       "                                          REVIEW_TEXT  target  ...  AFINN_POS  \\\n",
       "0   When least you think so, this product will sav...       0  ...        2.0   \n",
       "1   Lithium batteries are something new introduced...       0  ...        4.0   \n",
       "2   I purchased this swing for my baby. She is 6 m...       0  ...        9.0   \n",
       "3   I was looking for an inexpensive desk calcolat...       0  ...        1.0   \n",
       "4   I only use it twice a week and the results are...       0  ...        5.0   \n",
       "5   I'm not sure what this is supposed to be but I...       0  ...        4.0   \n",
       "6   Pleased with ping pong table. 11 year old and ...       0  ...        5.0   \n",
       "7   Great vitamin C serum... I really like the oil...       0  ...        9.0   \n",
       "8   I've used tide pods laundry detergent for many...       0  ...        7.0   \n",
       "9   Everybody wants to fall for their promises. Bu...       0  ...        0.0   \n",
       "10  Unfortunately they didn't work for me. They ma...       0  ...        0.0   \n",
       "11  This kettle is a gift for my daughter. She sai...       0  ...        8.0   \n",
       "12  I love this tumbler! Keeps drinks warm or cold...       0  ...        7.0   \n",
       "13  Only giving this 3 stars because it is so chea...       0  ...        0.0   \n",
       "14  I bought 2 of these in brown for my island. Th...       0  ...        3.0   \n",
       "15  wonderful headphones had them now for 5 months...       0  ...       14.0   \n",
       "16  Video quality if superb, fits just fine, looks...       0  ...       13.0   \n",
       "17  As sling shots go, this is good enough for my ...       0  ...        5.0   \n",
       "18  just to be fair, i really believe that this is...       0  ...        7.0   \n",
       "19  These tablets are especially helpful if you us...       0  ...        6.0   \n",
       "20  Looking decent as same shown in photos Thank Y...       0  ...        2.0   \n",
       "21  I find that the older I get, the harder it is ...       0  ...        0.0   \n",
       "22  I have had my camera for about 2 weeks now. Th...       0  ...        7.0   \n",
       "23  It is highly recommended not to buy this produ...       0  ...        0.0   \n",
       "24  The actual power supply is good, but the cable...       0  ...        4.0   \n",
       "25  My personal physician recommended it and I bou...       0  ...       23.0   \n",
       "26  I love the fact that you can easily read the m...       0  ...        7.0   \n",
       "27  I have bought several products from this selle...       0  ...        6.0   \n",
       "28  This is an extremely well made electric blanke...       0  ...        2.0   \n",
       "29  great product for coffee lover that want littl...       0  ...       12.0   \n",
       "\n",
       "    AFINN_NEG  S140_POS  S140_NEG   SWN_POS   SWN_NEG  NRC_HASH_POS  \\\n",
       "0         0.0   0.21302   0.00000  0.093750  0.007812       0.00000   \n",
       "1         0.0   0.00000   0.14293  0.036017  0.021186       0.05619   \n",
       "2         0.0   0.00000   0.06157  0.107558  0.017442       0.10604   \n",
       "3         0.0   0.00000   0.20638  0.031250  0.052083       0.00000   \n",
       "4         0.0   0.00000   0.20035  0.014205  0.014205       0.02408   \n",
       "5         0.0   0.23168   0.00000  0.027778  0.060185       0.00000   \n",
       "6         0.0   0.00000   0.11041  0.095238  0.059524       0.01396   \n",
       "7         0.0   0.00000   0.38425  0.062500  0.040179       0.14375   \n",
       "8         0.0   0.32881   0.00000  0.051136  0.045455       0.22687   \n",
       "9        -0.0   0.10118   0.00000  0.026515  0.037879       0.00000   \n",
       "10        2.0   0.00000   0.72399  0.035714  0.062500       0.00000   \n",
       "11        0.0   0.30214   0.00000  0.125000  0.054688       0.00454   \n",
       "12        0.0   0.00000   0.02778  0.084239  0.027174       0.15218   \n",
       "13       12.0   0.00000   0.25092  0.054545  0.086364       0.00000   \n",
       "14        0.0   0.00000   0.03157  0.057292  0.062500       0.00000   \n",
       "15        0.0   0.11444   0.00000  0.130435  0.048913       0.31048   \n",
       "16        0.0   0.00000   0.17288  0.094298  0.057018       0.00000   \n",
       "17        0.0   0.00000   0.07757  0.154412  0.007353       0.00000   \n",
       "18        0.0   0.00000   0.30057  0.052711  0.034639       0.00000   \n",
       "19        0.0   0.00000   0.04280  0.070312  0.000000       0.00000   \n",
       "20        0.0   0.03220   0.00000  0.151042  0.010417       0.05727   \n",
       "21        4.0   0.00000   0.37945  0.074597  0.042339       0.00000   \n",
       "22        0.0   0.01583   0.00000  0.092105  0.019737       0.17730   \n",
       "23        7.0   0.00000   0.35857  0.078125  0.109375       0.00000   \n",
       "24        0.0   0.00000   0.35269  0.075758  0.026515       0.00000   \n",
       "25        0.0   0.00000   0.10952  0.054974  0.043194       0.11001   \n",
       "26        0.0   0.02574   0.00000  0.064655  0.008621       0.00000   \n",
       "27        0.0   0.00000   0.02455  0.068548  0.024194       0.12850   \n",
       "28        0.0   0.00000   0.25617  0.086310  0.035714       0.00000   \n",
       "29        0.0   0.36279   0.00000  0.100000  0.058333       0.01722   \n",
       "\n",
       "    NRC_HASH_NEG  NRC_EMOTICON_POS  NRC_EMOTICON_NEG  \n",
       "0        0.01281           0.21302           0.00000  \n",
       "1        0.00000           0.00000           0.14293  \n",
       "2        0.00000           0.00000           0.06157  \n",
       "3        0.06995           0.00000           0.20638  \n",
       "4        0.00000           0.00000           0.20035  \n",
       "5        0.12150           0.23168           0.00000  \n",
       "6        0.00000           0.00000           0.11041  \n",
       "7        0.00000           0.00000           0.38425  \n",
       "8        0.00000           0.32881           0.00000  \n",
       "9        0.30493           0.10118           0.00000  \n",
       "10       0.34957           0.00000           0.72399  \n",
       "11       0.00000           0.30214           0.00000  \n",
       "12       0.00000           0.00000           0.02778  \n",
       "13       0.04647           0.00000           0.25092  \n",
       "14       0.04059           0.00000           0.03157  \n",
       "15       0.00000           0.11444           0.00000  \n",
       "16       0.04051           0.00000           0.17288  \n",
       "17       0.05340           0.00000           0.07757  \n",
       "18       0.39315           0.00000           0.30057  \n",
       "19       0.31013           0.00000           0.04280  \n",
       "20       0.00000           0.03220           0.00000  \n",
       "21       0.03519           0.00000           0.37945  \n",
       "22       0.00000           0.01583           0.00000  \n",
       "23       0.09905           0.00000           0.35857  \n",
       "24       0.15294           0.00000           0.35269  \n",
       "25       0.00000           0.00000           0.10952  \n",
       "26       0.19030           0.02574           0.00000  \n",
       "27       0.00000           0.00000           0.02455  \n",
       "28       0.08774           0.00000           0.25617  \n",
       "29       0.00000           0.36279           0.00000  \n",
       "\n",
       "[30 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping binary output label to numeric values 0 (fake review) and 1 (real review)\n",
    "df['target'] = pd.factorize(df['LABEL'])[0]\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5867af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5867af1",
    "outputId": "2bd2feb9-5d36-44be-8a80-b80cd775bf8e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500 10500\n"
     ]
    }
   ],
   "source": [
    "num_fake = len(df[df['target'] == 0])\n",
    "num_real = len(df[df['target'] == 1])\n",
    "\n",
    "print(num_real, num_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b13d7",
   "metadata": {
    "id": "684b13d7"
   },
   "source": [
    "As seen above, the dataset is evenly balanced across both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5189219",
   "metadata": {
    "id": "e5189219"
   },
   "source": [
    "# Review Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa116b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aa116b5",
    "outputId": "5db481f3-ae91-4711-de84-7b029336df01",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when least you think so this product will save the day just keep it around just in case you need it for something'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# converting to lowercase and tokenizing\n",
    "review_tokens = [tokenizer.tokenize(review.lower()) for review in df['REVIEW_TEXT']]\n",
    "\n",
    "#removing special characters\n",
    "review_tokens = [[unidecode(token) for token in review if token.isalnum()] for review in review_tokens]\n",
    "\" \".join(review_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f336868",
   "metadata": {},
   "source": [
    "# Emotion Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f4ca",
   "metadata": {},
   "source": [
    "### Polarity: OpinionFinder 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa1ab6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "\n",
    "# OpinionFinder2.0: Tags words with polarity (pos/neg)\n",
    "# Used t o develop two features: OPI_FIN_POS and OPI_FIN_NEG\n",
    "# defined as the number of words that corresponding to each polarity respectively, per review\n",
    "\n",
    "# parent_dir = \"database/docs/amazon_reviews/\"\n",
    "# f_count = 1\n",
    "# count = 0\n",
    "# doclist = \"amazon_reviews_\" + str(f_count) + \".doclist\"\n",
    "# f2 = open(doclist, \"a\")\n",
    "\n",
    "# for i in range(len(review_tokens)):\n",
    "#     fname = parent_dir + \"rev_id_\" + str(i + 1)\n",
    "#     fp = open(fname, 'w')\n",
    "#     review_text = ' '.join(review_tokens[i])\n",
    "#     fp.write(review_text)\n",
    "#     fp.close()\n",
    "    \n",
    "#     if count == 2100:\n",
    "#         f2.close()\n",
    "#         count = 0\n",
    "#         f_count += 1\n",
    "        \n",
    "#         doclist = \"amazon_reviews_\" + str(f_count) + \".doclist\"\n",
    "#         f2 = open(doclist, \"a\")\n",
    "        \n",
    "#     f2.write(fname+\"\\n\")         \n",
    "#     count += 1\n",
    "    \n",
    "# f2.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# commands to execute OpinionFinder2.0\n",
    "\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_1.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_2.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_3.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_4.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_5.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_6.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_7.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_8.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_9.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_10.doclist -d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extracting polarity labels from output file (exp_polarity.txt) and adding to dataset\n",
    "\n",
    "# opinion_finder_pos_count = []\n",
    "# opinion_finder_neg_count = []\n",
    "\n",
    "# parent_dir = \"database/docs/amazon_reviews/rev_id_\"\n",
    "# suffix = \"_auto_anns/exp_polarity.txt\"\n",
    "\n",
    "# for i in range(len(review_tokens)):\n",
    "#     fpath = parent_dir + str(i + 1) + suffix\n",
    "#     f = open(fpath, \"r\")\n",
    "#     content = f.read()\n",
    "#     f.close()\n",
    "    \n",
    "#     opinion_finder_pos_count.append(content.count(\"positive\"))\n",
    "#     opinion_finder_neg_count.append(content.count(\"negative\"))\n",
    "    \n",
    "\n",
    "# df['OPI_FIN_POS'] = opinion_finder_pos_count\n",
    "# df['OPI_FIN_NEG'] = opinion_finder_neg_count\n",
    "# df.to_csv(\"amazon_reviews_with_polarity.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a49628",
   "metadata": {},
   "source": [
    "### Polarity: Bing Liu's Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53362a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "\n",
    "# Bing Liu et al: Opinion Lexicon for positive and negative polarity tagging of words\n",
    "# Used to develop 2 features: BL_POS and BL_NEG\n",
    "# defined as the number of words that corresponding to each polarity respectively, per review\n",
    "\n",
    "# dir_name = \"../bing-liu-opinion-lexicon-English/\"\n",
    "# pos_file = dir_name + \"positive-words.txt\"\n",
    "# neg_file = dir_name + \"negative-words.txt\"\n",
    "\n",
    "# f1 = open(pos_file, \"r\")\n",
    "# f2 = open(neg_file, \"r\")\n",
    "\n",
    "# pos_lexicon = f1.read()\n",
    "# neg_lexicon = f2.read()\n",
    "\n",
    "# f1.close()\n",
    "# f2.close()\n",
    "\n",
    "# bl_pos = []\n",
    "# bl_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     count_pos = 0\n",
    "#     count_neg = 0\n",
    "    \n",
    "#     for token in review:\n",
    "#         if token in pos_lexicon:\n",
    "#             count_pos += 1\n",
    "#         if token in neg_lexicon:\n",
    "#             count_neg += 1\n",
    "            \n",
    "#     bl_pos.append(count_pos)\n",
    "#     bl_neg.append(count_neg)\n",
    "    \n",
    "# print(bl_pos[:15])\n",
    "# print(bl_neg[:15])\n",
    "\n",
    "# df['BL_POS'] = bl_pos\n",
    "# df['BL_NEG'] = bl_neg\n",
    "\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795591c",
   "metadata": {},
   "source": [
    "### Strength Score: AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c48852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip install afinn\n",
    "\n",
    "# # AFINN: Sentiment lexicon for measuring the positive and negative score of a review\n",
    "# # Used to develop 2 features: BL_POS and BL_NEG\n",
    "\n",
    "# from afinn import Afinn\n",
    "# afn = Afinn()\n",
    "\n",
    "# afinn_pos = []\n",
    "# afinn_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     review = \" \".join(review)\n",
    "#     s = afn.score(review)\n",
    "    \n",
    "#     if s > 0:\n",
    "#         afinn_pos.append(s)\n",
    "#         afinn_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         afinn_pos.append(0.0)\n",
    "#         afinn_neg.append(-1 * s)\n",
    "        \n",
    "# print(afinn_pos[:15])\n",
    "# print(afinn_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['AFINN_POS'] = afinn_pos\n",
    "# df['AFINN_NEG'] = afinn_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570d10b",
   "metadata": {},
   "source": [
    "### Strength Score: Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c04cfb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment140: Lexicon for measuring the positive and negative score of a review\n",
    "# Used to develop 2 features: S140_POS and S140_NEG\n",
    "\n",
    "# import csv\n",
    "\n",
    "# dir_name = \"../Sentiment140-Lexicon/\"\n",
    "# f1 = \"unigrams-pmilexicon.txt\"\n",
    "# f2 = \"bigrams-pmilexicon.txt\"\n",
    "\n",
    "\n",
    "# uni_lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "# bi_lex = pd.read_csv(dir_name + f2, sep = \"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "# uni_dict = dict(zip(uni_lex[\"term\"], uni_lex[\"score\"]))\n",
    "# bi_dict = dict(zip(bi_lex[\"term\"], bi_lex[\"score\"]))\n",
    "\n",
    "# s140_pos = []\n",
    "# s140_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     score = 0\n",
    "#     uni_score = 0\n",
    "#     uni_c = 0\n",
    "        \n",
    "#     for unigram in review:\n",
    "#         if unigram in uni_dict:\n",
    "#             uni_score += uni_dict[unigram]\n",
    "#             uni_c += 1\n",
    "    \n",
    "#     if uni_c > 0:\n",
    "#         uni_score /= uni_c\n",
    "    \n",
    "    \n",
    "#     bi_score = 0\n",
    "#     bi_c = 0\n",
    "    \n",
    "#     bigrams = list(ngrams(review, 2))\n",
    "#     for bigram in bigrams:\n",
    "#         text = \" \".join(bigram)\n",
    "#         if text in bi_dict:\n",
    "#             bi_score += bi_dict[text]\n",
    "#             bi_c += 1\n",
    "    \n",
    "#     if bi_c > 0:\n",
    "#         bi_score /= bi_c\n",
    "    \n",
    "    \n",
    "#     score = (bi_score + uni_score) / (int(uni_c > 0) + int(bi_c > 0))\n",
    "    \n",
    "#     if score > 0:\n",
    "#         s140_pos.append(round(score, 5))\n",
    "#         s140_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         s140_pos.append(0.0)\n",
    "#         s140_neg.append(round(-1 * score, 5))\n",
    "    \n",
    "    \n",
    "# print(s140_pos[:15])\n",
    "# print(s140_neg[:15])\n",
    "\n",
    "# print(s140_pos.count(0.0), s140_neg.count(0.0), len(s140_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['S140_POS'] = s140_pos\n",
    "# df['S140_NEG'] = s140_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3bc373",
   "metadata": {},
   "source": [
    "### Strength Score: SentiWordNet3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6df882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentiWordNet3.0: Lexicon for measuring the positive and negative score of synsets in WordNet 3.0\n",
    "# Used to develop 2 features: SWN_POS and SWN_NEG\n",
    "#(for this one, try to make the scores exclusive like the others)\n",
    "\n",
    "\n",
    "# # import nltk\n",
    "# # nltk.download('sentiwordnet')\n",
    "\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# swn_pos = []\n",
    "# swn_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     pos_score = 0\n",
    "#     neg_score = 0\n",
    "#     count = 0\n",
    "    \n",
    "#     for term in review:\n",
    "#         res = swn.senti_synsets(term)\n",
    "        \n",
    "#         try:\n",
    "#             res0 = list(res)[0]\n",
    "#             pos_score += res0.pos_score()\n",
    "#             neg_score += res0.neg_score()\n",
    "#             count += 1\n",
    "            \n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     if count > 0:\n",
    "#         pos_score = pos_score / count\n",
    "#         neg_score = neg_score / count\n",
    "        \n",
    "#     swn_pos.append(pos_score)\n",
    "#     swn_neg.append(neg_score)\n",
    "\n",
    "# print(swn_pos[:15])\n",
    "# print(swn_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada70b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['SWN_POS'] = swn_pos\n",
    "# df['SWN_NEG'] = swn_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458c3c7",
   "metadata": {},
   "source": [
    "### Strength Score: NRC Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRC Hashtag: Lexicon for measuring the positive and negative score of a review based on twitter hashtags\n",
    "# Used to develop 2 features: NRC_HASH_POS and NRC_HASH_NEG\n",
    "\n",
    "\n",
    "# import csv\n",
    "\n",
    "# dir_name = \"../NRC-Hashtag-Sentiment-Lexicon-v0.1/\"\n",
    "# f1 = \"unigrams-pmilexicon.txt\"\n",
    "# f2 = \"bigrams-pmilexicon.txt\"\n",
    "\n",
    "# uni_lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "# bi_lex = pd.read_csv(dir_name + f2, sep = \"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "# uni_dict = dict(zip(uni_lex[\"term\"], uni_lex[\"score\"]))\n",
    "# bi_dict = dict(zip(bi_lex[\"term\"], bi_lex[\"score\"]))\n",
    "\n",
    "# nrc_hash_pos = []\n",
    "# nrc_hash_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     score = 0\n",
    "#     uni_score = 0\n",
    "#     uni_c = 0\n",
    "        \n",
    "#     for unigram in review:\n",
    "#         if unigram in uni_dict:\n",
    "#             uni_score += uni_dict[unigram]\n",
    "#             uni_c += 1\n",
    "    \n",
    "#     if uni_c > 0:\n",
    "#         uni_score /= uni_c\n",
    "    \n",
    "    \n",
    "#     bi_score = 0\n",
    "#     bi_c = 0\n",
    "    \n",
    "#     bigrams = list(ngrams(review, 2))\n",
    "#     for bigram in bigrams:\n",
    "#         text = \" \".join(bigram)\n",
    "#         if text in bi_dict:\n",
    "#             bi_score += bi_dict[text]\n",
    "#             bi_c += 1\n",
    "    \n",
    "#     if bi_c > 0:\n",
    "#         bi_score /= bi_c\n",
    "    \n",
    "    \n",
    "#     score = (bi_score + uni_score) / (int(uni_c > 0) + int(bi_c > 0))\n",
    "    \n",
    "#     if score > 0:\n",
    "#         nrc_hash_pos.append(round(score, 5))\n",
    "#         nrc_hash_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         nrc_hash_pos.append(0.0)\n",
    "#         nrc_hash_neg.append(round(-1 * score, 5))\n",
    "    \n",
    "    \n",
    "# print(nrc_hash_pos[:15])\n",
    "# print(nrc_hash_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a65c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['NRC_HASH_POS'] = nrc_hash_pos\n",
    "# df['NRC_HASH_NEG'] = nrc_hash_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917ebeb",
   "metadata": {},
   "source": [
    "### Strength Score: NRC Emoticon (USELESS TBH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e15c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21302, 0.0, 0.0, 0.0, 0.0, 0.23168, 0.0, 0.0, 0.32881, 0.10118, 0.0, 0.30214, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.14293, 0.06157, 0.20638, 0.20035, 0.0, 0.11041, 0.38425, 0.0, 0.0, 0.72399, 0.0, 0.02778, 0.25092, 0.03157]\n"
     ]
    }
   ],
   "source": [
    "# # NRC Emoticon: Lexicon for measuring the positive and negative score of a review based on emojis\n",
    "# # Used to develop 2 features: NRC_EMOTICON_POS and NRC_EMOTICON_NEG\n",
    "\n",
    "\n",
    "# import csv\n",
    "\n",
    "# dir_name = \"../NRC-Emoticon-Lexicon-v1.0/\"\n",
    "# f1 = \"Emoticon-unigrams.txt\"\n",
    "# f2 = \"Emoticon-bigrams.txt\"\n",
    "\n",
    "# uni_lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "# bi_lex = pd.read_csv(dir_name + f2, sep = \"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "# uni_dict = dict(zip(uni_lex[\"term\"], uni_lex[\"score\"]))\n",
    "# bi_dict = dict(zip(bi_lex[\"term\"], bi_lex[\"score\"]))\n",
    "\n",
    "# nrc_emoticon_pos = []\n",
    "# nrc_emoticon_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     score = 0\n",
    "#     uni_score = 0\n",
    "#     uni_c = 0\n",
    "        \n",
    "#     for unigram in review:\n",
    "#         if unigram in uni_dict:\n",
    "#             uni_score += uni_dict[unigram]\n",
    "#             uni_c += 1\n",
    "    \n",
    "#     if uni_c > 0:\n",
    "#         uni_score /= uni_c\n",
    "    \n",
    "    \n",
    "#     bi_score = 0\n",
    "#     bi_c = 0\n",
    "    \n",
    "#     bigrams = list(ngrams(review, 2))\n",
    "#     for bigram in bigrams:\n",
    "#         text = \" \".join(bigram)\n",
    "#         if text in bi_dict:\n",
    "#             bi_score += bi_dict[text]\n",
    "#             bi_c += 1\n",
    "    \n",
    "#     if bi_c > 0:\n",
    "#         bi_score /= bi_c\n",
    "    \n",
    "    \n",
    "#     score = (bi_score + uni_score) / (int(uni_c > 0) + int(bi_c > 0))\n",
    "    \n",
    "#     if score > 0:\n",
    "#         nrc_emoticon_pos.append(round(score, 5))\n",
    "#         nrc_emoticon_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         nrc_emoticon_pos.append(0.0)\n",
    "#         nrc_emoticon_neg.append(round(-1 * score, 5))\n",
    "    \n",
    "    \n",
    "# print(nrc_emoticon_pos[:15])\n",
    "# print(nrc_emoticon_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1285eef",
   "metadata": {
    "id": "vHy6iP-GUBfX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "\n",
    "# reference for sentiment analysis with SentiWordNet\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('sentiwordnet')\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "# list(swn.senti_synsets('slow'))\n",
    "\n",
    "# sentence='It was a really good day'\n",
    "# from nltk.tag import pos_tag\n",
    "# token = nltk.word_tokenize(sentence)\n",
    "# after_tagging = nltk.pos_tag(token)\n",
    "# print (token)\n",
    "# print (after_tagging)\n",
    "# def penn_to_wn(tag):\n",
    "#     \"\"\"\n",
    "#     Convert between the PennTreebank tags to simple Wordnet tags\n",
    "#     \"\"\"\n",
    "#     if tag.startswith('J'):\n",
    "#         return wn.ADJ\n",
    "#     elif tag.startswith('N'):\n",
    "#         return wn.NOUN\n",
    "#     elif tag.startswith('R'):\n",
    "#         return wn.ADV\n",
    "#     elif tag.startswith('V'):\n",
    "#         return wn.VERB\n",
    "#     return None\n",
    "# sentiment = 0.0\n",
    "# tokens_count = 0\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# for word, tag in after_tagging:\n",
    "#             wn_tag = penn_to_wn(tag)\n",
    "#             if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "#                 continue\n",
    "\n",
    "#             lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "#             if not lemma:\n",
    "#                 continue\n",
    "\n",
    "#             synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "#             if not synsets:\n",
    "#                 continue\n",
    "\n",
    "#             # Take the first sense, the most common\n",
    "#             synset = synsets[0]\n",
    "#             swn_synset = swn.senti_synset(synset.name())\n",
    "#             print(swn_synset)\n",
    "\n",
    "#             sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "#             tokens_count += 1\n",
    "\n",
    "# print (sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbb23c",
   "metadata": {
    "id": "a4dbb23c"
   },
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd82e90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd82e90",
    "outputId": "e9140072-9cbd-4fe0-8f35-a9faf4c2780a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stop word removal:  ['love', 'the', 'bottle', 'very', 'much', 'br', 'br', 'iVm', 'a', 'tea', 'lover', 'when', 'i', 'saw', 'this', 'bottle', 'i', 'knew', 'that', 'it', 'was', 'what', 'i', 'wanted', 'the', 'shape', 'is', 'fantastic', 'feels', 'nice', 'in', 'your', 'hand', 'perfect', 'size', 'to', 'have', 'in', 'my', 'car', 'i', 'took', 'it', 'all', 'around', 'so', 'i', 'can', 'enjoy', 'my', 'tea', 'everywhere', 'love', 'it', 'very', 'much', 'the', 'one', 'with', 'infuser', 'also', 'looks', 'good']\n",
      "\n",
      "After stop word removal:  ['love', 'bottle', 'much', 'br', 'br', 'iVm', 'tea', 'lover', 'saw', 'bottle', 'knew', 'wanted', 'shape', 'fantastic', 'feels', 'nice', 'hand', 'perfect', 'size', 'car', 'took', 'around', 'enjoy', 'tea', 'everywhere', 'love', 'much', 'one', 'infuser', 'also', 'looks', 'good']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "content_review_tokens = [[token for token in review if token not in stop_words and token.isalnum()] for review in review_tokens]\n",
    "\n",
    "print(\"Before stop word removal: \", review_tokens[6914])\n",
    "print()\n",
    "print(\"After stop word removal: \", content_review_tokens[6914])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fb3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3fb3f4",
    "outputId": "42626a16-1fd4-448b-a269-bc56ff12ce9b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STEMMING AND LEMMATIZATION IS NOT REQUIRED I THINK, BECAUSE THE PAPER DOES NOT MENTION IT\n",
    "\n",
    "# from nltk.stem import SnowballStemmer     #porter 2 algorithm\n",
    "# snowball = SnowballStemmer(language = \"english\")\n",
    "# content_review_tokens = [[snowball.stem(token) for token in review] for review in content_review_tokens]\n",
    "# print(content_review_tokens[374])\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# content_review_tokens = [[lemmatizer.lemmatize(token) for token in review] for review in content_review_tokens]\n",
    "# print(content_review_tokens[374])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88797e4",
   "metadata": {
    "id": "c88797e4"
   },
   "source": [
    "# N-Gram Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88db9c59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88db9c59",
    "outputId": "6e7cd9d4-082d-40ba-f9a5-c87d444387aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brushes',), ('soft',), ('soon',), ('first',), ('usage',), ('see',), ('bristles',), ('coming',), ('worth',), ('purchase',), ('falling',), ('generic',), ('product',)]\n",
      "[('brushes', 'soft'), ('soft', 'soon'), ('soon', 'first'), ('first', 'usage'), ('usage', 'see'), ('see', 'bristles'), ('bristles', 'coming'), ('coming', 'worth'), ('worth', 'purchase'), ('purchase', 'falling'), ('falling', 'generic'), ('generic', 'product')]\n",
      "[('brushes', 'soft', 'soon'), ('soft', 'soon', 'first'), ('soon', 'first', 'usage'), ('first', 'usage', 'see'), ('usage', 'see', 'bristles'), ('see', 'bristles', 'coming'), ('bristles', 'coming', 'worth'), ('coming', 'worth', 'purchase'), ('worth', 'purchase', 'falling'), ('purchase', 'falling', 'generic'), ('falling', 'generic', 'product')]\n"
     ]
    }
   ],
   "source": [
    "review_text_unigrams = [list(ngrams(tokens, 1)) for tokens in content_review_tokens]\n",
    "review_text_bigrams = [list(ngrams(tokens, 2)) for tokens in content_review_tokens]\n",
    "review_text_trigrams = [list(ngrams(tokens, 3)) for tokens in content_review_tokens]\n",
    "\n",
    "print(review_text_unigrams[374])\n",
    "print(review_text_bigrams[374])\n",
    "print(review_text_trigrams[374])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "P0XO5LqymuoJ",
   "metadata": {
    "id": "P0XO5LqymuoJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Train the Skip-Gram model\n",
    "vector_size = 100  # Dimensionality of word embeddings\n",
    "window_size = 5    # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 1      # Minimum frequency count of words to consider when training the model\n",
    "workers = 4        # Number of threads to use while training\n",
    "\n",
    "# Train the Skip-Gram model\n",
    "skipgram_model = Word2Vec(sentences=content_review_tokens,\n",
    "                          vector_size=vector_size,\n",
    "                          window=window_size,\n",
    "                          min_count=min_count,\n",
    "                          workers=workers,\n",
    "                          sg=1)  # sg=1 specifies Skip-Gram model\n",
    "\n",
    "# Save the trained word embeddings\n",
    "# skipgram_model.save('skipgram_word_embeddings.model')\n",
    "\n",
    "#skipgram_model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41a01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02099521  0.09561954  0.0548805  -0.04881383 -0.07118601 -0.6665906\n",
      "  0.64163786  0.70358986 -0.30299774  0.066332    0.2826829  -0.53231275\n",
      " -0.2839308  -0.21144377  0.12996304  0.25996113 -0.20663016 -0.3951933\n",
      " -0.07755546 -0.22268693 -0.01645495  0.45674834  0.1858662  -0.25952634\n",
      "  0.06009199  0.15379114 -0.29662326 -0.0759635  -0.359151   -0.10081615\n",
      "  0.20416364 -0.17115799  0.03937411 -0.43139616 -0.3961922  -0.03462957\n",
      "  0.33540177 -0.26132286  0.08713812 -0.2932426   0.16218199 -0.01687521\n",
      " -0.20076196 -0.19930044  0.37899625  0.13617526 -0.244787    0.32846168\n",
      "  0.42483485  0.11272307 -0.06090441 -0.56031215 -0.10054269  0.06896328\n",
      "  0.14094675  0.09178472  0.3212381  -0.13634907 -0.11138833 -0.11977422\n",
      " -0.03117456 -0.02779451 -0.46357426 -0.04491124  0.03956963  0.53614825\n",
      "  0.00466609  0.52460355 -0.3753548   0.35372722 -0.20420942  0.54145205\n",
      "  0.73458654 -0.2379491   0.3407574   0.3601199  -0.14823644 -0.1386738\n",
      " -0.08234764 -0.07445995 -0.32587016 -0.39289358 -0.36685687  0.48370132\n",
      "  0.14359656 -0.01648156  0.08408342  0.42883104  0.01276175  0.20158453\n",
      "  0.70103645  0.05334721 -0.068979   -0.2134507   0.8250851   0.22900303\n",
      "  0.5872673  -0.37646264  0.24194483 -0.10045243]\n",
      "[-0.0690465   0.31491104 -0.16128682  0.08734705 -0.04826433 -0.43250778\n",
      "  0.39333358  0.6592255  -0.44377437 -0.03655935  0.21609767 -0.25192213\n",
      "  0.04858587  0.00596433  0.0439681   0.11713903  0.11772665 -0.47996575\n",
      " -0.2562455  -0.4794851  -0.13313699  0.5841936   0.22632857 -0.07389037\n",
      "  0.03933579  0.11878715 -0.1909512  -0.23703234 -0.40120363 -0.08630455\n",
      "  0.34132233 -0.17943776  0.27031454 -0.5717365  -0.32212478 -0.05906578\n",
      " -0.14764181  0.14700611  0.01474628 -0.4784005  -0.06110219 -0.05832799\n",
      " -0.1473411  -0.05046678  0.5650714  -0.11302167 -0.1987292   0.28480735\n",
      "  0.24574931  0.0734423  -0.23419599 -0.6130835   0.13301232 -0.02059595\n",
      " -0.05612888 -0.1348281   0.6371957  -0.16604711  0.06106206 -0.08637006\n",
      " -0.03233358 -0.0913607  -0.07453922  0.12331988 -0.44683167  0.6123601\n",
      "  0.12438421  0.6219358  -0.3602137   0.5123693  -0.15616101  0.36248302\n",
      "  0.7272908  -0.12540197  0.41892964  0.4588575  -0.3711344  -0.06109429\n",
      " -0.14141671 -0.05234018 -0.32789728 -0.6874499  -0.7105521   0.27088717\n",
      "  0.3127934   0.02755813 -0.17725202  0.46604905  0.06083376  0.12037852\n",
      "  0.31721315  0.40551382  0.24069172  0.06215914  0.49722975  0.05886479\n",
      "  0.5600861  -0.38895333  0.02818291 -0.29508045]\n"
     ]
    }
   ],
   "source": [
    "print(skipgram_model.wv['computer'])\n",
    "print(skipgram_model.wv['laptop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vLMVB3huWNQp",
   "metadata": {
    "id": "vLMVB3huWNQp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Model Architecture Design:\n",
    "\n",
    "# DFFNN Model:\n",
    "# Design a multilayer perceptron neural network with two hidden layers.\n",
    "# Determine the input layer size based on the features extracted in data preprocessing (e.g., 2000 n-grams, 30 emotion features, and word embeddings).\n",
    "# Define the number of neurons in each hidden layer based on a grid search procedure.\n",
    "# Choose rectified linear units as the activation function for the hidden layers.\n",
    "# Implement dropout regularization to prevent overfitting.\n",
    "# Utilize softmax activation in the output layer for binary classification (fake/truthful).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20a698",
   "metadata": {},
   "source": [
    "# DFFNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xRQWjt37WP5T",
   "metadata": {
    "id": "xRQWjt37WP5T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DFFNN Model:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming you have the features extracted in data preprocessing stored in X\n",
    "# and the binary labels in y (0 for fake, 1 for truthful)\n",
    "\n",
    "# Input layer size based on features\n",
    "input_layer_size = len(df.columns)  # Adjust based on the actual number of features\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_layer1_neurons = 128\n",
    "hidden_layer2_neurons = 64\n",
    "dropout_rate = 0.5  # Adjust as needed\n",
    "\n",
    "# Define the DFFNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(hidden_layer1_neurons, input_dim=input_layer_size, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(hidden_layer2_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(hidden_layer2_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer (binary classification with softmax activation)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae1cd1",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2UhxrWTaWS5L",
   "metadata": {
    "id": "2UhxrWTaWS5L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# CNN Model:\n",
    "# Design a convolutional neural network architecture.\n",
    "# Convert each sentence into a k-dimensional word representation using pre-trained word embeddings.\n",
    "# Concatenate word representations to obtain fixed-size input.\n",
    "# Define the number of filters in the convolutional layer and the size of the filter.\n",
    "# Utilize rectified linear units as the activation function for the convolutional layer.\n",
    "# Implement max pooling to downsample the feature maps.\n",
    "# Use softmax activation in the output layer for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LR1mmAZvWnUE",
   "metadata": {
    "id": "LR1mmAZvWnUE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100  # Dimensionality of word embeddings\n",
    "max_len = 100  # Maximum sequence length (number of words in a review)\n",
    "num_filters = 128  # Number of filters in the convolutional layer\n",
    "filter_size = 5  # Size of the filter window\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "vocab_size=1000\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_shape=(max_len,)))\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu'))\n",
    "\n",
    "# Max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Global max pooling layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GWKarpEAKPZh",
   "metadata": {
    "id": "GWKarpEAKPZh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. Model Training:\n",
    "# Split the dataset into training, validation, and testing sets.\n",
    "# Use mini-batch gradient descent for training the DFFNN model.\n",
    "# Apply stochastic gradient descent for training the CNN model.\n",
    "# Tune hyperparameters such as learning rate, dropout rate, and number of iterations using validation set performance.\n",
    "# Monitor training progress and adjust hyperparameters as needed to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_yzlSZLFKSiV",
   "metadata": {
    "id": "_yzlSZLFKSiV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I'm trying idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frIkzvgAKkLt",
   "metadata": {
    "id": "frIkzvgAKkLt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Evaluation:\n",
    "# Evaluate the trained models on the test set to measure their performance.\n",
    "# Compute metrics such as accuracy, precision, recall, and F1-score to assess the models' effectiveness in detecting fake reviews.\n",
    "# Compare the performance of the DFFNN and CNN models with baseline methods and state-of-the-art approaches mentioned in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ui2dnQRUKmuI",
   "metadata": {
    "id": "Ui2dnQRUKmuI",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc91d9",
   "metadata": {
    "id": "7dfc91d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. Optimization and Fine-tuning:\n",
    "# Experiment with different model architectures, hyperparameters, and training strategies to improve performance.\n",
    "# Consider techniques such as ensemble learning or transfer learning to further enhance model accuracy.\n",
    "# Fine-tune the models based on insights gained from initial evaluations and analyses.\n",
    "# By following these steps, you can implement the proposed DFFNN and CNN models for fake review detection based on the ideas presented in the paper. Remember to document your process thoroughly and validate your results to ensure the reliability and reproducibility of your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KVgNU8UBWIcq",
   "metadata": {
    "id": "KVgNU8UBWIcq",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WhfKRIng2i7Z",
   "metadata": {
    "id": "WhfKRIng2i7Z",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
