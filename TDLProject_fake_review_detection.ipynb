{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5v7BSveQWJz9",
   "metadata": {
    "id": "5v7BSveQWJz9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing:\n",
    "# Collect a dataset of consumer reviews labeled as fake or truthful.\n",
    "# Preprocess the text data by removing stop words, special symbols, and lowercasing the text.\n",
    "# Extract emotion features from the reviews using lexicon-based methods.\n",
    "# Tokenize the text into unigrams, bigrams, and trigrams.\n",
    "# Calculate tf.idf weights for the n-grams.\n",
    "# Pre-train word embeddings using the Skip-Gram model on a large corpus of text data (e.g., Amazon reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2efe4279",
   "metadata": {
    "id": "2efe4279",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7c9ac51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "a7c9ac51",
    "outputId": "caec03aa-6e88-429f-92e7-8e3f2585e859",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21000 entries, 0 to 20999\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   DOC_ID             21000 non-null  int64 \n",
      " 1   LABEL              21000 non-null  object\n",
      " 2   RATING             21000 non-null  int64 \n",
      " 3   VERIFIED_PURCHASE  21000 non-null  object\n",
      " 4   PRODUCT_CATEGORY   21000 non-null  object\n",
      " 5   PRODUCT_ID         21000 non-null  object\n",
      " 6   PRODUCT_TITLE      21000 non-null  object\n",
      " 7   REVIEW_TITLE       21000 non-null  object\n",
      " 8   REVIEW_TEXT        21000 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# The reviews are labelled as fake or real (in the dataset theyâ€™re mapped fake (label1) or real (label2)).\n",
    "# https://medium.com/@lievgarcia/deception-on-amazon-c1e30d977cfd\n",
    "\n",
    "df = pd.read_csv(\"../Fake-Amazon-Review-Detection/amazon_reviews.txt\", sep = \"\\t\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2671c0eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "2671c0eb",
    "outputId": "8c0c3610-05ec-4318-860d-4ccb97f1a9a7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_ID</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>RATING</th>\n",
       "      <th>VERIFIED_PURCHASE</th>\n",
       "      <th>PRODUCT_CATEGORY</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODUCT_TITLE</th>\n",
       "      <th>REVIEW_TITLE</th>\n",
       "      <th>REVIEW_TEXT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>PC</td>\n",
       "      <td>B00008NG7N</td>\n",
       "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
       "      <td>useful</td>\n",
       "      <td>When least you think so, this product will sav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>Y</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>B00LH0Y3NM</td>\n",
       "      <td>Note 3 Battery : Stalion Strength Replacement ...</td>\n",
       "      <td>New era for batteries</td>\n",
       "      <td>Lithium batteries are something new introduced...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>Baby</td>\n",
       "      <td>B000I5UZ1Q</td>\n",
       "      <td>Fisher-Price Papasan Cradle Swing, Starlight</td>\n",
       "      <td>doesn't swing very well.</td>\n",
       "      <td>I purchased this swing for my baby. She is 6 m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>B003822IRA</td>\n",
       "      <td>Casio MS-80B Standard Function Desktop Calculator</td>\n",
       "      <td>Great computing!</td>\n",
       "      <td>I was looking for an inexpensive desk calcolat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>__label1__</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00PWSAXAM</td>\n",
       "      <td>Shine Whitening - Zero Peroxide Teeth Whitenin...</td>\n",
       "      <td>Only use twice a week</td>\n",
       "      <td>I only use it twice a week and the results are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOC_ID       LABEL  RATING VERIFIED_PURCHASE PRODUCT_CATEGORY  PRODUCT_ID  \\\n",
       "0       1  __label1__       4                 N               PC  B00008NG7N   \n",
       "1       2  __label1__       4                 Y         Wireless  B00LH0Y3NM   \n",
       "2       3  __label1__       3                 N             Baby  B000I5UZ1Q   \n",
       "3       4  __label1__       4                 N  Office Products  B003822IRA   \n",
       "4       5  __label1__       4                 N           Beauty  B00PWSAXAM   \n",
       "\n",
       "                                       PRODUCT_TITLE  \\\n",
       "0        Targus PAUK10U Ultra Mini USB Keypad, Black   \n",
       "1  Note 3 Battery : Stalion Strength Replacement ...   \n",
       "2       Fisher-Price Papasan Cradle Swing, Starlight   \n",
       "3  Casio MS-80B Standard Function Desktop Calculator   \n",
       "4  Shine Whitening - Zero Peroxide Teeth Whitenin...   \n",
       "\n",
       "               REVIEW_TITLE  \\\n",
       "0                    useful   \n",
       "1     New era for batteries   \n",
       "2  doesn't swing very well.   \n",
       "3          Great computing!   \n",
       "4     Only use twice a week   \n",
       "\n",
       "                                         REVIEW_TEXT  target  \n",
       "0  When least you think so, this product will sav...       0  \n",
       "1  Lithium batteries are something new introduced...       0  \n",
       "2  I purchased this swing for my baby. She is 6 m...       0  \n",
       "3  I was looking for an inexpensive desk calcolat...       0  \n",
       "4  I only use it twice a week and the results are...       0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping binary output label to numeric values 0 (fake review) and 1 (real review)\n",
    "df['target'] = pd.factorize(df['LABEL'])[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5867af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5867af1",
    "outputId": "2bd2feb9-5d36-44be-8a80-b80cd775bf8e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500 10500\n"
     ]
    }
   ],
   "source": [
    "num_fake = len(df[df['target'] == 0])\n",
    "num_real = len(df[df['target'] == 1])\n",
    "\n",
    "print(num_real, num_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b13d7",
   "metadata": {
    "id": "684b13d7"
   },
   "source": [
    "As seen above, the dataset is evenly balanced across both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5189219",
   "metadata": {
    "id": "e5189219"
   },
   "source": [
    "# Review Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aa116b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aa116b5",
    "outputId": "5db481f3-ae91-4711-de84-7b029336df01",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'purchased',\n",
       " 'this',\n",
       " 'swing',\n",
       " 'for',\n",
       " 'my',\n",
       " 'baby',\n",
       " 'she',\n",
       " 'is',\n",
       " '6',\n",
       " 'months',\n",
       " 'now',\n",
       " 'and',\n",
       " 'has',\n",
       " 'pretty',\n",
       " 'much',\n",
       " 'out',\n",
       " 'grown',\n",
       " 'it',\n",
       " 'it',\n",
       " 'is',\n",
       " 'very',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'swing',\n",
       " 'very',\n",
       " 'well',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'though',\n",
       " 'i',\n",
       " 'love',\n",
       " 'the',\n",
       " 'colors',\n",
       " 'and',\n",
       " 'it',\n",
       " 'has',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'settings',\n",
       " 'but',\n",
       " 'i',\n",
       " 'don',\n",
       " 't',\n",
       " 'think',\n",
       " 'it',\n",
       " 'was',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'money']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# converting to lowercase and tokenizing\n",
    "review_tokens = [tokenizer.tokenize(review.lower()) for review in df['REVIEW_TEXT']]\n",
    "\n",
    "#removing special characters\n",
    "review_tokens = [[unidecode(token) for token in review if token.isalnum()] for review in review_tokens]\n",
    "review_tokens[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f336868",
   "metadata": {},
   "source": [
    "# Emotion Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f4ca",
   "metadata": {},
   "source": [
    "### Polarity-Based Emotion Representation using OpinionFinder 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa1ab6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "\n",
    "# mounting reviews into individual files for OpinionFinder, creating 10 batches\n",
    "\n",
    "# parent_dir = \"database/docs/amazon_reviews/\"\n",
    "# f_count = 1\n",
    "# count = 0\n",
    "# doclist = \"amazon_reviews_\" + str(f_count) + \".doclist\"\n",
    "# f2 = open(doclist, \"a\")\n",
    "\n",
    "# for i in range(len(review_tokens)):\n",
    "#     fname = parent_dir + \"rev_id_\" + str(i + 1)\n",
    "#     fp = open(fname, 'w')\n",
    "#     review_text = ' '.join(review_tokens[i])\n",
    "#     fp.write(review_text)\n",
    "#     fp.close()\n",
    "    \n",
    "#     if count == 2100:\n",
    "#         f2.close()\n",
    "#         count = 0\n",
    "#         f_count += 1\n",
    "        \n",
    "#         doclist = \"amazon_reviews_\" + str(f_count) + \".doclist\"\n",
    "#         f2 = open(doclist, \"a\")\n",
    "        \n",
    "#     f2.write(fname+\"\\n\")         \n",
    "#     count += 1\n",
    "    \n",
    "# f2.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# commands to execute OpinionFinder2.0\n",
    "\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_1.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_2.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_3.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_4.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_5.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_6.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_7.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_8.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_9.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_10.doclist -d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extracting polarity labels from output file (exp_polarity.txt) and adding to dataset\n",
    "\n",
    "# opinion_finder_pos_count = []\n",
    "# opinion_finder_neg_count = []\n",
    "\n",
    "# parent_dir = \"database/docs/amazon_reviews/rev_id_\"\n",
    "# suffix = \"_auto_anns/exp_polarity.txt\"\n",
    "\n",
    "# for i in range(len(review_tokens)):\n",
    "#     fpath = parent_dir + str(i + 1) + suffix\n",
    "#     f = open(fpath, \"r\")\n",
    "#     content = f.read()\n",
    "#     f.close()\n",
    "    \n",
    "#     opinion_finder_pos_count.append(content.count(\"positive\"))\n",
    "#     opinion_finder_neg_count.append(content.count(\"negative\"))\n",
    "    \n",
    "\n",
    "# df['OPI_FIN_POS'] = opinion_finder_pos_count\n",
    "# df['OPI_FIN_NEG'] = opinion_finder_neg_count\n",
    "# df.to_csv(\"amazon_reviews_with_polarity.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vHy6iP-GUBfX",
   "metadata": {
    "id": "vHy6iP-GUBfX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reference for sentiment analysis with SentiWordNet\n",
    "# don't  run\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('sentiwordnet')\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "# list(swn.senti_synsets('slow'))\n",
    "\n",
    "# sentence='It was a really good day'\n",
    "# from nltk.tag import pos_tag\n",
    "# token = nltk.word_tokenize(sentence)\n",
    "# after_tagging = nltk.pos_tag(token)\n",
    "# print (token)\n",
    "# print (after_tagging)\n",
    "# def penn_to_wn(tag):\n",
    "#     \"\"\"\n",
    "#     Convert between the PennTreebank tags to simple Wordnet tags\n",
    "#     \"\"\"\n",
    "#     if tag.startswith('J'):\n",
    "#         return wn.ADJ\n",
    "#     elif tag.startswith('N'):\n",
    "#         return wn.NOUN\n",
    "#     elif tag.startswith('R'):\n",
    "#         return wn.ADV\n",
    "#     elif tag.startswith('V'):\n",
    "#         return wn.VERB\n",
    "#     return None\n",
    "# sentiment = 0.0\n",
    "# tokens_count = 0\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# for word, tag in after_tagging:\n",
    "#             wn_tag = penn_to_wn(tag)\n",
    "#             if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "#                 continue\n",
    "\n",
    "#             lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "#             if not lemma:\n",
    "#                 continue\n",
    "\n",
    "#             synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "#             if not synsets:\n",
    "#                 continue\n",
    "\n",
    "#             # Take the first sense, the most common\n",
    "#             synset = synsets[0]\n",
    "#             swn_synset = swn.senti_synset(synset.name())\n",
    "#             print(swn_synset)\n",
    "\n",
    "#             sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "#             tokens_count += 1\n",
    "\n",
    "# print (sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbb23c",
   "metadata": {
    "id": "a4dbb23c"
   },
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dd82e90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd82e90",
    "outputId": "e9140072-9cbd-4fe0-8f35-a9faf4c2780a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stop word removal:  ['love', 'the', 'bottle', 'very', 'much', 'br', 'br', 'iVm', 'a', 'tea', 'lover', 'when', 'i', 'saw', 'this', 'bottle', 'i', 'knew', 'that', 'it', 'was', 'what', 'i', 'wanted', 'the', 'shape', 'is', 'fantastic', 'feels', 'nice', 'in', 'your', 'hand', 'perfect', 'size', 'to', 'have', 'in', 'my', 'car', 'i', 'took', 'it', 'all', 'around', 'so', 'i', 'can', 'enjoy', 'my', 'tea', 'everywhere', 'love', 'it', 'very', 'much', 'the', 'one', 'with', 'infuser', 'also', 'looks', 'good']\n",
      "\n",
      "After stop word removal:  ['love', 'bottle', 'much', 'br', 'br', 'iVm', 'tea', 'lover', 'saw', 'bottle', 'knew', 'wanted', 'shape', 'fantastic', 'feels', 'nice', 'hand', 'perfect', 'size', 'car', 'took', 'around', 'enjoy', 'tea', 'everywhere', 'love', 'much', 'one', 'infuser', 'also', 'looks', 'good']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "content_review_tokens = [[token for token in review if token not in stop_words and token.isalnum()] for review in review_tokens]\n",
    "\n",
    "print(\"Before stop word removal: \", review_tokens[6914])\n",
    "print()\n",
    "print(\"After stop word removal: \", content_review_tokens[6914])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fb3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3fb3f4",
    "outputId": "42626a16-1fd4-448b-a269-bc56ff12ce9b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.stem import SnowballStemmer     #porter 2 algorithm\n",
    "# snowball = SnowballStemmer(language = \"english\")\n",
    "\n",
    "# content_review_tokens = [[snowball.stem(token) for token in review] for review in content_review_tokens]\n",
    "# print(content_review_tokens[374])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d6ebd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "183d6ebd",
    "outputId": "a4479ed1-8731-4fe4-b196-bd1d1a8ec1df",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# content_review_tokens = [[lemmatizer.lemmatize(token) for token in review] for review in content_review_tokens]\n",
    "# print(content_review_tokens[374])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88797e4",
   "metadata": {
    "id": "c88797e4"
   },
   "source": [
    "# N-Gram Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db9c59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88db9c59",
    "outputId": "6e7cd9d4-082d-40ba-f9a5-c87d444387aa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "review_text_unigrams = [list(ngrams(tokens, 1)) for tokens in content_review_tokens]\n",
    "review_text_bigrams = [list(ngrams(tokens, 2)) for tokens in content_review_tokens]\n",
    "review_text_trigrams = [list(ngrams(tokens, 3)) for tokens in content_review_tokens]\n",
    "\n",
    "print(review_text_unigrams[374])\n",
    "print(review_text_bigrams[374])\n",
    "print(review_text_trigrams[374])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P0XO5LqymuoJ",
   "metadata": {
    "id": "P0XO5LqymuoJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Train the Skip-Gram model\n",
    "vector_size = 100  # Dimensionality of word embeddings\n",
    "window_size = 5    # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 1      # Minimum frequency count of words to consider when training the model\n",
    "workers = 4        # Number of threads to use while training\n",
    "\n",
    "# Train the Skip-Gram model\n",
    "skipgram_model = Word2Vec(sentences=content_review_tokens,\n",
    "                          vector_size=vector_size,\n",
    "                          window=window_size,\n",
    "                          min_count=min_count,\n",
    "                          workers=workers,\n",
    "                          sg=1)  # sg=1 specifies Skip-Gram model\n",
    "\n",
    "# Save the trained word embeddings\n",
    "skipgram_model.save('skipgram_word_embeddings.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vLMVB3huWNQp",
   "metadata": {
    "id": "vLMVB3huWNQp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Model Architecture Design:\n",
    "\n",
    "# DFFNN Model:\n",
    "# Design a multilayer perceptron neural network with two hidden layers.\n",
    "# Determine the input layer size based on the features extracted in data preprocessing (e.g., 2000 n-grams, 30 emotion features, and word embeddings).\n",
    "# Define the number of neurons in each hidden layer based on a grid search procedure.\n",
    "# Choose rectified linear units as the activation function for the hidden layers.\n",
    "# Implement dropout regularization to prevent overfitting.\n",
    "# Utilize softmax activation in the output layer for binary classification (fake/truthful).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20a698",
   "metadata": {},
   "source": [
    "# DFFNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xRQWjt37WP5T",
   "metadata": {
    "id": "xRQWjt37WP5T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DFFNN Model:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming you have the features extracted in data preprocessing stored in X\n",
    "# and the binary labels in y (0 for fake, 1 for truthful)\n",
    "\n",
    "# Input layer size based on features\n",
    "input_layer_size = len(df.columns)  # Adjust based on the actual number of features\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_layer1_neurons = 128\n",
    "hidden_layer2_neurons = 64\n",
    "dropout_rate = 0.5  # Adjust as needed\n",
    "\n",
    "# Define the DFFNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(hidden_layer1_neurons, input_dim=input_layer_size, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(hidden_layer2_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(hidden_layer2_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer (binary classification with softmax activation)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae1cd1",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2UhxrWTaWS5L",
   "metadata": {
    "id": "2UhxrWTaWS5L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# CNN Model:\n",
    "# Design a convolutional neural network architecture.\n",
    "# Convert each sentence into a k-dimensional word representation using pre-trained word embeddings.\n",
    "# Concatenate word representations to obtain fixed-size input.\n",
    "# Define the number of filters in the convolutional layer and the size of the filter.\n",
    "# Utilize rectified linear units as the activation function for the convolutional layer.\n",
    "# Implement max pooling to downsample the feature maps.\n",
    "# Use softmax activation in the output layer for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LR1mmAZvWnUE",
   "metadata": {
    "id": "LR1mmAZvWnUE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100  # Dimensionality of word embeddings\n",
    "max_len = 100  # Maximum sequence length (number of words in a review)\n",
    "num_filters = 128  # Number of filters in the convolutional layer\n",
    "filter_size = 5  # Size of the filter window\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "vocab_size=1000\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_shape=(max_len,)))\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu'))\n",
    "\n",
    "# Max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Global max pooling layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GWKarpEAKPZh",
   "metadata": {
    "id": "GWKarpEAKPZh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. Model Training:\n",
    "# Split the dataset into training, validation, and testing sets.\n",
    "# Use mini-batch gradient descent for training the DFFNN model.\n",
    "# Apply stochastic gradient descent for training the CNN model.\n",
    "# Tune hyperparameters such as learning rate, dropout rate, and number of iterations using validation set performance.\n",
    "# Monitor training progress and adjust hyperparameters as needed to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_yzlSZLFKSiV",
   "metadata": {
    "id": "_yzlSZLFKSiV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I'm trying idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frIkzvgAKkLt",
   "metadata": {
    "id": "frIkzvgAKkLt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Evaluation:\n",
    "# Evaluate the trained models on the test set to measure their performance.\n",
    "# Compute metrics such as accuracy, precision, recall, and F1-score to assess the models' effectiveness in detecting fake reviews.\n",
    "# Compare the performance of the DFFNN and CNN models with baseline methods and state-of-the-art approaches mentioned in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ui2dnQRUKmuI",
   "metadata": {
    "id": "Ui2dnQRUKmuI",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc91d9",
   "metadata": {
    "id": "7dfc91d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. Optimization and Fine-tuning:\n",
    "# Experiment with different model architectures, hyperparameters, and training strategies to improve performance.\n",
    "# Consider techniques such as ensemble learning or transfer learning to further enhance model accuracy.\n",
    "# Fine-tune the models based on insights gained from initial evaluations and analyses.\n",
    "# By following these steps, you can implement the proposed DFFNN and CNN models for fake review detection based on the ideas presented in the paper. Remember to document your process thoroughly and validate your results to ensure the reliability and reproducibility of your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KVgNU8UBWIcq",
   "metadata": {
    "id": "KVgNU8UBWIcq",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WhfKRIng2i7Z",
   "metadata": {
    "id": "WhfKRIng2i7Z",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
