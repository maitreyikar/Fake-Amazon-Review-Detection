{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5v7BSveQWJz9",
   "metadata": {
    "id": "5v7BSveQWJz9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Data Preprocessing:\n",
    "# Collect a dataset of consumer reviews labeled as fake or truthful.\n",
    "# Preprocess the text data by removing stop words, special symbols, and lowercasing the text.\n",
    "# Extract emotion features from the reviews using lexicon-based methods.\n",
    "# Tokenize the text into unigrams, bigrams, and trigrams.\n",
    "# Calculate tf.idf weights for the n-grams.\n",
    "# Pre-train word embeddings using the Skip-Gram model on a large corpus of text data (e.g., Amazon reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efe4279",
   "metadata": {
    "id": "2efe4279",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from unidecode import unidecode\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c9ac51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "a7c9ac51",
    "outputId": "caec03aa-6e88-429f-92e7-8e3f2585e859",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21000 entries, 0 to 20999\n",
      "Data columns (total 39 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   DOC_ID                21000 non-null  int64  \n",
      " 1   RATING                21000 non-null  int64  \n",
      " 2   VERIFIED_PURCHASE     21000 non-null  int64  \n",
      " 3   PRODUCT_CATEGORY      21000 non-null  object \n",
      " 4   PRODUCT_ID            21000 non-null  object \n",
      " 5   PRODUCT_TITLE         21000 non-null  object \n",
      " 6   REVIEW_TITLE          21000 non-null  object \n",
      " 7   REVIEW_TEXT           21000 non-null  object \n",
      " 8   OPI_FIN_POS           21000 non-null  int64  \n",
      " 9   OPI_FIN_NEG           21000 non-null  int64  \n",
      " 10  BL_POS                21000 non-null  int64  \n",
      " 11  BL_NEG                21000 non-null  int64  \n",
      " 12  AFINN_POS             21000 non-null  float64\n",
      " 13  AFINN_NEG             21000 non-null  float64\n",
      " 14  S140_POS              21000 non-null  float64\n",
      " 15  S140_NEG              21000 non-null  float64\n",
      " 16  SWN_POS               21000 non-null  float64\n",
      " 17  SWN_NEG               21000 non-null  float64\n",
      " 18  NRC_HASH_POS          21000 non-null  float64\n",
      " 19  NRC_HASH_NEG          21000 non-null  float64\n",
      " 20  EMOTICON_POS          21000 non-null  float64\n",
      " 21  EMOTICON_NEG          21000 non-null  float64\n",
      " 22  NRC_ANGER             21000 non-null  int64  \n",
      " 23  NRC_ANTICIPATION      21000 non-null  int64  \n",
      " 24  NRC_DISGUST           21000 non-null  int64  \n",
      " 25  NRC_FEAR              21000 non-null  int64  \n",
      " 26  NRC_JOY               21000 non-null  int64  \n",
      " 27  NRC_SADNESS           21000 non-null  int64  \n",
      " 28  NRC_SURPRISE          21000 non-null  int64  \n",
      " 29  NRC_TRUST             21000 non-null  int64  \n",
      " 30  NRC_EXP_ANGER         21000 non-null  float64\n",
      " 31  NRC_EXP_ANTICIPATION  21000 non-null  float64\n",
      " 32  NRC_EXP_DISGUST       21000 non-null  float64\n",
      " 33  NRC_EXP_FEAR          21000 non-null  float64\n",
      " 34  NRC_EXP_JOY           21000 non-null  float64\n",
      " 35  NRC_EXP_SADNESS       21000 non-null  float64\n",
      " 36  NRC_EXP_SURPRISE      21000 non-null  float64\n",
      " 37  NRC_EXP_TRUST         21000 non-null  float64\n",
      " 38  TARGET                21000 non-null  int64  \n",
      "dtypes: float64(18), int64(16), object(5)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# The reviews are labelled as fake or real (in the dataset they’re mapped fake (label1) or real (label2)).\n",
    "# https://medium.com/@lievgarcia/deception-on-amazon-c1e30d977cfd\n",
    "\n",
    "df = pd.read_csv(\"amazon_reviews_features.txt\", sep = \"\\t\")   \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2671c0eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "2671c0eb",
    "outputId": "8c0c3610-05ec-4318-860d-4ccb97f1a9a7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>VERIFIED_PURCHASE</th>\n",
       "      <th>PRODUCT_CATEGORY</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODUCT_TITLE</th>\n",
       "      <th>REVIEW_TITLE</th>\n",
       "      <th>REVIEW_TEXT</th>\n",
       "      <th>OPI_FIN_POS</th>\n",
       "      <th>OPI_FIN_NEG</th>\n",
       "      <th>...</th>\n",
       "      <th>NRC_TRUST</th>\n",
       "      <th>NRC_EXP_ANGER</th>\n",
       "      <th>NRC_EXP_ANTICIPATION</th>\n",
       "      <th>NRC_EXP_DISGUST</th>\n",
       "      <th>NRC_EXP_FEAR</th>\n",
       "      <th>NRC_EXP_JOY</th>\n",
       "      <th>NRC_EXP_SADNESS</th>\n",
       "      <th>NRC_EXP_SURPRISE</th>\n",
       "      <th>NRC_EXP_TRUST</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "      <td>B00008NG7N</td>\n",
       "      <td>Targus PAUK10U Ultra Mini USB Keypad, Black</td>\n",
       "      <td>useful</td>\n",
       "      <td>When least you think so, this product will sav...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278005</td>\n",
       "      <td>0.751573</td>\n",
       "      <td>0.747909</td>\n",
       "      <td>1.744297</td>\n",
       "      <td>0.865995</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.641720</td>\n",
       "      <td>0.895334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>B00LH0Y3NM</td>\n",
       "      <td>Note 3 Battery : Stalion Strength Replacement ...</td>\n",
       "      <td>New era for batteries</td>\n",
       "      <td>Lithium batteries are something new introduced...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.352953</td>\n",
       "      <td>1.342698</td>\n",
       "      <td>2.456664</td>\n",
       "      <td>4.800844</td>\n",
       "      <td>1.938679</td>\n",
       "      <td>1.385059</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>3.041116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Baby</td>\n",
       "      <td>B000I5UZ1Q</td>\n",
       "      <td>Fisher-Price Papasan Cradle Swing, Starlight</td>\n",
       "      <td>doesn't swing very well.</td>\n",
       "      <td>I purchased this swing for my baby. She is 6 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.549796</td>\n",
       "      <td>0.950917</td>\n",
       "      <td>1.655262</td>\n",
       "      <td>3.158048</td>\n",
       "      <td>1.485825</td>\n",
       "      <td>0.834764</td>\n",
       "      <td>0.803053</td>\n",
       "      <td>1.717241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>B003822IRA</td>\n",
       "      <td>Casio MS-80B Standard Function Desktop Calculator</td>\n",
       "      <td>Great computing!</td>\n",
       "      <td>I was looking for an inexpensive desk calcolat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.425796</td>\n",
       "      <td>0.865427</td>\n",
       "      <td>1.265500</td>\n",
       "      <td>2.410741</td>\n",
       "      <td>0.934545</td>\n",
       "      <td>0.731801</td>\n",
       "      <td>0.654469</td>\n",
       "      <td>1.207135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00PWSAXAM</td>\n",
       "      <td>Shine Whitening - Zero Peroxide Teeth Whitenin...</td>\n",
       "      <td>Only use twice a week</td>\n",
       "      <td>I only use it twice a week and the results are...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803461</td>\n",
       "      <td>1.777630</td>\n",
       "      <td>2.727954</td>\n",
       "      <td>4.902292</td>\n",
       "      <td>2.320584</td>\n",
       "      <td>1.053444</td>\n",
       "      <td>1.218491</td>\n",
       "      <td>2.612396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00686HNUK</td>\n",
       "      <td>Tobacco Pipe Stand - Fold-away Portable - Ligh...</td>\n",
       "      <td>not sure</td>\n",
       "      <td>I'm not sure what this is supposed to be but I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.454425</td>\n",
       "      <td>0.956854</td>\n",
       "      <td>1.486316</td>\n",
       "      <td>3.011503</td>\n",
       "      <td>1.378531</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.676561</td>\n",
       "      <td>1.370321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Toys</td>\n",
       "      <td>B00NUG865W</td>\n",
       "      <td>ESPN 2-Piece Table Tennis</td>\n",
       "      <td>PING PONG TABLE GREAT FOR YOUTHS AND FAMILY</td>\n",
       "      <td>Pleased with ping pong table. 11 year old and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.534051</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>1.283938</td>\n",
       "      <td>2.636138</td>\n",
       "      <td>2.005626</td>\n",
       "      <td>0.822033</td>\n",
       "      <td>0.778795</td>\n",
       "      <td>1.413699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>B00QUL8VX6</td>\n",
       "      <td>Abundant Health 25% Vitamin C Serum with Vitam...</td>\n",
       "      <td>Great vitamin C serum</td>\n",
       "      <td>Great vitamin C serum... I really like the oil...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.620149</td>\n",
       "      <td>0.844568</td>\n",
       "      <td>1.557884</td>\n",
       "      <td>2.430153</td>\n",
       "      <td>1.475274</td>\n",
       "      <td>0.690926</td>\n",
       "      <td>0.615871</td>\n",
       "      <td>1.065018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B004YHKVCM</td>\n",
       "      <td>PODS Spring Meadow HE Turbo Laundry Detergent ...</td>\n",
       "      <td>wonderful detergent.</td>\n",
       "      <td>I've used tide pods laundry detergent for many...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.188430</td>\n",
       "      <td>0.760328</td>\n",
       "      <td>0.602009</td>\n",
       "      <td>0.995714</td>\n",
       "      <td>1.831817</td>\n",
       "      <td>0.328094</td>\n",
       "      <td>0.450988</td>\n",
       "      <td>1.043719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00H4IBD0M</td>\n",
       "      <td>Sheer TEST, Best Testosterone Booster Suppleme...</td>\n",
       "      <td>WARNING: do not waste your money on this</td>\n",
       "      <td>Everybody wants to fall for their promises. Bu...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.712392</td>\n",
       "      <td>1.183724</td>\n",
       "      <td>1.634331</td>\n",
       "      <td>2.929736</td>\n",
       "      <td>1.864249</td>\n",
       "      <td>0.834775</td>\n",
       "      <td>0.854013</td>\n",
       "      <td>2.268477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00VQNLQL2</td>\n",
       "      <td>#1 Most Potent Male Performance Enhancement Su...</td>\n",
       "      <td>Unfortunately they didn't work for me.</td>\n",
       "      <td>Unfortunately they didn't work for me. They ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334876</td>\n",
       "      <td>0.401599</td>\n",
       "      <td>0.878173</td>\n",
       "      <td>1.736902</td>\n",
       "      <td>0.498598</td>\n",
       "      <td>0.901945</td>\n",
       "      <td>0.325767</td>\n",
       "      <td>0.747812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00IWOJS9A</td>\n",
       "      <td>Proctor Silex K2070YA Electric Kettle, 1-Liter</td>\n",
       "      <td>She said that it is easy to use</td>\n",
       "      <td>This kettle is a gift for my daughter. She sai...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324574</td>\n",
       "      <td>0.547389</td>\n",
       "      <td>1.192528</td>\n",
       "      <td>1.717236</td>\n",
       "      <td>1.271854</td>\n",
       "      <td>0.377434</td>\n",
       "      <td>0.443841</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00JEGSZCQ</td>\n",
       "      <td>Stainless Steel Tumbler with Straw- Hot and Co...</td>\n",
       "      <td>Works great</td>\n",
       "      <td>I love this tumbler! Keeps drinks warm or cold...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.636305</td>\n",
       "      <td>1.081323</td>\n",
       "      <td>1.883448</td>\n",
       "      <td>3.676488</td>\n",
       "      <td>1.842977</td>\n",
       "      <td>0.866172</td>\n",
       "      <td>0.924723</td>\n",
       "      <td>1.698857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B000TD0PXU</td>\n",
       "      <td>Now Foods Mood Support With St Johns Wort Veg-...</td>\n",
       "      <td>Just OK</td>\n",
       "      <td>Only giving this 3 stars because it is so chea...</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.935641</td>\n",
       "      <td>1.013345</td>\n",
       "      <td>1.746132</td>\n",
       "      <td>4.487308</td>\n",
       "      <td>1.418162</td>\n",
       "      <td>1.793939</td>\n",
       "      <td>0.906286</td>\n",
       "      <td>1.089468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>B0019QITHK</td>\n",
       "      <td>Boss Caressoft Medical Stool, black</td>\n",
       "      <td>Color Is Accurate</td>\n",
       "      <td>I bought 2 of these in brown for my island. Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.197308</td>\n",
       "      <td>1.415868</td>\n",
       "      <td>2.843604</td>\n",
       "      <td>4.882948</td>\n",
       "      <td>2.097050</td>\n",
       "      <td>1.740008</td>\n",
       "      <td>1.056396</td>\n",
       "      <td>1.935327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>B004WODP20</td>\n",
       "      <td>Sony MDRZX100  ZX Series Stereo Headphones (Bl...</td>\n",
       "      <td>Price is right!</td>\n",
       "      <td>wonderful headphones had them now for 5 months...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.399982</td>\n",
       "      <td>1.124832</td>\n",
       "      <td>1.304398</td>\n",
       "      <td>2.206281</td>\n",
       "      <td>2.358863</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.642908</td>\n",
       "      <td>1.640175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Camera</td>\n",
       "      <td>B00E56WY18</td>\n",
       "      <td>Falcon Zero F360 HD DVR Dual Dash Cam, Rear Vi...</td>\n",
       "      <td>Good design, but a recommendation</td>\n",
       "      <td>Video quality if superb, fits just fine, looks...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.755200</td>\n",
       "      <td>1.301941</td>\n",
       "      <td>3.159305</td>\n",
       "      <td>6.056989</td>\n",
       "      <td>1.851407</td>\n",
       "      <td>1.906313</td>\n",
       "      <td>1.376321</td>\n",
       "      <td>2.025006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>B000HE8NK8</td>\n",
       "      <td>Barnett Diablo Slingshot</td>\n",
       "      <td>good enough</td>\n",
       "      <td>As sling shots go, this is good enough for my ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225442</td>\n",
       "      <td>0.744401</td>\n",
       "      <td>0.541071</td>\n",
       "      <td>1.249774</td>\n",
       "      <td>0.926661</td>\n",
       "      <td>0.368171</td>\n",
       "      <td>0.407661</td>\n",
       "      <td>0.974472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00FJKEGRY</td>\n",
       "      <td>Cuisavour's Deluxe Spiral Slicer with Japanese...</td>\n",
       "      <td>I just wish that there were guides and manuals...</td>\n",
       "      <td>just to be fair, i really believe that this is...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.402041</td>\n",
       "      <td>2.540544</td>\n",
       "      <td>3.384171</td>\n",
       "      <td>7.640069</td>\n",
       "      <td>3.193300</td>\n",
       "      <td>2.596984</td>\n",
       "      <td>2.101588</td>\n",
       "      <td>3.647735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00KNXIDH6</td>\n",
       "      <td>100 Tablet CleanGuard Nightguard Cleaner</td>\n",
       "      <td>and he was satisfied with this</td>\n",
       "      <td>These tablets are especially helpful if you us...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.269060</td>\n",
       "      <td>0.574301</td>\n",
       "      <td>0.759899</td>\n",
       "      <td>1.490461</td>\n",
       "      <td>0.895358</td>\n",
       "      <td>0.391820</td>\n",
       "      <td>0.332547</td>\n",
       "      <td>1.435454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Home</td>\n",
       "      <td>B00LWRZFAA</td>\n",
       "      <td>Birds Flying Black Tree Branches Wall Sticker ...</td>\n",
       "      <td>best of money value</td>\n",
       "      <td>Looking decent as same shown in photos Thank Y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217055</td>\n",
       "      <td>1.138700</td>\n",
       "      <td>0.462045</td>\n",
       "      <td>1.267491</td>\n",
       "      <td>1.895261</td>\n",
       "      <td>0.412312</td>\n",
       "      <td>0.472833</td>\n",
       "      <td>1.326889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00B2YGB9M</td>\n",
       "      <td>Garcinia Cambogia Pure Extract Supplement, 80%...</td>\n",
       "      <td>It's harder to lose weight the older you get</td>\n",
       "      <td>I find that the older I get, the harder it is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.947856</td>\n",
       "      <td>1.916179</td>\n",
       "      <td>2.591848</td>\n",
       "      <td>5.139749</td>\n",
       "      <td>2.291677</td>\n",
       "      <td>1.623496</td>\n",
       "      <td>1.270700</td>\n",
       "      <td>2.244886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Camera</td>\n",
       "      <td>B004TJ6JH6</td>\n",
       "      <td>NEEWER® 160 LED CN-160 Dimmable Ultra High Pow...</td>\n",
       "      <td>So easy to use!!</td>\n",
       "      <td>I have had my camera for about 2 weeks now. Th...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249888</td>\n",
       "      <td>0.760534</td>\n",
       "      <td>0.671024</td>\n",
       "      <td>1.319332</td>\n",
       "      <td>1.366421</td>\n",
       "      <td>0.406517</td>\n",
       "      <td>0.549294</td>\n",
       "      <td>0.930137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B00OBDRLVS</td>\n",
       "      <td>NatureWise Garcinia Cambogia Extract (Not Synt...</td>\n",
       "      <td>Stay Away And Don't Buy It</td>\n",
       "      <td>It is highly recommended not to buy this produ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.567984</td>\n",
       "      <td>0.557856</td>\n",
       "      <td>1.589391</td>\n",
       "      <td>1.700441</td>\n",
       "      <td>0.745282</td>\n",
       "      <td>0.850759</td>\n",
       "      <td>0.356645</td>\n",
       "      <td>1.203154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>PC</td>\n",
       "      <td>B002RWJGHM</td>\n",
       "      <td>Thermaltake Power Supply 240-Pin 600 Power Sup...</td>\n",
       "      <td>good supply</td>\n",
       "      <td>The actual power supply is good, but the cable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.646407</td>\n",
       "      <td>0.816967</td>\n",
       "      <td>1.589215</td>\n",
       "      <td>3.146362</td>\n",
       "      <td>0.919126</td>\n",
       "      <td>0.894577</td>\n",
       "      <td>0.679032</td>\n",
       "      <td>1.614640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B0018KL0YG</td>\n",
       "      <td>Oregano Oil CERTIFIED ORGANIC- Joy of the Moun...</td>\n",
       "      <td>Did you know? Oregano delivers more antioxidan...</td>\n",
       "      <td>My personal physician recommended it and I bou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3.965912</td>\n",
       "      <td>5.531216</td>\n",
       "      <td>8.753177</td>\n",
       "      <td>15.869369</td>\n",
       "      <td>9.658683</td>\n",
       "      <td>4.594421</td>\n",
       "      <td>4.262845</td>\n",
       "      <td>8.464074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00004OCO6</td>\n",
       "      <td>OXO Good Grips 6-Piece Measuring Cup Set</td>\n",
       "      <td>Products are great</td>\n",
       "      <td>I love the fact that you can easily read the m...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629154</td>\n",
       "      <td>1.269329</td>\n",
       "      <td>2.383878</td>\n",
       "      <td>4.206231</td>\n",
       "      <td>1.945790</td>\n",
       "      <td>0.887393</td>\n",
       "      <td>0.770967</td>\n",
       "      <td>2.156631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "      <td>B009LO31HK</td>\n",
       "      <td>SVETOL® Green Coffee Bean Extract 50%  Acid - ...</td>\n",
       "      <td>One of the better ones</td>\n",
       "      <td>I have bought several products from this selle...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.465930</td>\n",
       "      <td>0.960728</td>\n",
       "      <td>1.294692</td>\n",
       "      <td>2.559305</td>\n",
       "      <td>1.252352</td>\n",
       "      <td>0.808867</td>\n",
       "      <td>0.785456</td>\n",
       "      <td>1.419068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Home</td>\n",
       "      <td>B001L1R3SO</td>\n",
       "      <td>Soft Heat Luxury Micro-Fleece Low-Voltage Elec...</td>\n",
       "      <td>Better than expected</td>\n",
       "      <td>This is an extremely well made electric blanke...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608126</td>\n",
       "      <td>1.315606</td>\n",
       "      <td>1.956146</td>\n",
       "      <td>3.272838</td>\n",
       "      <td>1.665340</td>\n",
       "      <td>0.988131</td>\n",
       "      <td>0.837273</td>\n",
       "      <td>1.625165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>B00PYF1J24</td>\n",
       "      <td>Cafe Deluxe French Press Coffee Maker &amp; Coffee...</td>\n",
       "      <td>great product for coffee lover that want littl...</td>\n",
       "      <td>great product for coffee lover that want littl...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.316677</td>\n",
       "      <td>0.618768</td>\n",
       "      <td>0.797873</td>\n",
       "      <td>0.984629</td>\n",
       "      <td>1.135390</td>\n",
       "      <td>0.337226</td>\n",
       "      <td>0.312636</td>\n",
       "      <td>0.833146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    DOC_ID  RATING  VERIFIED_PURCHASE        PRODUCT_CATEGORY  PRODUCT_ID  \\\n",
       "0        1       4                  0                      PC  B00008NG7N   \n",
       "1        2       4                  1                Wireless  B00LH0Y3NM   \n",
       "2        3       3                  0                    Baby  B000I5UZ1Q   \n",
       "3        4       4                  0         Office Products  B003822IRA   \n",
       "4        5       4                  0                  Beauty  B00PWSAXAM   \n",
       "5        6       3                  0  Health & Personal Care  B00686HNUK   \n",
       "6        7       4                  0                    Toys  B00NUG865W   \n",
       "7        8       4                  1                  Beauty  B00QUL8VX6   \n",
       "8        9       4                  0  Health & Personal Care  B004YHKVCM   \n",
       "9       10       1                  0  Health & Personal Care  B00H4IBD0M   \n",
       "10      11       1                  0  Health & Personal Care  B00VQNLQL2   \n",
       "11      12       4                  0                 Kitchen  B00IWOJS9A   \n",
       "12      13       4                  0                 Kitchen  B00JEGSZCQ   \n",
       "13      14       4                  0  Health & Personal Care  B000TD0PXU   \n",
       "14      15       4                  0               Furniture  B0019QITHK   \n",
       "15      16       4                  0             Electronics  B004WODP20   \n",
       "16      17       4                  1                  Camera  B00E56WY18   \n",
       "17      18       4                  0                  Sports  B000HE8NK8   \n",
       "18      19       4                  1                 Kitchen  B00FJKEGRY   \n",
       "19      20       4                  1  Health & Personal Care  B00KNXIDH6   \n",
       "20      21       4                  0                    Home  B00LWRZFAA   \n",
       "21      22       4                  1  Health & Personal Care  B00B2YGB9M   \n",
       "22      23       3                  0                  Camera  B004TJ6JH6   \n",
       "23      24       2                  1  Health & Personal Care  B00OBDRLVS   \n",
       "24      25       3                  0                      PC  B002RWJGHM   \n",
       "25      26       4                  0  Health & Personal Care  B0018KL0YG   \n",
       "26      27       4                  0                 Kitchen  B00004OCO6   \n",
       "27      28       4                  1  Health & Personal Care  B009LO31HK   \n",
       "28      29       4                  0                    Home  B001L1R3SO   \n",
       "29      30       4                  1                 Kitchen  B00PYF1J24   \n",
       "\n",
       "                                        PRODUCT_TITLE  \\\n",
       "0         Targus PAUK10U Ultra Mini USB Keypad, Black   \n",
       "1   Note 3 Battery : Stalion Strength Replacement ...   \n",
       "2        Fisher-Price Papasan Cradle Swing, Starlight   \n",
       "3   Casio MS-80B Standard Function Desktop Calculator   \n",
       "4   Shine Whitening - Zero Peroxide Teeth Whitenin...   \n",
       "5   Tobacco Pipe Stand - Fold-away Portable - Ligh...   \n",
       "6                           ESPN 2-Piece Table Tennis   \n",
       "7   Abundant Health 25% Vitamin C Serum with Vitam...   \n",
       "8   PODS Spring Meadow HE Turbo Laundry Detergent ...   \n",
       "9   Sheer TEST, Best Testosterone Booster Suppleme...   \n",
       "10  #1 Most Potent Male Performance Enhancement Su...   \n",
       "11     Proctor Silex K2070YA Electric Kettle, 1-Liter   \n",
       "12  Stainless Steel Tumbler with Straw- Hot and Co...   \n",
       "13  Now Foods Mood Support With St Johns Wort Veg-...   \n",
       "14                Boss Caressoft Medical Stool, black   \n",
       "15  Sony MDRZX100  ZX Series Stereo Headphones (Bl...   \n",
       "16  Falcon Zero F360 HD DVR Dual Dash Cam, Rear Vi...   \n",
       "17                           Barnett Diablo Slingshot   \n",
       "18  Cuisavour's Deluxe Spiral Slicer with Japanese...   \n",
       "19           100 Tablet CleanGuard Nightguard Cleaner   \n",
       "20  Birds Flying Black Tree Branches Wall Sticker ...   \n",
       "21  Garcinia Cambogia Pure Extract Supplement, 80%...   \n",
       "22  NEEWER® 160 LED CN-160 Dimmable Ultra High Pow...   \n",
       "23  NatureWise Garcinia Cambogia Extract (Not Synt...   \n",
       "24  Thermaltake Power Supply 240-Pin 600 Power Sup...   \n",
       "25  Oregano Oil CERTIFIED ORGANIC- Joy of the Moun...   \n",
       "26           OXO Good Grips 6-Piece Measuring Cup Set   \n",
       "27  SVETOL® Green Coffee Bean Extract 50%  Acid - ...   \n",
       "28  Soft Heat Luxury Micro-Fleece Low-Voltage Elec...   \n",
       "29  Cafe Deluxe French Press Coffee Maker & Coffee...   \n",
       "\n",
       "                                         REVIEW_TITLE  \\\n",
       "0                                              useful   \n",
       "1                               New era for batteries   \n",
       "2                            doesn't swing very well.   \n",
       "3                                    Great computing!   \n",
       "4                               Only use twice a week   \n",
       "5                                            not sure   \n",
       "6         PING PONG TABLE GREAT FOR YOUTHS AND FAMILY   \n",
       "7                               Great vitamin C serum   \n",
       "8                                wonderful detergent.   \n",
       "9            WARNING: do not waste your money on this   \n",
       "10             Unfortunately they didn't work for me.   \n",
       "11                    She said that it is easy to use   \n",
       "12                                        Works great   \n",
       "13                                            Just OK   \n",
       "14                                  Color Is Accurate   \n",
       "15                                    Price is right!   \n",
       "16                  Good design, but a recommendation   \n",
       "17                                        good enough   \n",
       "18  I just wish that there were guides and manuals...   \n",
       "19                     and he was satisfied with this   \n",
       "20                                best of money value   \n",
       "21       It's harder to lose weight the older you get   \n",
       "22                                   So easy to use!!   \n",
       "23                         Stay Away And Don't Buy It   \n",
       "24                                        good supply   \n",
       "25  Did you know? Oregano delivers more antioxidan...   \n",
       "26                                 Products are great   \n",
       "27                             One of the better ones   \n",
       "28                               Better than expected   \n",
       "29  great product for coffee lover that want littl...   \n",
       "\n",
       "                                          REVIEW_TEXT  OPI_FIN_POS  \\\n",
       "0   When least you think so, this product will sav...            1   \n",
       "1   Lithium batteries are something new introduced...            0   \n",
       "2   I purchased this swing for my baby. She is 6 m...            3   \n",
       "3   I was looking for an inexpensive desk calcolat...            0   \n",
       "4   I only use it twice a week and the results are...            1   \n",
       "5   I'm not sure what this is supposed to be but I...            1   \n",
       "6   Pleased with ping pong table. 11 year old and ...            1   \n",
       "7   Great vitamin C serum... I really like the oil...            1   \n",
       "8   I've used tide pods laundry detergent for many...            1   \n",
       "9   Everybody wants to fall for their promises. Bu...            2   \n",
       "10  Unfortunately they didn't work for me. They ma...            0   \n",
       "11  This kettle is a gift for my daughter. She sai...            2   \n",
       "12  I love this tumbler! Keeps drinks warm or cold...            8   \n",
       "13  Only giving this 3 stars because it is so chea...            3   \n",
       "14  I bought 2 of these in brown for my island. Th...            1   \n",
       "15  wonderful headphones had them now for 5 months...            4   \n",
       "16  Video quality if superb, fits just fine, looks...            8   \n",
       "17  As sling shots go, this is good enough for my ...            2   \n",
       "18  just to be fair, i really believe that this is...            8   \n",
       "19  These tablets are especially helpful if you us...            2   \n",
       "20  Looking decent as same shown in photos Thank Y...            1   \n",
       "21  I find that the older I get, the harder it is ...            1   \n",
       "22  I have had my camera for about 2 weeks now. Th...            2   \n",
       "23  It is highly recommended not to buy this produ...            1   \n",
       "24  The actual power supply is good, but the cable...            0   \n",
       "25  My personal physician recommended it and I bou...            0   \n",
       "26  I love the fact that you can easily read the m...            3   \n",
       "27  I have bought several products from this selle...            2   \n",
       "28  This is an extremely well made electric blanke...            1   \n",
       "29  great product for coffee lover that want littl...            3   \n",
       "\n",
       "    OPI_FIN_NEG  ...  NRC_TRUST  NRC_EXP_ANGER  NRC_EXP_ANTICIPATION  \\\n",
       "0             0  ...          1       0.278005              0.751573   \n",
       "1             0  ...          0       1.352953              1.342698   \n",
       "2             9  ...          3       0.549796              0.950917   \n",
       "3             0  ...          1       0.425796              0.865427   \n",
       "4             0  ...          0       0.803461              1.777630   \n",
       "5             1  ...          1       0.454425              0.956854   \n",
       "6             0  ...          2       0.534051              0.973214   \n",
       "7             3  ...          2       0.620149              0.844568   \n",
       "8             0  ...          0       0.188430              0.760328   \n",
       "9             1  ...          4       0.712392              1.183724   \n",
       "10            0  ...          0       0.334876              0.401599   \n",
       "11            1  ...          0       0.324574              0.547389   \n",
       "12            2  ...          2       0.636305              1.081323   \n",
       "13           18  ...          5       1.935641              1.013345   \n",
       "14            1  ...          1       1.197308              1.415868   \n",
       "15            0  ...          1       0.399982              1.124832   \n",
       "16            4  ...          2       1.755200              1.301941   \n",
       "17            0  ...          1       0.225442              0.744401   \n",
       "18            3  ...          2       1.402041              2.540544   \n",
       "19            0  ...          2       0.269060              0.574301   \n",
       "20            1  ...          0       0.217055              1.138700   \n",
       "21            3  ...          3       0.947856              1.916179   \n",
       "22            2  ...          0       0.249888              0.760534   \n",
       "23            0  ...          0       0.567984              0.557856   \n",
       "24            0  ...          2       0.646407              0.816967   \n",
       "25            0  ...          7       3.965912              5.531216   \n",
       "26            2  ...          1       0.629154              1.269329   \n",
       "27            0  ...          2       0.465930              0.960728   \n",
       "28            2  ...          1       0.608126              1.315606   \n",
       "29            0  ...          2       0.316677              0.618768   \n",
       "\n",
       "    NRC_EXP_DISGUST  NRC_EXP_FEAR  NRC_EXP_JOY  NRC_EXP_SADNESS  \\\n",
       "0          0.747909      1.744297     0.865995         0.314947   \n",
       "1          2.456664      4.800844     1.938679         1.385059   \n",
       "2          1.655262      3.158048     1.485825         0.834764   \n",
       "3          1.265500      2.410741     0.934545         0.731801   \n",
       "4          2.727954      4.902292     2.320584         1.053444   \n",
       "5          1.486316      3.011503     1.378531         0.767059   \n",
       "6          1.283938      2.636138     2.005626         0.822033   \n",
       "7          1.557884      2.430153     1.475274         0.690926   \n",
       "8          0.602009      0.995714     1.831817         0.328094   \n",
       "9          1.634331      2.929736     1.864249         0.834775   \n",
       "10         0.878173      1.736902     0.498598         0.901945   \n",
       "11         1.192528      1.717236     1.271854         0.377434   \n",
       "12         1.883448      3.676488     1.842977         0.866172   \n",
       "13         1.746132      4.487308     1.418162         1.793939   \n",
       "14         2.843604      4.882948     2.097050         1.740008   \n",
       "15         1.304398      2.206281     2.358863         0.589625   \n",
       "16         3.159305      6.056989     1.851407         1.906313   \n",
       "17         0.541071      1.249774     0.926661         0.368171   \n",
       "18         3.384171      7.640069     3.193300         2.596984   \n",
       "19         0.759899      1.490461     0.895358         0.391820   \n",
       "20         0.462045      1.267491     1.895261         0.412312   \n",
       "21         2.591848      5.139749     2.291677         1.623496   \n",
       "22         0.671024      1.319332     1.366421         0.406517   \n",
       "23         1.589391      1.700441     0.745282         0.850759   \n",
       "24         1.589215      3.146362     0.919126         0.894577   \n",
       "25         8.753177     15.869369     9.658683         4.594421   \n",
       "26         2.383878      4.206231     1.945790         0.887393   \n",
       "27         1.294692      2.559305     1.252352         0.808867   \n",
       "28         1.956146      3.272838     1.665340         0.988131   \n",
       "29         0.797873      0.984629     1.135390         0.337226   \n",
       "\n",
       "    NRC_EXP_SURPRISE  NRC_EXP_TRUST  TARGET  \n",
       "0           0.641720       0.895334       0  \n",
       "1           0.863584       3.041116       0  \n",
       "2           0.803053       1.717241       0  \n",
       "3           0.654469       1.207135       0  \n",
       "4           1.218491       2.612396       0  \n",
       "5           0.676561       1.370321       0  \n",
       "6           0.778795       1.413699       0  \n",
       "7           0.615871       1.065018       0  \n",
       "8           0.450988       1.043719       0  \n",
       "9           0.854013       2.268477       0  \n",
       "10          0.325767       0.747812       0  \n",
       "11          0.443841       0.981651       0  \n",
       "12          0.924723       1.698857       0  \n",
       "13          0.906286       1.089468       0  \n",
       "14          1.056396       1.935327       0  \n",
       "15          0.642908       1.640175       0  \n",
       "16          1.376321       2.025006       0  \n",
       "17          0.407661       0.974472       0  \n",
       "18          2.101588       3.647735       0  \n",
       "19          0.332547       1.435454       0  \n",
       "20          0.472833       1.326889       0  \n",
       "21          1.270700       2.244886       0  \n",
       "22          0.549294       0.930137       0  \n",
       "23          0.356645       1.203154       0  \n",
       "24          0.679032       1.614640       0  \n",
       "25          4.262845       8.464074       0  \n",
       "26          0.770967       2.156631       0  \n",
       "27          0.785456       1.419068       0  \n",
       "28          0.837273       1.625165       0  \n",
       "29          0.312636       0.833146       0  \n",
       "\n",
       "[30 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping binary output label to numeric values 0 (fake review) and 1 (real review)\n",
    "\n",
    "# df['TARGET'] = pd.factorize(df['LABEL'])[0]\n",
    "# df['VERIFIED_PURCHASE'] = pd.factorize(df['VERIFIED_PURCHASE'])[0]   #Y -> 1, N -> 0\n",
    "\n",
    "# df.drop([\"target\", \"LABEL\"], inplace = True, axis = 1)\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5867af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5867af1",
    "outputId": "2bd2feb9-5d36-44be-8a80-b80cd775bf8e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500 10500\n"
     ]
    }
   ],
   "source": [
    "num_fake = len(df[df['TARGET'] == 0])\n",
    "num_real = len(df[df['TARGET'] == 1])\n",
    "\n",
    "print(num_real, num_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b13d7",
   "metadata": {
    "id": "684b13d7"
   },
   "source": [
    "As seen above, the dataset is evenly balanced across both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5189219",
   "metadata": {
    "id": "e5189219"
   },
   "source": [
    "# Review Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa116b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aa116b5",
    "outputId": "5db481f3-ae91-4711-de84-7b029336df01",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when least you think so this product will save the day just keep it around just in case you need it for something'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# converting to lowercase and tokenizing\n",
    "review_tokens = [tokenizer.tokenize(review.lower()) for review in df['REVIEW_TEXT']]\n",
    "\n",
    "#removing special characters\n",
    "review_tokens = [[unidecode(token) for token in review if token.isalnum()] for review in review_tokens]\n",
    "\" \".join(review_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f336868",
   "metadata": {},
   "source": [
    "# Emotion Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f4ca",
   "metadata": {},
   "source": [
    "### Polarity: OpinionFinder 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa1ab6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "\n",
    "# OpinionFinder2.0: Tags words with polarity (pos/neg)\n",
    "# Used t o develop two features: OPI_FIN_POS and OPI_FIN_NEG\n",
    "# defined as the number of words that corresponding to each polarity respectively, per review\n",
    "\n",
    "# parent_dir = \"database/docs/amazon_reviews/\"\n",
    "# f_count = 1\n",
    "# count = 0\n",
    "# doclist = \"amazon_reviews_\" + str(f_count) + \".doclist\"\n",
    "# f2 = open(doclist, \"a\")\n",
    "\n",
    "# for i in range(len(review_tokens)):\n",
    "#     fname = parent_dir + \"rev_id_\" + str(i + 1)\n",
    "#     fp = open(fname, 'w')\n",
    "#     review_text = ' '.join(review_tokens[i])\n",
    "#     fp.write(review_text)\n",
    "#     fp.close()\n",
    "    \n",
    "#     if count == 2100:\n",
    "#         f2.close()\n",
    "#         count = 0\n",
    "#         f_count += 1\n",
    "        \n",
    "#         doclist = \"amazon_reviews_\" + str(f_count) + \".doclist\"\n",
    "#         f2 = open(doclist, \"a\")\n",
    "        \n",
    "#     f2.write(fname+\"\\n\")         \n",
    "#     count += 1\n",
    "    \n",
    "# f2.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# commands to execute OpinionFinder2.0\n",
    "\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_1.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_2.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_3.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_4.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_5.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_6.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_7.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_8.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_9.doclist -d\n",
    "# !java -Xmx1g -classpath lib\\weka.jar;lib\\stanford-postagger.jar;opinionfinder.jar opin.main.RunOpinionFinder amazon_reviews_10.doclist -d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extracting polarity labels from output file (exp_polarity.txt) and adding to dataset\n",
    "\n",
    "# opinion_finder_pos_count = []\n",
    "# opinion_finder_neg_count = []\n",
    "\n",
    "# parent_dir = \"database/docs/amazon_reviews/rev_id_\"\n",
    "# suffix = \"_auto_anns/exp_polarity.txt\"\n",
    "\n",
    "# for i in range(len(review_tokens)):\n",
    "#     fpath = parent_dir + str(i + 1) + suffix\n",
    "#     f = open(fpath, \"r\")\n",
    "#     content = f.read()\n",
    "#     f.close()\n",
    "    \n",
    "#     opinion_finder_pos_count.append(content.count(\"positive\"))\n",
    "#     opinion_finder_neg_count.append(content.count(\"negative\"))\n",
    "    \n",
    "\n",
    "# df['OPI_FIN_POS'] = opinion_finder_pos_count\n",
    "# df['OPI_FIN_NEG'] = opinion_finder_neg_count\n",
    "# df.to_csv(\"amazon_reviews_with_polarity.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a49628",
   "metadata": {},
   "source": [
    "### Polarity: Bing Liu's Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53362a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS\n",
    "\n",
    "# Bing Liu et al: Opinion Lexicon for positive and negative polarity tagging of words\n",
    "# Used to develop 2 features: BL_POS and BL_NEG\n",
    "# defined as the number of words that corresponding to each polarity respectively, per review\n",
    "\n",
    "# dir_name = \"../bing-liu-opinion-lexicon-English/\"\n",
    "# pos_file = dir_name + \"positive-words.txt\"\n",
    "# neg_file = dir_name + \"negative-words.txt\"\n",
    "\n",
    "# f1 = open(pos_file, \"r\")\n",
    "# f2 = open(neg_file, \"r\")\n",
    "\n",
    "# pos_lexicon = f1.read()\n",
    "# neg_lexicon = f2.read()\n",
    "\n",
    "# f1.close()\n",
    "# f2.close()\n",
    "\n",
    "# bl_pos = []\n",
    "# bl_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     count_pos = 0\n",
    "#     count_neg = 0\n",
    "    \n",
    "#     for token in review:\n",
    "#         if token in pos_lexicon:\n",
    "#             count_pos += 1\n",
    "#         if token in neg_lexicon:\n",
    "#             count_neg += 1\n",
    "            \n",
    "#     bl_pos.append(count_pos)\n",
    "#     bl_neg.append(count_neg)\n",
    "    \n",
    "# print(bl_pos[:15])\n",
    "# print(bl_neg[:15])\n",
    "\n",
    "# df['BL_POS'] = bl_pos\n",
    "# df['BL_NEG'] = bl_neg\n",
    "\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795591c",
   "metadata": {},
   "source": [
    "### Strength Score: AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c48852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip install afinn\n",
    "\n",
    "# # AFINN: Sentiment lexicon for measuring the positive and negative score of a review\n",
    "# # Used to develop 2 features: BL_POS and BL_NEG\n",
    "\n",
    "# from afinn import Afinn\n",
    "# afn = Afinn()\n",
    "\n",
    "# afinn_pos = []\n",
    "# afinn_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     review = \" \".join(review)\n",
    "#     s = afn.score(review)\n",
    "    \n",
    "#     if s > 0:\n",
    "#         afinn_pos.append(s)\n",
    "#         afinn_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         afinn_pos.append(0.0)\n",
    "#         afinn_neg.append(-1 * s)\n",
    "        \n",
    "# print(afinn_pos[:15])\n",
    "# print(afinn_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d493a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['AFINN_POS'] = afinn_pos\n",
    "# df['AFINN_NEG'] = afinn_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570d10b",
   "metadata": {},
   "source": [
    "### Strength Score: Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c04cfb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment140: Lexicon for measuring the positive and negative score of a unigram/bigram\n",
    "# Used to develop 2 features: S140_POS and S140_NEG\n",
    "\n",
    "# import csv\n",
    "\n",
    "# dir_name = \"../Sentiment140-Lexicon/\"\n",
    "# f1 = \"unigrams-pmilexicon.txt\"\n",
    "# f2 = \"bigrams-pmilexicon.txt\"\n",
    "\n",
    "\n",
    "# uni_lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "# bi_lex = pd.read_csv(dir_name + f2, sep = \"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "# uni_dict = dict(zip(uni_lex[\"term\"], uni_lex[\"score\"]))\n",
    "# bi_dict = dict(zip(bi_lex[\"term\"], bi_lex[\"score\"]))\n",
    "\n",
    "# s140_pos = []\n",
    "# s140_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     score = 0\n",
    "#     uni_score = 0\n",
    "#     uni_c = 0\n",
    "        \n",
    "#     for unigram in review:\n",
    "#         if unigram in uni_dict:\n",
    "#             uni_score += uni_dict[unigram]\n",
    "#             uni_c += 1\n",
    "    \n",
    "#     if uni_c > 0:\n",
    "#         uni_score /= uni_c\n",
    "    \n",
    "    \n",
    "#     bi_score = 0\n",
    "#     bi_c = 0\n",
    "    \n",
    "#     bigrams = list(ngrams(review, 2))\n",
    "#     for bigram in bigrams:\n",
    "#         text = \" \".join(bigram)\n",
    "#         if text in bi_dict:\n",
    "#             bi_score += bi_dict[text]\n",
    "#             bi_c += 1\n",
    "    \n",
    "#     if bi_c > 0:\n",
    "#         bi_score /= bi_c\n",
    "    \n",
    "    \n",
    "#     score = (bi_score + uni_score) / (int(uni_c > 0) + int(bi_c > 0))\n",
    "    \n",
    "#     if score > 0:\n",
    "#         s140_pos.append(round(score, 5))\n",
    "#         s140_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         s140_pos.append(0.0)\n",
    "#         s140_neg.append(round(-1 * score, 5))\n",
    "    \n",
    "    \n",
    "# print(s140_pos[:15])\n",
    "# print(s140_neg[:15])\n",
    "\n",
    "# print(s140_pos.count(0.0), s140_neg.count(0.0), len(s140_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fb3ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['S140_POS'] = s140_pos\n",
    "# df['S140_NEG'] = s140_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3bc373",
   "metadata": {},
   "source": [
    "### Strength Score: SentiWordNet3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6df882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentiWordNet3.0: Lexicon for measuring the positive and negative score of synsets in WordNet 3.0\n",
    "# Used to develop 2 features: SWN_POS and SWN_NEG\n",
    "#(for this one, try to make the scores exclusive like the others)\n",
    "\n",
    "\n",
    "# # import nltk\n",
    "# # nltk.download('sentiwordnet')\n",
    "\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# swn_pos = []\n",
    "# swn_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     pos_score = 0\n",
    "#     neg_score = 0\n",
    "#     count = 0\n",
    "    \n",
    "#     for term in review:\n",
    "#         res = swn.senti_synsets(term)\n",
    "        \n",
    "#         try:\n",
    "#             res0 = list(res)[0]\n",
    "#             pos_score += res0.pos_score()\n",
    "#             neg_score += res0.neg_score()\n",
    "#             count += 1\n",
    "            \n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#     if count > 0:\n",
    "#         pos_score = pos_score / count\n",
    "#         neg_score = neg_score / count\n",
    "        \n",
    "#     swn_pos.append(pos_score)\n",
    "#     swn_neg.append(neg_score)\n",
    "\n",
    "# print(swn_pos[:15])\n",
    "# print(swn_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada70b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['SWN_POS'] = swn_pos\n",
    "# df['SWN_NEG'] = swn_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458c3c7",
   "metadata": {},
   "source": [
    "### Strength Score: NRC Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e3f96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRC Hashtag: Lexicon for measuring the positive and negative score of a unigram/bigram based on twitter hashtags\n",
    "# Used to develop 2 features: NRC_HASH_POS and NRC_HASH_NEG\n",
    "\n",
    "\n",
    "# import csv\n",
    "\n",
    "# dir_name = \"../NRC-Hashtag-Sentiment-Lexicon-v0.1/\"\n",
    "# f1 = \"unigrams-pmilexicon.txt\"\n",
    "# f2 = \"bigrams-pmilexicon.txt\"\n",
    "\n",
    "# uni_lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "# bi_lex = pd.read_csv(dir_name + f2, sep = \"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "\n",
    "# uni_dict = dict(zip(uni_lex[\"term\"], uni_lex[\"score\"]))\n",
    "# bi_dict = dict(zip(bi_lex[\"term\"], bi_lex[\"score\"]))\n",
    "\n",
    "# nrc_hash_pos = []\n",
    "# nrc_hash_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     score = 0\n",
    "#     uni_score = 0\n",
    "#     uni_c = 0\n",
    "        \n",
    "#     for unigram in review:\n",
    "#         if unigram in uni_dict:\n",
    "#             uni_score += uni_dict[unigram]\n",
    "#             uni_c += 1\n",
    "    \n",
    "#     if uni_c > 0:\n",
    "#         uni_score /= uni_c\n",
    "    \n",
    "    \n",
    "#     bi_score = 0\n",
    "#     bi_c = 0\n",
    "    \n",
    "#     bigrams = list(ngrams(review, 2))\n",
    "#     for bigram in bigrams:\n",
    "#         text = \" \".join(bigram)\n",
    "#         if text in bi_dict:\n",
    "#             bi_score += bi_dict[text]\n",
    "#             bi_c += 1\n",
    "    \n",
    "#     if bi_c > 0:\n",
    "#         bi_score /= bi_c\n",
    "    \n",
    "    \n",
    "#     score = (bi_score + uni_score) / (int(uni_c > 0) + int(bi_c > 0))\n",
    "    \n",
    "#     if score > 0:\n",
    "#         nrc_hash_pos.append(round(score, 5))\n",
    "#         nrc_hash_neg.append(0.0)\n",
    "        \n",
    "#     else:\n",
    "#         nrc_hash_pos.append(0.0)\n",
    "#         nrc_hash_neg.append(round(-1 * score, 5))\n",
    "    \n",
    "    \n",
    "# print(nrc_hash_pos[:15])\n",
    "# print(nrc_hash_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a65c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['NRC_HASH_POS'] = nrc_hash_pos\n",
    "# df['NRC_HASH_NEG'] = nrc_hash_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917ebeb",
   "metadata": {},
   "source": [
    "### Strength Score: Emoticon - Based Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "062524e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Emoticon Based Lexicon: Lexicon for measuring the positive and negative score of a word based on co-oocurence with emoticons\n",
    "# # Used to develop 2 features: NRC_HASH_POS and NRC_HASH_NEG\n",
    "\n",
    "\n",
    "# import csv\n",
    "\n",
    "# dir_name = \"../references_and_lexicons/ijcai-kbs_emoticon/\"\n",
    "# f1 = \"STS_OR.csv\"\n",
    "\n",
    "# lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "\n",
    "# lex['word'] = lex['word'].str.split('-').str[1]\n",
    "\n",
    "# my_dict = dict([(i,(a,b)) for i, a, b in zip(lex['word'], lex['positive'], lex['negative'])])\n",
    "\n",
    "# emoticon_pos = []\n",
    "# emoticon_neg = []\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     pos_score = 0\n",
    "#     neg_score = 0\n",
    "#     count = 0\n",
    "        \n",
    "#     for word in review:\n",
    "#         if word in my_dict:\n",
    "#             pos_score += my_dict[word][0]\n",
    "#             neg_score += my_dict[word][1]\n",
    "#             count += 1    \n",
    "      \n",
    "#     if count > 0:\n",
    "#         pos_score = round(pos_score / count, 5)\n",
    "#         neg_score = round(neg_score / count, 5)\n",
    "        \n",
    "#     emoticon_pos.append(pos_score)\n",
    "#     emoticon_neg.append(neg_score)\n",
    "\n",
    "# print(emoticon_pos[:15])\n",
    "# print(emoticon_neg[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a3aeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['EMOTICON_POS'] = emoticon_pos\n",
    "# df['EMOTICON_NEG'] = emoticon_neg\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b8120",
   "metadata": {},
   "source": [
    "### NRCLex: Emotion - Based Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95a5708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install NRCLex\n",
    "# from nrclex import NRCLex\n",
    "\n",
    "# my_dict = {\"NRC_ANGER\": [], \n",
    "#            \"NRC_ANTICIPATION\": [],\n",
    "#            \"NRC_DISGUST\": [], \n",
    "#            \"NRC_FEAR\": [], \n",
    "#            \"NRC_JOY\": [], \n",
    "#            \"NRC_SADNESS\": [], \n",
    "#            \"NRC_SURPRISE\": [],\n",
    "#            \"NRC_TRUST\": []}\n",
    "\n",
    "\n",
    "\n",
    "# for review in review_tokens:\n",
    "#     text = \" \".join(review)\n",
    "#     res = NRCLex(text)\n",
    "#     emotion_scores = res.raw_emotion_scores\n",
    "        \n",
    "#     for key in my_dict:\n",
    "#         emotion =  key[4:].lower()\n",
    "#         if emotion in emotion_scores:\n",
    "#             my_dict[key].append(emotion_scores[emotion])\n",
    "#         else:\n",
    "#             my_dict[key].append(0)\n",
    "            \n",
    "# df2 = pd.DataFrame.from_dict(my_dict)\n",
    "# df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7d2a9",
   "metadata": {},
   "source": [
    "### NRCLex Expanded: Emotion - Based Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "896a9da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2780046027161847, 1.3529529085431935, 0.5497961440348219]\n",
      "[0.7515731018557517, 1.3426978241954444, 0.9509167526417303]\n",
      "[0.7479090741959875, 2.4566635439257225, 1.6552617575405246]\n",
      "[1.7442972519267799, 4.800843659469453, 3.1580480013120606]\n",
      "[0.8659946371977526, 1.9386790415546618, 1.4858246032393256]\n",
      "[0.3149466767086976, 1.385059438063274, 0.8347636579443728]\n",
      "[0.6417195280917016, 0.8635843619883152, 0.8030526443970994]\n",
      "[0.8953342091683308, 3.041116462595752, 1.7172406314417379]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "# my_dict = {\"NRC_EXP_ANGER\": [], \n",
    "#            \"NRC_EXP_ANTICIPATION\": [],\n",
    "#            \"NRC_EXP_DISGUST\": [], \n",
    "#            \"NRC_EXP_FEAR\": [], \n",
    "#            \"NRC_EXP_JOY\": [], \n",
    "#            \"NRC_EXP_SADNESS\": [], \n",
    "#            \"NRC_EXP_SURPRISE\": [],\n",
    "#            \"NRC_EXP_TRUST\": []}\n",
    "\n",
    "\n",
    "# dir_name = \"../references_and_lexicons/emo_lex_expanded/\"\n",
    "# f1 = \"w2v-dp-CC-Lex.csv\"\n",
    "\n",
    "# lex = pd.read_csv(dir_name + f1, sep = \"\\t\")\n",
    "\n",
    "# lex = dict([(i,(a,b,c,d,e,f,g,h)) for i, a, b, c, d, e, f, g, h in zip(lex['word'], lex['anger'], lex['anticipation'], lex['disgust'], lex['fear'], lex['joy'], lex['sadness'], lex['surprise'], lex['trust'])])\n",
    "\n",
    "\n",
    "# for review in review_tokens:\n",
    "    \n",
    "#     for key in my_dict:\n",
    "#         my_dict[key].append(0)\n",
    "    \n",
    "#     for word in review:\n",
    "        \n",
    "#         if word in lex:\n",
    "#             i = 0\n",
    "#             for key in my_dict:\n",
    "#                 my_dict[key][-1] += lex[word][i]\n",
    "#                 i += 1\n",
    "        \n",
    "        \n",
    "# for key in my_dict:\n",
    "#     print(my_dict[key][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b68c0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.DataFrame.from_dict(my_dict)\n",
    "# df = pd.concat([df, df2], axis=1)\n",
    "# df.to_csv(\"amazon_reviews_features.txt\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbb23c",
   "metadata": {
    "id": "a4dbb23c"
   },
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd82e90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd82e90",
    "outputId": "e9140072-9cbd-4fe0-8f35-a9faf4c2780a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stop word removal:  ['love', 'the', 'bottle', 'very', 'much', 'br', 'br', 'iVm', 'a', 'tea', 'lover', 'when', 'i', 'saw', 'this', 'bottle', 'i', 'knew', 'that', 'it', 'was', 'what', 'i', 'wanted', 'the', 'shape', 'is', 'fantastic', 'feels', 'nice', 'in', 'your', 'hand', 'perfect', 'size', 'to', 'have', 'in', 'my', 'car', 'i', 'took', 'it', 'all', 'around', 'so', 'i', 'can', 'enjoy', 'my', 'tea', 'everywhere', 'love', 'it', 'very', 'much', 'the', 'one', 'with', 'infuser', 'also', 'looks', 'good']\n",
      "\n",
      "After stop word removal:  ['love', 'bottle', 'much', 'br', 'br', 'iVm', 'tea', 'lover', 'saw', 'bottle', 'knew', 'wanted', 'shape', 'fantastic', 'feels', 'nice', 'hand', 'perfect', 'size', 'car', 'took', 'around', 'enjoy', 'tea', 'everywhere', 'love', 'much', 'one', 'infuser', 'also', 'looks', 'good']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "content_review_tokens = [[token for token in review if token not in stop_words and token.isalnum()] for review in review_tokens]\n",
    "\n",
    "print(\"Before stop word removal: \", review_tokens[6914])\n",
    "print()\n",
    "print(\"After stop word removal: \", content_review_tokens[6914])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fb3f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c3fb3f4",
    "outputId": "42626a16-1fd4-448b-a269-bc56ff12ce9b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STEMMING AND LEMMATIZATION IS NOT REQUIRED I THINK, BECAUSE THE PAPER DOES NOT MENTION IT\n",
    "\n",
    "# from nltk.stem import SnowballStemmer     #porter 2 algorithm\n",
    "# snowball = SnowballStemmer(language = \"english\")\n",
    "# content_review_tokens = [[snowball.stem(token) for token in review] for review in content_review_tokens]\n",
    "# print(content_review_tokens[374])\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# content_review_tokens = [[lemmatizer.lemmatize(token) for token in review] for review in content_review_tokens]\n",
    "# print(content_review_tokens[374])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88797e4",
   "metadata": {
    "id": "c88797e4"
   },
   "source": [
    "# N-Gram Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88db9c59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88db9c59",
    "outputId": "6e7cd9d4-082d-40ba-f9a5-c87d444387aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brushes',), ('soft',), ('soon',), ('first',), ('usage',), ('see',), ('bristles',), ('coming',), ('worth',), ('purchase',), ('falling',), ('generic',), ('product',)]\n",
      "[('brushes', 'soft'), ('soft', 'soon'), ('soon', 'first'), ('first', 'usage'), ('usage', 'see'), ('see', 'bristles'), ('bristles', 'coming'), ('coming', 'worth'), ('worth', 'purchase'), ('purchase', 'falling'), ('falling', 'generic'), ('generic', 'product')]\n",
      "[('brushes', 'soft', 'soon'), ('soft', 'soon', 'first'), ('soon', 'first', 'usage'), ('first', 'usage', 'see'), ('usage', 'see', 'bristles'), ('see', 'bristles', 'coming'), ('bristles', 'coming', 'worth'), ('coming', 'worth', 'purchase'), ('worth', 'purchase', 'falling'), ('purchase', 'falling', 'generic'), ('falling', 'generic', 'product')]\n"
     ]
    }
   ],
   "source": [
    "review_text_unigrams = [list(ngrams(tokens, 1)) for tokens in content_review_tokens]\n",
    "review_text_bigrams = [list(ngrams(tokens, 2)) for tokens in content_review_tokens]\n",
    "review_text_trigrams = [list(ngrams(tokens, 3)) for tokens in content_review_tokens]\n",
    "\n",
    "print(review_text_unigrams[374])\n",
    "print(review_text_bigrams[374])\n",
    "print(review_text_trigrams[374])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742b524",
   "metadata": {},
   "source": [
    "# Feature 1 of the dffnn model, top 2000 n grams according to their tfidf weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b0606991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. br: 683.5375098455908\n",
      "2. great: 485.67385779350764\n",
      "3. one: 405.3027978587271\n",
      "4. good: 404.02605479916946\n",
      "5. like: 379.3174868725596\n",
      "6. product: 339.8697354279675\n",
      "7. love: 328.7302710917243\n",
      "8. really: 328.21438431499655\n",
      "9. use: 319.57330483986317\n",
      "10. well: 316.95741739486925\n",
      "11. would: 311.9556270334749\n",
      "12. quality: 282.7200529018454\n",
      "13. br br: 264.85096957259157\n",
      "14. get: 263.62894814916916\n",
      "15. time: 258.1832341503777\n",
      "16. price: 256.3732679193606\n",
      "17. easy: 240.8427188899182\n",
      "18. bought: 239.4415912565312\n",
      "19. nice: 230.6021592528332\n",
      "20. much: 226.78623763812826\n",
      "21. little: 220.82492781588758\n",
      "22. 34: 213.82089672639142\n",
      "23. also: 212.64020052500717\n",
      "24. works: 207.00762473970457\n",
      "25. recommend: 204.99685522715518\n",
      "26. got: 204.78086642991764\n",
      "27. work: 200.53023332237706\n",
      "28. buy: 198.64007161997776\n",
      "29. perfect: 189.91220569507732\n",
      "30. used: 186.71283986113454\n",
      "31. even: 183.07758287625373\n",
      "32. made: 181.12588697644114\n",
      "33. watch: 173.43014255462052\n",
      "34. looks: 170.94993808906568\n",
      "35. better: 168.57048804632043\n",
      "36. size: 161.012013642325\n",
      "37. fit: 160.00308853857211\n",
      "38. bag: 157.28736122396754\n",
      "39. look: 157.2352291059333\n",
      "40. using: 154.3205722166391\n",
      "41. best: 152.69239384893606\n",
      "42. back: 152.54206839782503\n",
      "43. still: 152.2609129140411\n",
      "44. small: 151.54741581741095\n",
      "45. first: 150.08257038051477\n",
      "46. need: 147.61087607225542\n",
      "47. light: 146.3210579497199\n",
      "48. set: 144.62940723869448\n",
      "49. way: 142.73750941823562\n",
      "50. looking: 142.25997847494028\n",
      "51. make: 141.90515083402582\n",
      "52. put: 141.83168086820598\n",
      "53. happy: 140.75349568425446\n",
      "54. lot: 140.37724879780907\n",
      "55. could: 135.896806403074\n",
      "56. long: 134.17197078741077\n",
      "57. go: 133.42818765937017\n",
      "58. new: 132.6906221156551\n",
      "59. two: 130.80389604959967\n",
      "60. case: 130.03351640358983\n",
      "61. right: 129.80786459220135\n",
      "62. pretty: 128.57541893766094\n",
      "63. old: 125.60618632554815\n",
      "64. thing: 125.58308773617634\n",
      "65. think: 124.62494497677764\n",
      "66. want: 124.53024119654596\n",
      "67. see: 124.31599347131352\n",
      "68. book: 123.58300475312339\n",
      "69. day: 122.55072731308054\n",
      "70. color: 121.40806160123336\n",
      "71. enough: 120.716390248479\n",
      "72. bit: 119.6925208046774\n",
      "73. far: 117.87800486822574\n",
      "74. purchase: 117.71124627662061\n",
      "75. know: 117.10650015536828\n",
      "76. comfortable: 115.38642651198272\n",
      "77. purchased: 115.19330921957625\n",
      "78. sound: 115.06812036612645\n",
      "79. since: 114.09968265308929\n",
      "80. around: 112.76161831400105\n",
      "81. many: 112.34928067649497\n",
      "82. money: 111.7450104644038\n",
      "83. something: 110.49619092960843\n",
      "84. find: 110.45821062914914\n",
      "85. feel: 110.28352760848686\n",
      "86. big: 108.92147147755733\n",
      "87. definitely: 108.88601785135764\n",
      "88. another: 108.47604120163125\n",
      "89. game: 108.46591749350198\n",
      "90. keep: 108.32111213335024\n",
      "91. though: 108.280754157652\n",
      "92. worth: 108.11054569772372\n",
      "93. every: 107.78903346178795\n",
      "94. without: 107.17866132832985\n",
      "95. came: 106.61339266538384\n",
      "96. last: 106.52827488962542\n",
      "97. sure: 105.89001401948858\n",
      "98. loves: 105.16102370345406\n",
      "99. never: 103.99539010467672\n",
      "100. gift: 103.04955397254435\n",
      "101. take: 102.92576276180748\n",
      "102. wear: 102.9111212266236\n",
      "103. makes: 102.49806233818474\n",
      "104. say: 102.45313981261172\n",
      "105. always: 101.77394596773256\n",
      "106. item: 101.30339491270466\n",
      "107. phone: 101.07341046178747\n",
      "108. highly: 100.32451270315099\n",
      "109. thought: 100.31881480914791\n",
      "110. years: 100.2356342311578\n",
      "111. needed: 99.85008682199\n",
      "112. tv: 99.76897374191256\n",
      "113. found: 99.62825250498302\n",
      "114. cheap: 99.54370723277162\n",
      "115. excellent: 98.91627389303166\n",
      "116. fits: 98.22870679463131\n",
      "117. hard: 98.09723623361891\n",
      "118. amazon: 98.08088483096901\n",
      "119. however: 97.80789900490531\n",
      "120. high: 97.18514753374762\n",
      "121. different: 97.00812969142453\n",
      "122. year: 96.83825735325095\n",
      "123. going: 96.34233213912333\n",
      "124. easily: 96.104230459133\n",
      "125. ordered: 96.06261060782217\n",
      "126. box: 94.14914937929446\n",
      "127. water: 93.44749350102212\n",
      "128. battery: 93.30526107046914\n",
      "129. worked: 93.29387267206515\n",
      "130. fine: 92.96627087155181\n",
      "131. camera: 92.77189052194862\n",
      "132. room: 92.36288233769633\n",
      "133. getting: 92.32935811654296\n",
      "134. expected: 92.21042733743785\n",
      "135. awesome: 91.52956061243879\n",
      "136. everything: 91.47053161548459\n",
      "137. tried: 90.92947057857437\n",
      "138. wanted: 90.5285703220308\n",
      "139. picture: 90.50754189769481\n",
      "140. amazing: 90.14148995733744\n",
      "141. give: 90.02986397141986\n",
      "142. son: 88.86301816666202\n",
      "143. read: 88.61154270017303\n",
      "144. seems: 88.3442958958122\n",
      "145. problem: 87.7344908169019\n",
      "146. beautiful: 86.82835358776904\n",
      "147. ever: 86.62301364091789\n",
      "148. play: 86.1503437391117\n",
      "149. perfectly: 85.60945656520411\n",
      "150. quite: 85.00917696483233\n",
      "151. actually: 84.93629227056458\n",
      "152. come: 84.9094796535063\n",
      "153. movie: 84.61809799846154\n",
      "154. bad: 84.59650424079011\n",
      "155. try: 83.25200030428492\n",
      "156. top: 82.99849267520835\n",
      "157. daughter: 82.4100517614752\n",
      "158. received: 82.33645327455072\n",
      "159. job: 82.16657082474299\n",
      "160. fun: 82.16127364613274\n",
      "161. design: 81.98492429621284\n",
      "162. loved: 81.43377919777812\n",
      "163. people: 81.20795274802636\n",
      "164. things: 81.08600509616518\n",
      "165. kids: 81.05705300603238\n",
      "166. able: 80.37221368556236\n",
      "167. fast: 80.13761925576412\n",
      "168. days: 80.13416186352862\n",
      "169. months: 80.01383530744994\n",
      "170. would recommend: 79.62297176059862\n",
      "171. anything: 78.495924986849\n",
      "172. hold: 78.44813049894763\n",
      "173. sturdy: 78.42574671413179\n",
      "174. exactly: 78.32929365095394\n",
      "175. car: 78.18127837479864\n",
      "176. plastic: 77.76827354283672\n",
      "177. weight: 77.10847253759785\n",
      "178. yet: 75.97690543273264\n",
      "179. large: 75.13287526392735\n",
      "180. cute: 74.96478572664002\n",
      "181. working: 73.90662941566639\n",
      "182. home: 73.5849639817153\n",
      "183. dog: 73.17258160114916\n",
      "184. comes: 72.6182450363963\n",
      "185. times: 72.43609879901723\n",
      "186. durable: 72.21908767196147\n",
      "187. together: 71.85794226099412\n",
      "188. took: 71.80680397141649\n",
      "189. disappointed: 71.78238971959901\n",
      "190. hair: 71.68344772805361\n",
      "191. super: 71.18764490986467\n",
      "192. anyone: 71.1755763558143\n",
      "193. husband: 70.59084052026067\n",
      "194. side: 70.54332908852042\n",
      "195. away: 70.46766239293025\n",
      "196. clean: 70.45062199315252\n",
      "197. arrived: 70.05352301790478\n",
      "198. heavy: 69.89415294377109\n",
      "199. screen: 69.73722318363762\n",
      "200. brand: 69.3270266371491\n",
      "201. reviews: 69.30374569376987\n",
      "202. soft: 69.17649048617642\n",
      "203. works great: 68.95530197232566\n",
      "204. highly recommend: 68.95047831262801\n",
      "205. help: 68.87246747142704\n",
      "206. order: 68.80368264776631\n",
      "207. recommended: 68.09531646399854\n",
      "208. less: 68.07472647851525\n",
      "209. house: 68.04411084437275\n",
      "210. buying: 67.51586215449436\n",
      "211. pleased: 66.68237332252782\n",
      "212. almost: 66.50348989077385\n",
      "213. overall: 66.42177993681729\n",
      "214. good quality: 66.28267998472377\n",
      "215. piece: 66.2125210961234\n",
      "216. already: 65.53794572425123\n",
      "217. black: 65.05421696886611\n",
      "218. several: 64.93318041276515\n",
      "219. problems: 64.92985689449087\n",
      "220. inside: 64.68287504413404\n",
      "221. place: 64.51755912023418\n",
      "222. quickly: 64.48651347814341\n",
      "223. wish: 64.43738982749113\n",
      "224. products: 63.95429106428766\n",
      "225. life: 63.92791537402831\n",
      "226. material: 63.88727323122711\n",
      "227. real: 63.52758773004181\n",
      "228. must: 63.485569759677595\n",
      "229. week: 63.458405361681\n",
      "230. easy use: 63.39262621170215\n",
      "231. simple: 63.38221659456498\n",
      "232. baby: 63.14570489789567\n",
      "233. nothing: 62.80240715984939\n",
      "234. skin: 62.670847211226594\n",
      "235. store: 62.45807823395968\n",
      "236. stuff: 62.37714480538968\n",
      "237. said: 62.05987838061837\n",
      "238. friends: 62.02809616067341\n",
      "239. ones: 61.726891135194144\n",
      "240. part: 61.72356172494983\n",
      "241. absolutely: 61.58979965370368\n",
      "242. may: 61.45128010488584\n",
      "243. 10: 61.301610386128885\n",
      "244. colors: 60.322073260085276\n",
      "245. night: 60.231299571963014\n",
      "246. expensive: 60.178656820485564\n",
      "247. strong: 59.81869146239272\n",
      "248. others: 59.815789384766255\n",
      "249. gave: 59.505308064982785\n",
      "250. thanks: 59.48159645484312\n",
      "251. wonderful: 59.147380465004\n",
      "252. power: 59.06972184227883\n",
      "253. friend: 58.92686054724383\n",
      "254. stars: 58.92281905043924\n",
      "255. well made: 58.76039770638456\n",
      "256. family: 58.59411896432981\n",
      "257. cool: 58.576283379042444\n",
      "258. batteries: 58.53532574840821\n",
      "259. cover: 58.45120764273076\n",
      "260. extra: 57.97165653838892\n",
      "261. especially: 57.499326827008126\n",
      "262. started: 57.47700747832667\n",
      "263. seem: 57.381149377052374\n",
      "264. weeks: 57.116320495356554\n",
      "265. end: 56.92616226926493\n",
      "266. great product: 56.723188515950525\n",
      "267. system: 56.687592870460094\n",
      "268. leather: 56.339785170377056\n",
      "269. clear: 56.333978274160934\n",
      "270. month: 56.21358820698538\n",
      "271. carry: 56.20576519868301\n",
      "272. value: 56.10018095186349\n",
      "273. saw: 56.090551399282226\n",
      "274. coffee: 55.76038261360632\n",
      "275. hand: 55.43509988697494\n",
      "276. needs: 55.35831470374895\n",
      "277. full: 55.244482219066576\n",
      "278. hours: 55.19099810045935\n",
      "279. pair: 55.06711541654404\n",
      "280. works well: 54.93231962269127\n",
      "281. cable: 54.920622656085584\n",
      "282. glad: 54.71932834499493\n",
      "283. second: 54.402652976993714\n",
      "284. went: 54.3456638261315\n",
      "285. low: 54.32815204272675\n",
      "286. shipping: 54.22303163649184\n",
      "287. feels: 54.1838395693953\n",
      "288. us: 54.144969951395446\n",
      "289. wife: 54.102883784385504\n",
      "290. liked: 53.91495282165565\n",
      "291. face: 53.91289672824177\n",
      "292. deal: 53.56147680623151\n",
      "293. favorite: 53.43934343824039\n",
      "294. couple: 53.42107748815945\n",
      "295. shoes: 53.09430550227598\n",
      "296. smaller: 52.91942196716956\n",
      "297. bright: 52.86195149715461\n",
      "298. might: 52.678738207137165\n",
      "299. longer: 52.43336630974741\n",
      "300. install: 52.32034564324537\n",
      "301. handle: 52.2233697983994\n",
      "302. plus: 52.15374742204261\n",
      "303. trying: 51.784029684356774\n",
      "304. video: 51.70962969584366\n",
      "305. cost: 51.70058964659109\n",
      "306. next: 51.201841620029455\n",
      "307. thank: 51.103483043445955\n",
      "308. issues: 50.84852545415092\n",
      "309. tool: 50.739015420855495\n",
      "310. looked: 50.63424931341016\n",
      "311. unit: 50.51007087351521\n",
      "312. taste: 50.377125475118234\n",
      "313. ok: 50.26230628102068\n",
      "314. great price: 49.84698033782226\n",
      "315. return: 49.81430330333091\n",
      "316. table: 49.73062071389288\n",
      "317. high quality: 49.6983307871278\n",
      "318. kind: 49.65366272904195\n",
      "319. three: 49.54686860148245\n",
      "320. probably: 49.45209707904566\n",
      "321. year old: 49.2636018072907\n",
      "322. really like: 49.146712885250786\n",
      "323. pack: 49.01178276367333\n",
      "324. stand: 48.97622456969898\n",
      "325. playing: 48.8863551394018\n",
      "326. bottle: 48.86311324710563\n",
      "327. whole: 48.78221953298878\n",
      "328. short: 48.59851437730061\n",
      "329. music: 48.50572051141597\n",
      "330. head: 48.485223106493564\n",
      "331. gets: 48.454781631029505\n",
      "332. smell: 48.33676973061858\n",
      "333. done: 48.26275980076037\n",
      "334. everyone: 48.25188909224252\n",
      "335. maybe: 48.18207705583881\n",
      "336. satisfied: 48.11790500406826\n",
      "337. free: 48.0012183515131\n",
      "338. fact: 47.94597172495339\n",
      "339. original: 47.71058954121164\n",
      "340. ring: 47.6511030460346\n",
      "341. space: 47.533359513906255\n",
      "342. ago: 47.53051309467389\n",
      "343. device: 47.291106856423795\n",
      "344. review: 47.16263177601173\n",
      "345. toy: 46.96859485047824\n",
      "346. bed: 46.95121809596852\n",
      "347. making: 46.92441342307252\n",
      "348. open: 46.88376207258908\n",
      "349. fan: 46.838987405194736\n",
      "350. keeps: 46.80427911352478\n",
      "351. taking: 46.79062992501158\n",
      "352. games: 46.60681018174705\n",
      "353. extremely: 46.278377261876116\n",
      "354. easier: 46.21844479996972\n",
      "355. enjoy: 46.17272764743677\n",
      "356. broke: 46.12635083453179\n",
      "357. christmas: 45.968444454066685\n",
      "358. decided: 45.94096019040507\n",
      "359. instead: 45.90067650672188\n",
      "360. style: 45.567691148532475\n",
      "361. useful: 45.43276618278659\n",
      "362. lights: 45.40880591219914\n",
      "363. least: 45.35675149657784\n",
      "364. much better: 45.24720336391083\n",
      "365. holds: 45.23252574496197\n",
      "366. white: 45.22723795160558\n",
      "367. cut: 45.19036962049598\n",
      "368. air: 44.983619757929155\n",
      "369. although: 44.951973046342346\n",
      "370. mine: 44.80412111684284\n",
      "371. tell: 44.78666814077775\n",
      "372. items: 44.615047069379884\n",
      "373. hope: 44.60389866111209\n",
      "374. either: 44.56810825720862\n",
      "375. half: 44.51935376554802\n",
      "376. minutes: 44.474231151907354\n",
      "377. else: 44.33962425400498\n",
      "378. company: 44.324893090919026\n",
      "379. charge: 44.27931138653061\n",
      "380. service: 44.228743438236954\n",
      "381. change: 44.21648521872026\n",
      "382. hands: 44.19283692805692\n",
      "383. takes: 43.92671692064178\n",
      "384. wrong: 43.7888247609104\n",
      "385. run: 43.73888698586086\n",
      "386. solid: 43.536613705368815\n",
      "387. seen: 43.507315371346145\n",
      "388. replacement: 43.49862001301415\n",
      "389. wearing: 43.19547732877066\n",
      "390. let: 43.18320567873525\n",
      "391. story: 43.11439435402508\n",
      "392. fantastic: 42.98518497014841\n",
      "393. bottom: 42.925622713837214\n",
      "394. package: 42.9246949448431\n",
      "395. computer: 42.7732957591868\n",
      "396. issue: 42.72197184580334\n",
      "397. front: 42.62395482104477\n",
      "398. dvd: 42.48360343123259\n",
      "399. food: 42.45494684441333\n",
      "400. pay: 42.31817081549665\n",
      "401. decent: 42.20134070932629\n",
      "402. quick: 42.172763702883906\n",
      "403. point: 42.13872437437143\n",
      "404. guess: 42.12566200615595\n",
      "405. oil: 42.08778524120782\n",
      "406. says: 42.024919287405204\n",
      "407. expect: 41.95526148215799\n",
      "408. believe: 41.879694759666826\n",
      "409. gives: 41.51136313620274\n",
      "410. thin: 41.435980573669795\n",
      "411. stay: 41.42223555872862\n",
      "412. pictures: 41.01980662844017\n",
      "413. line: 40.87545967214233\n",
      "414. waste: 40.69184459622889\n",
      "415. lots: 40.59419033929401\n",
      "416. left: 40.55253481645268\n",
      "417. show: 40.544526393050354\n",
      "418. built: 40.44716454161606\n",
      "419. kit: 40.13769502826689\n",
      "420. difference: 40.076618522334144\n",
      "421. support: 40.04670795434735\n",
      "422. hot: 40.04140951581779\n",
      "423. long time: 39.933818592444474\n",
      "424. seller: 39.9069231579986\n",
      "425. add: 39.90342647887424\n",
      "426. blue: 39.86269586524051\n",
      "427. totally: 39.859735906931505\n",
      "428. bags: 39.81429108879176\n",
      "429. turn: 39.66607945466999\n",
      "430. start: 39.663455074060195\n",
      "431. close: 39.612686767640206\n",
      "432. star: 39.59825266337076\n",
      "433. online: 39.582547866886074\n",
      "434. goes: 39.57647711047268\n",
      "435. nicely: 39.501553329998295\n",
      "436. handy: 39.49601615397784\n",
      "437. travel: 39.45066836516479\n",
      "438. replace: 39.38896098460367\n",
      "439. experience: 39.338499124869095\n",
      "440. reading: 39.20918791388094\n",
      "441. surprised: 39.104613872523686\n",
      "442. dry: 39.05726799074166\n",
      "443. feet: 39.0157944961984\n",
      "444. metal: 38.89842250551967\n",
      "445. helps: 38.62225249362012\n",
      "446. strap: 38.612996978142725\n",
      "447. laptop: 38.56430899886954\n",
      "448. wall: 38.50867086990292\n",
      "449. looks great: 38.49170280160719\n",
      "450. type: 38.40634960740318\n",
      "451. larger: 38.38499500591398\n",
      "452. instructions: 38.367110458646884\n",
      "453. someone: 38.315925131610086\n",
      "454. finally: 38.25052336860105\n",
      "455. results: 38.105820267110715\n",
      "456. pieces: 37.91463280362803\n",
      "457. sometimes: 37.88146989809389\n",
      "458. trip: 37.798457839693704\n",
      "459. band: 37.78177628742575\n",
      "460. really good: 37.74601261494598\n",
      "461. person: 37.72391872189375\n",
      "462. huge: 37.71443839341843\n",
      "463. etc: 37.5228433862142\n",
      "464. lost: 37.492808067966465\n",
      "465. chair: 37.46734216648812\n",
      "466. control: 37.36562693901899\n",
      "467. regular: 37.30580854113895\n",
      "468. smooth: 37.228440165635185\n",
      "469. birthday: 37.19660161180034\n",
      "470. good price: 37.04598374197159\n",
      "471. advertised: 37.043869915066\n",
      "472. cheaper: 36.99199179914024\n",
      "473. tight: 36.97198982671855\n",
      "474. pocket: 36.89118445930846\n",
      "475. difficult: 36.819565312747436\n",
      "476. within: 36.78612723641725\n",
      "477. amount: 36.78582535021777\n",
      "478. tea: 36.68262453904517\n",
      "479. cannot: 36.4505771350799\n",
      "480. soon: 36.35919967896842\n",
      "481. iphone: 36.246081616541225\n",
      "482. necklace: 36.22662213883949\n",
      "483. player: 36.21302266467986\n",
      "484. bigger: 36.20953708466087\n",
      "485. books: 36.086906844723636\n",
      "486. simply: 36.079601884610675\n",
      "487. described: 36.0478692632643\n",
      "488. seat: 36.01522400873067\n",
      "489. body: 35.96930123690123\n",
      "490. looks like: 35.89746555769147\n",
      "491. movies: 35.81708117694982\n",
      "492. impressed: 35.810867575453685\n",
      "493. parts: 35.79977154832206\n",
      "494. reason: 35.74457385692024\n",
      "495. shape: 35.55607237213212\n",
      "496. completely: 35.41933621637531\n",
      "497. running: 35.40668541784949\n",
      "498. wait: 35.371562630475744\n",
      "499. care: 35.31765172935408\n",
      "500. dark: 35.30068097048295\n",
      "501. uses: 35.29877874401586\n",
      "502. 20: 35.26171547702853\n",
      "503. stick: 35.16757925087606\n",
      "504. felt: 35.12829136458599\n",
      "505. apart: 35.10053025064486\n",
      "506. along: 35.08016358702933\n",
      "507. machine: 35.06537035271016\n",
      "508. headphones: 34.96613874575234\n",
      "509. card: 34.96592585271829\n",
      "510. thick: 34.87548007328736\n",
      "511. safe: 34.70603070909773\n",
      "512. charger: 34.67712910513512\n",
      "513. features: 34.60523167250903\n",
      "514. area: 34.55822158640274\n",
      "515. dogs: 34.52437608923503\n",
      "516. knife: 34.46124335812229\n",
      "517. feature: 34.39792076270466\n",
      "518. usually: 34.32394257799805\n",
      "519. remote: 34.23044668945225\n",
      "520. break: 34.219640391540516\n",
      "521. touch: 34.21748694545418\n",
      "522. paid: 34.154369088274876\n",
      "523. kitchen: 34.136790090549965\n",
      "524. red: 34.05275261446075\n",
      "525. rather: 34.04657623220405\n",
      "526. speakers: 34.039551660613206\n",
      "527. outside: 33.99044274951007\n",
      "528. often: 33.98676059034385\n",
      "529. past: 33.95303523825583\n",
      "530. warm: 33.84476449614233\n",
      "531. live: 33.67236523050348\n",
      "532. move: 33.61845402742876\n",
      "533. paper: 33.52358042403831\n",
      "534. excited: 33.507067552022896\n",
      "535. poor: 33.49946960353797\n",
      "536. expecting: 33.49812895104591\n",
      "537. cord: 33.349525618467574\n",
      "538. ipad: 33.32750857999785\n",
      "539. mattress: 33.281978575558114\n",
      "540. thought would: 33.23581527526077\n",
      "541. make sure: 33.22314894364528\n",
      "542. bar: 32.993229387414054\n",
      "543. likes: 32.99138547683231\n",
      "544. cards: 32.930340669701685\n",
      "545. idea: 32.89785903709066\n",
      "546. due: 32.768180654358225\n",
      "547. helpful: 32.734581031299605\n",
      "548. highly recommended: 32.63628318248171\n",
      "549. delivery: 32.37335929088627\n",
      "550. everyday: 32.3463080044403\n",
      "551. version: 32.312779533221175\n",
      "552. glass: 32.19772741730354\n",
      "553. packaging: 32.093491314567196\n",
      "554. customer: 32.093232332079054\n",
      "555. save: 32.087814098181745\n",
      "556. lightweight: 32.03553835814202\n",
      "557. chain: 32.02028227800506\n",
      "558. school: 31.946845385531912\n",
      "559. sharp: 31.87237156909758\n",
      "560. otherwise: 31.793300331328833\n",
      "561. finish: 31.781842391566453\n",
      "562. recommend anyone: 31.773619890952766\n",
      "563. good product: 31.582825848677153\n",
      "564. party: 31.534265763240878\n",
      "565. great quality: 31.524246427174262\n",
      "566. office: 31.344182243957288\n",
      "567. cat: 31.265160415504475\n",
      "568. button: 31.228595943932525\n",
      "569. noticed: 31.187137645567542\n",
      "570. affordable: 31.17515304958057\n",
      "571. shoe: 31.173063274323955\n",
      "572. series: 31.14149830168142\n",
      "573. storage: 31.131208265271923\n",
      "574. wide: 31.09349150213044\n",
      "575. helped: 31.084501119352982\n",
      "576. guitar: 31.00731112150027\n",
      "577. yes: 30.985281177172574\n",
      "578. kept: 30.97644168759904\n",
      "579. supposed: 30.95887820559573\n",
      "580. protection: 30.881914849870917\n",
      "581. glasses: 30.855243071224464\n",
      "582. flavor: 30.794820784543784\n",
      "583. easy install: 30.79165529323382\n",
      "584. 100: 30.765507457927566\n",
      "585. broken: 30.742863870129636\n",
      "586. thinking: 30.709970946029898\n",
      "587. led: 30.708917846653666\n",
      "588. another one: 30.68593305010551\n",
      "589. putting: 30.684378909885282\n",
      "590. door: 30.66792209604741\n",
      "591. children: 30.601719403332552\n",
      "592. loose: 30.583844833167383\n",
      "593. wash: 30.418306200100552\n",
      "594. model: 30.40470560979836\n",
      "595. given: 30.358531881805593\n",
      "596. sounds: 30.302107437707903\n",
      "597. true: 30.246946282156046\n",
      "598. told: 30.205364120093225\n",
      "599. film: 30.19843542571153\n",
      "600. belt: 30.194750722685228\n",
      "601. shower: 30.140758360863106\n",
      "602. leave: 30.033755941346698\n",
      "603. put together: 30.017196361278348\n",
      "604. usb: 30.004149841538123\n",
      "605. recommend product: 29.94256579054955\n",
      "606. okay: 29.91146802515679\n",
      "607. flat: 29.878911057429253\n",
      "608. pockets: 29.851430878134067\n",
      "609. tools: 29.844646856775032\n",
      "610. sleep: 29.843124486221498\n",
      "611. wood: 29.841790501172046\n",
      "612. definitely recommend: 29.7987472944305\n",
      "613. installed: 29.729714162045425\n",
      "614. child: 29.63525820260512\n",
      "615. compliments: 29.547138442843284\n",
      "616. first time: 29.53509292034373\n",
      "617. five: 29.444202879159757\n",
      "618. available: 29.440664230929528\n",
      "619. spend: 29.433600222303035\n",
      "620. included: 29.300746452294405\n",
      "621. earrings: 29.252833130718372\n",
      "622. audio: 29.198872672376165\n",
      "623. returned: 29.19126294570056\n",
      "624. course: 29.147405114779765\n",
      "625. green: 29.11835431673221\n",
      "626. every time: 29.06170828349337\n",
      "627. world: 29.02785223153815\n",
      "628. understand: 29.022110383261765\n",
      "629. condition: 28.999608653310872\n",
      "630. recently: 28.936808244487004\n",
      "631. shows: 28.936102666481812\n",
      "632. local: 28.904387766350574\n",
      "633. range: 28.888464299801406\n",
      "634. mind: 28.691171002089238\n",
      "635. wallet: 28.68540695986986\n",
      "636. coming: 28.682885928559184\n",
      "637. stop: 28.65848009562956\n",
      "638. hear: 28.563434915192907\n",
      "639. watching: 28.492576458373048\n",
      "640. feel like: 28.394038053290046\n",
      "641. lens: 28.390730574748044\n",
      "642. four: 28.37540469378615\n",
      "643. 30: 28.363941839644518\n",
      "644. fall: 28.249997660266175\n",
      "645. watches: 28.214956606582597\n",
      "646. 12: 28.188447599475907\n",
      "647. later: 28.106493503057184\n",
      "648. bike: 28.076246952455364\n",
      "649. cup: 28.040375563731203\n",
      "650. would buy: 28.035290498621823\n",
      "651. cleaning: 28.017431188935234\n",
      "652. enjoyed: 28.004243608969347\n",
      "653. test: 27.969261232900987\n",
      "654. brands: 27.959705841158286\n",
      "655. remove: 27.918763018869175\n",
      "656. similar: 27.81629860032801\n",
      "657. daily: 27.811076991879094\n",
      "658. printer: 27.78504953963214\n",
      "659. pick: 27.73672915083546\n",
      "660. floor: 27.724291624161193\n",
      "661. ended: 27.690495591762154\n",
      "662. plug: 27.689668361803555\n",
      "663. matter: 27.65872738222428\n",
      "664. effective: 27.611400061203003\n",
      "665. drive: 27.59592488646459\n",
      "666. fabric: 27.59320719836625\n",
      "667. accurate: 27.493903548249403\n",
      "668. work well: 27.46015893516555\n",
      "669. description: 27.4561964067023\n",
      "670. unfortunately: 27.42662098940998\n",
      "671. stylish: 27.35162829886192\n",
      "672. worry: 27.34551852310046\n",
      "673. speaker: 27.344613409573387\n",
      "674. happy purchase: 27.23145835420251\n",
      "675. waste money: 27.173797185021545\n",
      "676. opened: 27.13660772861983\n",
      "677. performance: 27.11427392306925\n",
      "678. turned: 27.099824782439555\n",
      "679. luggage: 27.058011671221575\n",
      "680. today: 27.033864543435502\n",
      "681. addition: 27.005770343647534\n",
      "682. backpack: 26.95698706567306\n",
      "683. ordering: 26.94572342036992\n",
      "684. would definitely: 26.9420530699456\n",
      "685. really nice: 26.895596728824493\n",
      "686. figure: 26.863884134672965\n",
      "687. volume: 26.853815031192276\n",
      "688. even though: 26.837239890072517\n",
      "689. shipped: 26.8044960262457\n",
      "690. print: 26.793651611844453\n",
      "691. feeling: 26.77945665824888\n",
      "692. looks good: 26.777637198412133\n",
      "693. switch: 26.75077181249408\n",
      "694. reasonable: 26.73579480012628\n",
      "695. level: 26.732606121975913\n",
      "696. length: 26.6187192326785\n",
      "697. 15: 26.599211759059475\n",
      "698. giving: 26.562886349479697\n",
      "699. added: 26.55344596173462\n",
      "700. work great: 26.517702622557337\n",
      "701. screws: 26.491059182268753\n",
      "702. lose: 26.449453663662158\n",
      "703. purpose: 26.4115126531958\n",
      "704. summer: 26.387893697451442\n",
      "705. cold: 26.37501182290027\n",
      "706. sent: 26.36747115362914\n",
      "707. one best: 26.327535585113733\n",
      "708. lovely: 26.294534525996255\n",
      "709. last long: 26.27194389928666\n",
      "710. check: 26.22981593190536\n",
      "711. slightly: 26.15121701480978\n",
      "712. foot: 26.14313510045611\n",
      "713. expectations: 26.056775070746838\n",
      "714. seemed: 25.949817240647015\n",
      "715. grip: 25.941543386997395\n",
      "716. eyes: 25.917232284734624\n",
      "717. man: 25.860750315611458\n",
      "718. inexpensive: 25.855015786052363\n",
      "719. gloves: 25.842406851822393\n",
      "720. pump: 25.78680781410455\n",
      "721. pain: 25.758967434682702\n",
      "722. inch: 25.756526455209542\n",
      "723. standard: 25.687627782418488\n",
      "724. purchasing: 25.684476470868567\n",
      "725. twice: 25.66632996740431\n",
      "726. beat: 25.661250658712678\n",
      "727. natural: 25.65150886918065\n",
      "728. plenty: 25.610592768189985\n",
      "729. single: 25.557474767017148\n",
      "730. hit: 25.536393244582538\n",
      "731. eye: 25.536034633986173\n",
      "732. plan: 25.514578011938998\n",
      "733. wrist: 25.449795699818097\n",
      "734. monitor: 25.44551974558876\n",
      "735. filter: 25.439377219147957\n",
      "736. choice: 25.438955218873026\n",
      "737. photo: 25.43114178017138\n",
      "738. held: 25.423799845633813\n",
      "739. straps: 25.42200036628448\n",
      "740. properly: 25.410177225917167\n",
      "741. eat: 25.389307400365357\n",
      "742. collection: 25.351901684403288\n",
      "743. opinion: 25.290812075177783\n",
      "744. desk: 25.27504243790275\n",
      "745. wants: 25.273552743848132\n",
      "746. compared: 25.272825842470912\n",
      "747. dress: 25.27200619538544\n",
      "748. stores: 25.18666134359065\n",
      "749. pull: 25.163447657857027\n",
      "750. non: 25.16250213600273\n",
      "751. tablet: 25.128239138364222\n",
      "752. quality product: 25.115218364859025\n",
      "753. normal: 25.095472462497828\n",
      "754. worn: 25.082997867299145\n",
      "755. holes: 25.079928368893256\n",
      "756. certainly: 25.058264608311642\n",
      "757. pretty good: 24.98488587402591\n",
      "758. hoping: 24.92445511193697\n",
      "759. fairly: 24.87119206274106\n",
      "760. older: 24.839720370353245\n",
      "761. cant: 24.809571973032472\n",
      "762. attractive: 24.79209994655727\n",
      "763. truly: 24.758607345949706\n",
      "764. light weight: 24.755339302256083\n",
      "765. look like: 24.75154477649215\n",
      "766. keyboard: 24.746109477952224\n",
      "767. unless: 24.73627032834738\n",
      "768. entire: 24.710073792901643\n",
      "769. gold: 24.670496048083358\n",
      "770. complaint: 24.58548848520468\n",
      "771. brush: 24.560745287870358\n",
      "772. materials: 24.487506759679952\n",
      "773. hour: 24.486310821859924\n",
      "774. flimsy: 24.484074138815686\n",
      "775. important: 24.435734661394736\n",
      "776. delivered: 24.43322046434931\n",
      "777. neck: 24.43116079357135\n",
      "778. anywhere: 24.39280067483296\n",
      "779. sound quality: 24.359338526488756\n",
      "780. brother: 24.272940703379206\n",
      "781. protect: 24.251368607003005\n",
      "782. worth money: 24.240235143191097\n",
      "783. rug: 24.23161673538859\n",
      "784. buttons: 24.215976854468977\n",
      "785. loud: 24.21286842412301\n",
      "786. adjust: 24.201863864209695\n",
      "787. rest: 24.181198260123065\n",
      "788. immediately: 24.11123317828056\n",
      "789. designed: 24.11053308305385\n",
      "790. assemble: 24.106810379174608\n",
      "791. compact: 24.1064653909885\n",
      "792. rubber: 24.080982965684377\n",
      "793. owned: 24.044803136155164\n",
      "794. morning: 23.986086537917423\n",
      "795. allows: 23.98295357340998\n",
      "796. dont: 23.979175300161078\n",
      "797. market: 23.96572822317622\n",
      "798. heat: 23.959848907075852\n",
      "799. mount: 23.956385745691485\n",
      "800. special: 23.945438868661906\n",
      "801. fresh: 23.945387880735858\n",
      "802. 50: 23.941429423275853\n",
      "803. future: 23.92159755645591\n",
      "804. living: 23.91940096229496\n",
      "805. clothes: 23.8968596514651\n",
      "806. ear: 23.87914298175945\n",
      "807. gone: 23.870112135644867\n",
      "808. ink: 23.785924688435184\n",
      "809. stopped: 23.78159642806751\n",
      "810. wow: 23.763368324551177\n",
      "811. match: 23.73326663622301\n",
      "812. sit: 23.72920574093982\n",
      "813. little bit: 23.702978171125014\n",
      "814. bracelet: 23.663605647892204\n",
      "815. setting: 23.66068863621251\n",
      "816. please: 23.65570725444709\n",
      "817. update: 23.64635488295576\n",
      "818. tiny: 23.643069845549466\n",
      "819. anyway: 23.62820270988406\n",
      "820. main: 23.593553447062295\n",
      "821. blade: 23.58869073826905\n",
      "822. sun: 23.493236851200944\n",
      "823. priced: 23.458681754444363\n",
      "824. convenient: 23.413315839252235\n",
      "825. near: 23.3532638071718\n",
      "826. mouse: 23.35265603250781\n",
      "827. ball: 23.315889616664148\n",
      "828. great buy: 23.305565763720722\n",
      "829. complaints: 23.182029335137813\n",
      "830. based: 23.166422586787203\n",
      "831. learn: 23.160846808188193\n",
      "832. bring: 23.08138382017124\n",
      "833. secure: 23.069705869735294\n",
      "834. mini: 23.034994621820292\n",
      "835. across: 23.03493184990965\n",
      "836. send: 23.031373217348733\n",
      "837. name: 23.018704768922884\n",
      "838. installation: 22.98379197568353\n",
      "839. wireless: 22.980719744329125\n",
      "840. played: 22.96177341874454\n",
      "841. hate: 22.93371088600222\n",
      "842. build: 22.92850053986892\n",
      "843. careful: 22.903221538832327\n",
      "844. replaced: 22.90102793818433\n",
      "845. holding: 22.89515881186148\n",
      "846. fell: 22.88817812829889\n",
      "847. gotten: 22.85542050311838\n",
      "848. present: 22.842143821289334\n",
      "849. display: 22.809698132120634\n",
      "850. setup: 22.8078201821284\n",
      "851. bought one: 22.751149638130933\n",
      "852. provides: 22.7087206820942\n",
      "853. multiple: 22.6808069425542\n",
      "854. previous: 22.65762082462888\n",
      "855. http: 22.648911851811224\n",
      "856. well worth: 22.6483326976936\n",
      "857. sweet: 22.638976001288963\n",
      "858. anymore: 22.63298149894481\n",
      "859. great value: 22.62549401719424\n",
      "860. charging: 22.622522903372776\n",
      "861. packaged: 22.620651989434613\n",
      "862. legs: 22.616215143586736\n",
      "863. ready: 22.569471424131176\n",
      "864. knew: 22.55667192416072\n",
      "865. hose: 22.534928418195328\n",
      "866. complete: 22.515969150853582\n",
      "867. portable: 22.493215966645263\n",
      "868. smells: 22.488324435704836\n",
      "869. construction: 22.483566229415022\n",
      "870. lock: 22.477578623398855\n",
      "871. unique: 22.47655235954849\n",
      "872. keys: 22.45433617199319\n",
      "873. headset: 22.441635351537577\n",
      "874. spent: 22.43277617652022\n",
      "875. season: 22.420738072803957\n",
      "876. professional: 22.407710887460762\n",
      "877. camping: 22.380298298149434\n",
      "878. energy: 22.339976228527572\n",
      "879. keeping: 22.309842107974053\n",
      "880. sizes: 22.247644895931252\n",
      "881. shirt: 22.23622795574637\n",
      "882. speed: 22.196764539103402\n",
      "883. walk: 22.184649816506415\n",
      "884. base: 22.177593017616843\n",
      "885. really well: 22.168894478233707\n",
      "886. classic: 22.15145458735399\n",
      "887. considering: 22.114283883258594\n",
      "888. samsung: 22.098969277759668\n",
      "889. works fine: 22.0962809707758\n",
      "890. perfect size: 22.093798017687458\n",
      "891. zipper: 22.089090696285773\n",
      "892. ray: 22.063402962824128\n",
      "893. except: 21.970547986496673\n",
      "894. higher: 21.9442145240063\n",
      "895. duty: 21.8686814684171\n",
      "896. board: 21.82787758689332\n",
      "897. business: 21.814151684777\n",
      "898. kid: 21.800125183119622\n",
      "899. warranty: 21.780811531551286\n",
      "900. possible: 21.681898199116965\n",
      "901. option: 21.672326234597612\n",
      "902. function: 21.66411496660506\n",
      "903. lower: 21.66202016223891\n",
      "904. hang: 21.613615959155915\n",
      "905. clip: 21.57144456048223\n",
      "906. frame: 21.56229974029615\n",
      "907. characters: 21.51944007608171\n",
      "908. mom: 21.487836685911283\n",
      "909. lamp: 21.4663757659774\n",
      "910. funny: 21.4445356372348\n",
      "911. adapter: 21.409737384922895\n",
      "912. asin: 21.392942780519355\n",
      "913. steel: 21.384280664610973\n",
      "914. screw: 21.35515500288124\n",
      "915. inches: 21.35302188113274\n",
      "916. information: 21.334622566173046\n",
      "917. buy another: 21.31744311839823\n",
      "918. trouble: 21.308397242270416\n",
      "919. worth price: 21.302592743340554\n",
      "920. note: 21.296248766329676\n",
      "921. connect: 21.278864813570166\n",
      "922. cap: 21.267301981689393\n",
      "923. deep: 21.257439540114333\n",
      "924. im: 21.242232181195888\n",
      "925. sister: 21.222377829905803\n",
      "926. surface: 21.2185698696284\n",
      "927. grill: 21.19278845886428\n",
      "928. drink: 21.1919246040002\n",
      "929. worked great: 21.149813069472202\n",
      "930. forward: 21.148290303798515\n",
      "931. sized: 21.137055346338776\n",
      "932. basic: 21.136282511400413\n",
      "933. pillow: 21.10755573397062\n",
      "934. purse: 21.095652680573604\n",
      "935. bottles: 21.09512753755015\n",
      "936. controller: 21.077520306510166\n",
      "937. medium: 21.069275845562075\n",
      "938. every day: 21.06086132915818\n",
      "939. key: 21.027750424407056\n",
      "940. customer service: 21.0190372199897\n",
      "941. quality good: 21.01345373959382\n",
      "942. barely: 20.962470596695454\n",
      "943. pounds: 20.9103659309955\n",
      "944. taken: 20.897574571332203\n",
      "945. functions: 20.888189936731045\n",
      "946. silver: 20.871602033983805\n",
      "947. stays: 20.848726993679872\n",
      "948. easy set: 20.823000622691506\n",
      "949. guy: 20.8218462259335\n",
      "950. socks: 20.77382624536248\n",
      "951. cables: 20.749970659568767\n",
      "952. pad: 20.682742919277494\n",
      "953. call: 20.67109504987794\n",
      "954. opening: 20.609199400612983\n",
      "955. mat: 20.607282133715543\n",
      "956. lasted: 20.601577929135072\n",
      "957. protector: 20.596547302026686\n",
      "958. pressure: 20.547223685875522\n",
      "959. adjustable: 20.528159887952874\n",
      "960. terrible: 20.518347941600357\n",
      "961. returning: 20.481094669678992\n",
      "962. delicious: 20.465819037426908\n",
      "963. wore: 20.433572557665737\n",
      "964. absolutely love: 20.35650759271099\n",
      "965. hole: 20.34097184976968\n",
      "966. prefer: 20.287637589038397\n",
      "967. really great: 20.23995245429878\n",
      "968. bass: 20.22833989874533\n",
      "969. straight: 20.163052575509163\n",
      "970. collar: 20.161965905253652\n",
      "971. tape: 20.150754305190798\n",
      "972. notice: 20.11726317610241\n",
      "973. mother: 20.116146662285832\n",
      "974. bedroom: 20.114450457904777\n",
      "975. attention: 20.10345188865757\n",
      "976. young: 20.094368564255436\n",
      "977. slide: 20.087378064975695\n",
      "978. pro: 20.075627108198155\n",
      "979. fully: 20.053263215088194\n",
      "980. toys: 20.034579354945883\n",
      "981. fix: 20.00935635320185\n",
      "982. bulky: 20.002568864397873\n",
      "983. interesting: 19.947920247906996\n",
      "984. stuck: 19.945688294108923\n",
      "985. bluetooth: 19.94324726306014\n",
      "986. easy clean: 19.917421527639895\n",
      "987. devices: 19.89137872220529\n",
      "988. means: 19.864880245460665\n",
      "989. memory: 19.85657469753115\n",
      "990. cutting: 19.84998601491336\n",
      "991. lol: 19.82598922491816\n",
      "992. slip: 19.819326258026923\n",
      "993. remember: 19.80541843328443\n",
      "994. cause: 19.793256512849663\n",
      "995. window: 19.760353912422758\n",
      "996. throw: 19.735708007301376\n",
      "997. receive: 19.715958721684594\n",
      "998. allow: 19.693114212763835\n",
      "999. useless: 19.65350456892373\n",
      "1000. mix: 19.63945557214678\n",
      "1001. fits perfectly: 19.600706604210533\n",
      "1002. provide: 19.589675914587374\n",
      "1003. number: 19.563609432114422\n",
      "1004. eating: 19.55734734380203\n",
      "1005. looks nice: 19.543647521218226\n",
      "1006. ears: 19.53244418416634\n",
      "1007. image: 19.51505567098884\n",
      "1008. quality great: 19.470992420631067\n",
      "1009. dead: 19.445789905313696\n",
      "1010. girl: 19.403833783081307\n",
      "1011. look great: 19.375605902036288\n",
      "1012. none: 19.37233391342647\n",
      "1013. chance: 19.341302412187893\n",
      "1014. mean: 19.328695093478718\n",
      "1015. chairs: 19.31627258748896\n",
      "1016. boots: 19.294323882910053\n",
      "1017. really love: 19.233567570534696\n",
      "1018. crazy: 19.222472440632327\n",
      "1019. directions: 19.190628494273362\n",
      "1020. horrible: 19.18644783130056\n",
      "1021. years ago: 19.1479332299733\n",
      "1022. would highly: 19.14365616179914\n",
      "1023. wears: 19.133574716444155\n",
      "1024. functional: 19.121772496268942\n",
      "1025. faster: 19.09794789553324\n",
      "1026. write: 19.064163050743765\n",
      "1027. suggest: 19.059255398199536\n",
      "1028. photos: 19.052455471126553\n",
      "1029. edge: 19.023748725943896\n",
      "1030. heard: 19.02312004886461\n",
      "1031. dropped: 19.01716300918397\n",
      "1032. solution: 19.009690801446496\n",
      "1033. jewelry: 18.99044785237396\n",
      "1034. variety: 18.98049719415077\n",
      "1035. would like: 18.965468520325906\n",
      "1036. wheels: 18.96339225688791\n",
      "1037. hat: 18.956407838152725\n",
      "1038. continue: 18.94931842040878\n",
      "1039. granddaughter: 18.93551237477973\n",
      "1040. ask: 18.915640966788303\n",
      "1041. seeing: 18.903728423704816\n",
      "1042. slow: 18.87835103059719\n",
      "1043. settings: 18.877434446543894\n",
      "1044. digital: 18.83926781564526\n",
      "1045. roll: 18.828518874772502\n",
      "1046. sunglasses: 18.813315737254307\n",
      "1047. junk: 18.81044023410885\n",
      "1048. carrying: 18.7951886753541\n",
      "1049. noise: 18.714301125629223\n",
      "1050. called: 18.694318557699415\n",
      "1051. holder: 18.688258358744722\n",
      "1052. covers: 18.640958603778312\n",
      "1053. picked: 18.619010372530603\n",
      "1054. apple: 18.617970563506493\n",
      "1055. paint: 18.5803596720235\n",
      "1056. price good: 18.564094916816764\n",
      "1057. oh: 18.561749551399302\n",
      "1058. price great: 18.555396598601444\n",
      "1059. trips: 18.554200195206285\n",
      "1060. falling: 18.54567497866605\n",
      "1061. would highly recommend: 18.537804654914133\n",
      "1062. per: 18.53590573550549\n",
      "1063. connection: 18.491699553616687\n",
      "1064. written: 18.48889527221176\n",
      "1065. cream: 18.479709492093757\n",
      "1066. reliable: 18.468382218470726\n",
      "1067. workout: 18.40872365992458\n",
      "1068. easy put: 18.408511711112002\n",
      "1069. cups: 18.391721628506282\n",
      "1070. girlfriend: 18.38573330266796\n",
      "1071. appears: 18.37058508132172\n",
      "1072. mic: 18.345197588415548\n",
      "1073. nearly: 18.331744698551883\n",
      "1074. places: 18.326988449157497\n",
      "1075. action: 18.30300254025572\n",
      "1076. 3d: 18.276392543226077\n",
      "1077. drop: 18.26178703496052\n",
      "1078. healthy: 18.241536467877665\n",
      "1079. guys: 18.230173591672397\n",
      "1080. charged: 18.227149838616928\n",
      "1081. chocolate: 18.21624962930164\n",
      "1082. wire: 18.20621698968901\n",
      "1083. spot: 18.172991777056804\n",
      "1084. shopping: 18.154505925023923\n",
      "1085. videos: 18.1531672427351\n",
      "1086. sides: 18.08359021543304\n",
      "1087. elegant: 18.037628450155168\n",
      "1088. would recommend anyone: 18.034715336159742\n",
      "1089. hd: 18.034471115455823\n",
      "1090. runs: 18.03260193100732\n",
      "1091. middle: 18.00586714160082\n",
      "1092. brown: 18.001535577707827\n",
      "1093. rate: 17.989506699485023\n",
      "1094. yellow: 17.89886860196621\n",
      "1095. scent: 17.897375918611168\n",
      "1096. date: 17.895235722019986\n",
      "1097. much easier: 17.86667993296846\n",
      "1098. gorgeous: 17.837624818344253\n",
      "1099. dad: 17.836267752412766\n",
      "1100. sold: 17.835242635009386\n",
      "1101. pink: 17.834558555203625\n",
      "1102. walking: 17.834021830562545\n",
      "1103. behind: 17.815747418082097\n",
      "1104. additional: 17.805569848214702\n",
      "1105. love love: 17.803891023589006\n",
      "1106. graphics: 17.743912173325768\n",
      "1107. actual: 17.719572505558848\n",
      "1108. options: 17.678410186989062\n",
      "1109. shown: 17.675389714809324\n",
      "1110. boyfriend: 17.672191787572807\n",
      "1111. push: 17.641074434359286\n",
      "1112. smart: 17.618781742908464\n",
      "1113. spray: 17.612752018560318\n",
      "1114. worked well: 17.611975261774525\n",
      "1115. starting: 17.599425386380528\n",
      "1116. follow: 17.596796981157762\n",
      "1117. whenever: 17.56984188407858\n",
      "1118. tough: 17.53161200198528\n",
      "1119. heavy duty: 17.52932675818337\n",
      "1120. grandson: 17.528850582609742\n",
      "1121. unlike: 17.483542872758175\n",
      "1122. alot: 17.466328283312293\n",
      "1123. author: 17.451204662858792\n",
      "1124. process: 17.409137235559548\n",
      "1125. boy: 17.385993737295546\n",
      "1126. 40: 17.366128977787426\n",
      "1127. comfort: 17.358804926911198\n",
      "1128. winter: 17.33233775110649\n",
      "1129. wet: 17.3283036372926\n",
      "1130. access: 17.32772328997517\n",
      "1131. worst: 17.32124064900972\n",
      "1132. happened: 17.31563166112777\n",
      "1133. supplement: 17.281031707232593\n",
      "1134. turns: 17.258143545707306\n",
      "1135. honestly: 17.245922342081883\n",
      "1136. including: 17.241949555566368\n",
      "1137. weather: 17.240854857081043\n",
      "1138. shop: 17.19338446158869\n",
      "1139. sorry: 17.18732787041335\n",
      "1140. disappointing: 17.172539046414386\n",
      "1141. onto: 17.171914724141445\n",
      "1142. sitting: 17.16724156282715\n",
      "1143. pants: 17.152070413729092\n",
      "1144. self: 17.142255205093083\n",
      "1145. brought: 17.14132876352215\n",
      "1146. flashlight: 17.126369572599792\n",
      "1147. everywhere: 17.10736805478394\n",
      "1148. powerful: 17.08474532355256\n",
      "1149. pen: 17.073955383018966\n",
      "1150. apply: 17.06623598805037\n",
      "1151. double: 17.053289259385263\n",
      "1152. dust: 17.016224439629138\n",
      "1153. guide: 17.00876953815853\n",
      "1154. hdmi: 16.993332708223367\n",
      "1155. particular: 16.984055883452907\n",
      "1156. internet: 16.953690647718222\n",
      "1157. asked: 16.943317907178926\n",
      "1158. connected: 16.934996398256033\n",
      "1159. buy one: 16.925597425867696\n",
      "1160. sony: 16.918733240029624\n",
      "1161. sticks: 16.896012127382825\n",
      "1162. furniture: 16.87499465221594\n",
      "1163. obviously: 16.864918746387076\n",
      "1164. seconds: 16.8528104720457\n",
      "1165. far good: 16.843934536100896\n",
      "1166. bathroom: 16.827194823845794\n",
      "1167. provided: 16.826243271004792\n",
      "1168. areas: 16.820600996038788\n",
      "1169. somewhat: 16.813947013991466\n",
      "1170. cats: 16.81062962606811\n",
      "1171. cartridges: 16.807866864778234\n",
      "1172. leaves: 16.80179435851877\n",
      "1173. china: 16.789415962596628\n",
      "1174. alone: 16.789315203177583\n",
      "1175. like one: 16.764514725164748\n",
      "1176. result: 16.75892493821257\n",
      "1177. equipment: 16.75413651757114\n",
      "1178. would definitely recommend: 16.747315310484215\n",
      "1179. blu: 16.72508377503032\n",
      "1180. learning: 16.68133956834861\n",
      "1181. become: 16.675476242999036\n",
      "1182. strings: 16.652955545044293\n",
      "1183. rings: 16.64915596835782\n",
      "1184. software: 16.64551629520732\n",
      "1185. beginning: 16.643278983807505\n",
      "1186. easy assemble: 16.63198141110749\n",
      "1187. research: 16.62854547947104\n",
      "1188. lid: 16.618993691804658\n",
      "1189. works perfectly: 16.591959398591875\n",
      "1190. annoying: 16.57142989445122\n",
      "1191. sugar: 16.560929562146402\n",
      "1192. living room: 16.552163996789876\n",
      "1193. reach: 16.53171650399838\n",
      "1194. damage: 16.520848842958976\n",
      "1195. bulb: 16.514962153708062\n",
      "1196. lasts: 16.499414966213404\n",
      "1197. hook: 16.487396521147105\n",
      "1198. bulbs: 16.48285589713322\n",
      "1199. great job: 16.46272611114493\n",
      "1200. mentioned: 16.450745741299542\n",
      "1201. projector: 16.445996937259565\n",
      "1202. sets: 16.41910026274361\n",
      "1203. texture: 16.405682129408184\n",
      "1204. edges: 16.401099047583617\n",
      "1205. effect: 16.39232231184826\n",
      "1206. whatever: 16.389543909184294\n",
      "1207. ease: 16.383557839066686\n",
      "1208. watched: 16.352474519901428\n",
      "1209. wedding: 16.325936618188724\n",
      "1210. age: 16.297408893956923\n",
      "1211. clearly: 16.291594337143604\n",
      "1212. fingers: 16.28548804098166\n",
      "1213. damaged: 16.27023988183753\n",
      "1214. sort: 16.270088441792144\n",
      "1215. windows: 16.240710090365546\n",
      "1216. daughter loves: 16.22834798183594\n",
      "1217. words: 16.211687298925685\n",
      "1218. plays: 16.21100555518535\n",
      "1219. constantly: 16.19498681676546\n",
      "1220. loss: 16.18113118141931\n",
      "1221. sheets: 16.180963420083714\n",
      "1222. get pay: 16.166450548668518\n",
      "1223. new one: 16.16460214605683\n",
      "1224. center: 16.16408929460505\n",
      "1225. changed: 16.16368514923236\n",
      "1226. manual: 16.163262901657763\n",
      "1227. various: 16.161950334638895\n",
      "1228. right size: 16.151864026110985\n",
      "1229. fair: 16.150999854419883\n",
      "1230. beautifully: 16.14995922761094\n",
      "1231. hassle: 16.129921213999957\n",
      "1232. press: 16.122434392340313\n",
      "1233. ice: 16.055368497717588\n",
      "1234. mostly: 16.03011730391547\n",
      "1235. writing: 16.022603756352094\n",
      "1236. handles: 16.00353923064731\n",
      "1237. drill: 15.984178749604979\n",
      "1238. tastes: 15.979287490394698\n",
      "1239. shampoo: 15.948974484640493\n",
      "1240. bucks: 15.943044061458806\n",
      "1241. saved: 15.938143745534507\n",
      "1242. pet: 15.937826241569828\n",
      "1243. negative: 15.931920808221246\n",
      "1244. shot: 15.925214792794725\n",
      "1245. height: 15.898789026573873\n",
      "1246. stock: 15.890308269409214\n",
      "1247. moving: 15.86527815263367\n",
      "1248. placed: 15.853131421451607\n",
      "1249. charm: 15.821288085096208\n",
      "1250. total: 15.815819532958008\n",
      "1251. packed: 15.814827445636109\n",
      "1252. nephew: 15.814667057168863\n",
      "1253. gifts: 15.74304752050965\n",
      "1254. normally: 15.739393969748608\n",
      "1255. mail: 15.728173202929629\n",
      "1256. heart: 15.717452831934665\n",
      "1257. product good: 15.71524023135548\n",
      "1258. effects: 15.711563534448683\n",
      "1259. manufacturer: 15.652539921027817\n",
      "1260. figured: 15.636142375694803\n",
      "1261. scratches: 15.627883870789567\n",
      "1262. packing: 15.626250600980352\n",
      "1263. happier: 15.625154618344135\n",
      "1264. quiet: 15.619407268073859\n",
      "1265. rating: 15.611148325824642\n",
      "1266. adorable: 15.60520813388898\n",
      "1267. average: 15.570646860471282\n",
      "1268. girls: 15.561616164936463\n",
      "1269. program: 15.549815211621727\n",
      "1270. imagine: 15.546769444207358\n",
      "1271. third: 15.517222752772646\n",
      "1272. tested: 15.512413858059087\n",
      "1273. cheaply: 15.509277331446093\n",
      "1274. incredible: 15.50286869655751\n",
      "1275. spring: 15.485788417762912\n",
      "1276. com: 15.485333755238747\n",
      "1277. class: 15.48304857912653\n",
      "1278. mess: 15.472339020007258\n",
      "1279. channels: 15.463526903171015\n",
      "1280. wont: 15.448811984495924\n",
      "1281. enjoying: 15.442234684402035\n",
      "1282. blu ray: 15.440459284997274\n",
      "1283. br also: 15.434954058997045\n",
      "1284. wind: 15.405671795991557\n",
      "1285. weekend: 15.396371986136598\n",
      "1286. minute: 15.394238145216676\n",
      "1287. cooking: 15.39320080646992\n",
      "1288. like much: 15.375661745977622\n",
      "1289. umbrella: 15.353006434225536\n",
      "1290. right away: 15.343413404132566\n",
      "1291. gaming: 15.33891246091864\n",
      "1292. tip: 15.326757532803764\n",
      "1293. missing: 15.32289080853086\n",
      "1294. trust: 15.308697961526242\n",
      "1295. user: 15.308102488305554\n",
      "1296. fire: 15.299623420446784\n",
      "1297. early: 15.293741949047277\n",
      "1298. cases: 15.282804029739806\n",
      "1299. truck: 15.278045759734384\n",
      "1300. personal: 15.249129962874909\n",
      "1301. radio: 15.246736591834443\n",
      "1302. mode: 15.24277406416945\n",
      "1303. attached: 15.20274285350174\n",
      "1304. exact: 15.19763098309469\n",
      "1305. snap: 15.193067678812326\n",
      "1306. friendly: 15.190494554486984\n",
      "1307. yard: 15.184154240464181\n",
      "1308. cameras: 15.180658884632392\n",
      "1309. worse: 15.172538728431025\n",
      "1310. study: 15.160539434932483\n",
      "1311. form: 15.158915212567743\n",
      "1312. vacuum: 15.155765809153124\n",
      "1313. step: 15.128821905785125\n",
      "1314. finger: 15.106213997666291\n",
      "1315. practice: 15.094893283402056\n",
      "1316. poor quality: 15.084777950828384\n",
      "1317. removed: 15.077544237517355\n",
      "1318. pretty much: 15.074896875220993\n",
      "1319. ground: 15.068202539899508\n",
      "1320. uncomfortable: 15.050208444069478\n",
      "1321. br would: 15.021966537805612\n",
      "1322. correct: 14.965189201844634\n",
      "1323. good value: 14.960035652015808\n",
      "1324. app: 14.959577600405426\n",
      "1325. upon: 14.945172835431647\n",
      "1326. reviewers: 14.94206743568044\n",
      "1327. throughout: 14.93636346718332\n",
      "1328. canon: 14.915230962994633\n",
      "1329. mask: 14.898172089060077\n",
      "1330. serious: 14.890900843923086\n",
      "1331. bought two: 14.867695070426182\n",
      "1332. fast shipping: 14.818501600223064\n",
      "1333. types: 14.813418853222936\n",
      "1334. sensitive: 14.797397913105\n",
      "1335. like picture: 14.771839294752647\n",
      "1336. shoulder: 14.763195066146071\n",
      "1337. personally: 14.754415438081297\n",
      "1338. perhaps: 14.742288213951774\n",
      "1339. lbs: 14.724117840448953\n",
      "1340. stainless: 14.717917667785718\n",
      "1341. lighter: 14.715595732986431\n",
      "1342. saying: 14.71182436431185\n",
      "1343. lenses: 14.691221400499312\n",
      "1344. dollars: 14.68962663124464\n",
      "1345. leak: 14.673707784532697\n",
      "1346. god: 14.662731199623517\n",
      "1347. got one: 14.662598586163723\n",
      "1348. intended: 14.654175412638635\n",
      "1349. arrived time: 14.636234130609509\n",
      "1350. defective: 14.633598945866284\n",
      "1351. plugged: 14.617214097819884\n",
      "1352. garage: 14.61715388282665\n",
      "1353. shelf: 14.607925680529059\n",
      "1354. attach: 14.599618770361088\n",
      "1355. pendant: 14.593755651518288\n",
      "1356. beach: 14.591687754084044\n",
      "1357. recording: 14.567365156701534\n",
      "1358. seems like: 14.556663879120453\n",
      "1359. complain: 14.552735923449275\n",
      "1360. flash: 14.545084349390246\n",
      "1361. round: 14.533642200236832\n",
      "1362. good buy: 14.524322735538648\n",
      "1363. xbox: 14.521482189459004\n",
      "1364. phones: 14.519914052965\n",
      "1365. poorly: 14.51085769878979\n",
      "1366. sell: 14.493638071991613\n",
      "1367. amazed: 14.462992346368923\n",
      "1368. pan: 14.458447025259135\n",
      "1369. gym: 14.441033472231547\n",
      "1370. bunch: 14.433102428116571\n",
      "1371. positive: 14.398899619544741\n",
      "1372. www: 14.398227852811349\n",
      "1373. weak: 14.39727988124215\n",
      "1374. love product: 14.379608802234264\n",
      "1375. interested: 14.379553542022327\n",
      "1376. using product: 14.365646407973701\n",
      "1377. character: 14.36060275298197\n",
      "1378. 13: 14.338753100124864\n",
      "1379. concerned: 14.336038813649738\n",
      "1380. fit perfectly: 14.323825071411411\n",
      "1381. garbage: 14.31916221678708\n",
      "1382. foam: 14.297855681065759\n",
      "1383. certain: 14.292862688674042\n",
      "1384. tired: 14.283365358292329\n",
      "1385. worried: 14.281676651860547\n",
      "1386. bonus: 14.278411028130016\n",
      "1387. moved: 14.27675051432378\n",
      "1388. website: 14.276059172137709\n",
      "1389. boxes: 14.271842706578276\n",
      "1390. get one: 14.258813661641607\n",
      "1391. blades: 14.237467659643244\n",
      "1392. awful: 14.230817310483834\n",
      "1393. comfy: 14.22951961725691\n",
      "1394. forever: 14.216337508021882\n",
      "1395. honest: 14.214253657213588\n",
      "1396. levels: 14.21237073527636\n",
      "1397. excellent product: 14.210367024116726\n",
      "1398. well built: 14.210132185978306\n",
      "1399. matches: 14.201905762731622\n",
      "1400. hoped: 14.195656124454057\n",
      "1401. whether: 14.174989481158427\n",
      "1402. dirty: 14.161427021869622\n",
      "1403. fake: 14.139932345160135\n",
      "1404. accessories: 14.139558635652936\n",
      "1405. ripped: 14.133022350052952\n",
      "1406. exercise: 14.129993731996672\n",
      "1407. sense: 14.120791066540868\n",
      "1408. fits well: 14.088155753421702\n",
      "1409. scratch: 14.083372986210183\n",
      "1410. prevent: 14.076743980071736\n",
      "1411. correctly: 14.065956270871453\n",
      "1412. rack: 14.056632897672465\n",
      "1413. sleek: 14.041734871817571\n",
      "1414. recipes: 14.026268534815562\n",
      "1415. learned: 14.01471396921653\n",
      "1416. appearance: 14.007390960356103\n",
      "1417. temperature: 14.00204242112828\n",
      "1418. changing: 13.993250716128749\n",
      "1419. angle: 13.9659707092433\n",
      "1420. 34 34: 13.963253485344227\n",
      "1421. stands: 13.95685291613138\n",
      "1422. outdoor: 13.94430559683005\n",
      "1423. hurt: 13.943843258884584\n",
      "1424. instrument: 13.913235742247537\n",
      "1425. position: 13.906017817178228\n",
      "1426. bother: 13.90424810795696\n",
      "1427. balls: 13.903375581093586\n",
      "1428. ingredients: 13.902781476975557\n",
      "1429. listen: 13.901603938350647\n",
      "1430. happy product: 13.891808420668363\n",
      "1431. stiff: 13.885240746939862\n",
      "1432. charges: 13.879370021902385\n",
      "1433. waterproof: 13.876186413245971\n",
      "1434. training: 13.870163525149241\n",
      "1435. fat: 13.842079137707907\n",
      "1436. trash: 13.836081084328088\n",
      "1437. incredibly: 13.820103216841153\n",
      "1438. sale: 13.816703161399644\n",
      "1439. tone: 13.804649525020894\n",
      "1440. losing: 13.796568922913243\n",
      "1441. br great: 13.796359239495233\n",
      "1442. traveling: 13.796274835789525\n",
      "1443. finished: 13.789935111074124\n",
      "1444. falls: 13.780475385126264\n",
      "1445. organic: 13.752573106516229\n",
      "1446. 60: 13.727300363388936\n",
      "1447. site: 13.712821004547907\n",
      "1448. gun: 13.711090930321847\n",
      "1449. planning: 13.710626556499868\n",
      "1450. known: 13.682230346161354\n",
      "1451. ran: 13.661291543724907\n",
      "1452. project: 13.659833929503952\n",
      "1453. definitely buy: 13.65736232253984\n",
      "1454. microphone: 13.654607919722022\n",
      "1455. men: 13.638736656381354\n",
      "1456. modern: 13.635073789085352\n",
      "1457. iron: 13.629920430882262\n",
      "1458. parents: 13.615096361150002\n",
      "1459. combination: 13.608848842493048\n",
      "1460. originally: 13.597442439265746\n",
      "1461. suit: 13.58721258866563\n",
      "1462. security: 13.580302153794142\n",
      "1463. view: 13.574595892462241\n",
      "1464. strength: 13.568566821145614\n",
      "1465. sleeping: 13.56768634454756\n",
      "1466. lines: 13.56694589980678\n",
      "1467. adult: 13.555463763761765\n",
      "1468. pages: 13.551161368972545\n",
      "1469. tall: 13.549697973900441\n",
      "1470. detail: 13.548766538425108\n",
      "1471. mounted: 13.540556933099001\n",
      "1472. classy: 13.539956823862811\n",
      "1473. tree: 13.537317486892084\n",
      "1474. adds: 13.53475533917484\n",
      "1475. cook: 13.524160370020077\n",
      "1476. easy read: 13.523492466514513\n",
      "1477. niece: 13.519223320605207\n",
      "1478. signal: 13.515174422471732\n",
      "1479. adding: 13.495642657556605\n",
      "1480. refund: 13.4822775542832\n",
      "1481. basically: 13.480697997985292\n",
      "1482. mats: 13.457655823653116\n",
      "1483. garden: 13.457545426887522\n",
      "1484. voice: 13.454129716425552\n",
      "1485. android: 13.427598285703775\n",
      "1486. minor: 13.425755492835979\n",
      "1487. puppy: 13.425369602342885\n",
      "1488. tear: 13.413453465253355\n",
      "1489. washing: 13.408638790456738\n",
      "1490. http www: 13.404955631032701\n",
      "1491. 25: 13.392746029804144\n",
      "1492. road: 13.38683489491089\n",
      "1493. hooked: 13.375452978095414\n",
      "1494. seriously: 13.374032353106806\n",
      "1495. perfect fit: 13.367375284957918\n",
      "1496. lighting: 13.36509266881049\n",
      "1497. didnt: 13.362136334309868\n",
      "1498. literally: 13.361063990967672\n",
      "1499. give stars: 13.350674462125628\n",
      "1500. long enough: 13.338834270769299\n",
      "1501. cloth: 13.320173501163532\n",
      "1502. tag: 13.298341636029265\n",
      "1503. fly: 13.285225012409821\n",
      "1504. required: 13.27811603613022\n",
      "1505. surprise: 13.270505321667208\n",
      "1506. jump: 13.268073940433982\n",
      "1507. mouth: 13.266857814496957\n",
      "1508. stroller: 13.264621964650447\n",
      "1509. diet: 13.26045399679992\n",
      "1510. pictured: 13.259661385398598\n",
      "1511. powder: 13.242287752030569\n",
      "1512. bands: 13.235269957061309\n",
      "1513. major: 13.227460868512178\n",
      "1514. alarm: 13.222802022509292\n",
      "1515. eventually: 13.222049661531468\n",
      "1516. flip: 13.215969740208118\n",
      "1517. pop: 13.20755621419437\n",
      "1518. old son: 13.20590248119191\n",
      "1519. around house: 13.204386070149518\n",
      "1520. product great: 13.195206891366956\n",
      "1521. son loves: 13.161743612464845\n",
      "1522. spare: 13.1516847968023\n",
      "1523. women: 13.148085048072211\n",
      "1524. good thing: 13.14278089648168\n",
      "1525. fill: 13.132614994686804\n",
      "1526. heavier: 13.122389292756024\n",
      "1527. pool: 13.077904805360493\n",
      "1528. waiting: 13.07778954564983\n",
      "1529. choose: 13.07488015192197\n",
      "1530. 14: 13.069903394893803\n",
      "1531. love much: 13.066351454625257\n",
      "1532. suitcase: 13.064261455396462\n",
      "1533. pads: 13.060731443129049\n",
      "1534. 18: 13.056629138070877\n",
      "1535. usual: 13.05610982174803\n",
      "1536. consider: 13.051417514234407\n",
      "1537. rough: 13.032693076850737\n",
      "1538. ipod: 13.018713190050173\n",
      "1539. good deal: 13.01360394361704\n",
      "1540. mirror: 13.012181080857362\n",
      "1541. cuts: 13.004884791711598\n",
      "1542. luck: 12.994417389988673\n",
      "1543. regret: 12.99138566601593\n",
      "1544. arms: 12.97955685122897\n",
      "1545. definitely worth: 12.975369672031391\n",
      "1546. snug: 12.97078491927228\n",
      "1547. father: 12.968733093885945\n",
      "1548. feels like: 12.964759400303077\n",
      "1549. ways: 12.96311293961624\n",
      "1550. ideal: 12.959318533172365\n",
      "1551. soap: 12.957371958928677\n",
      "1552. earlier: 12.954743503986371\n",
      "1553. first one: 12.946011072780074\n",
      "1554. died: 12.931375258918521\n",
      "1555. wifi: 12.925315906694875\n",
      "1556. rain: 12.922321696094375\n",
      "1557. good looking: 12.909528261808573\n",
      "1558. offer: 12.905911896720879\n",
      "1559. thicker: 12.902579428930412\n",
      "1560. driver: 12.900527440827897\n",
      "1561. salt: 12.900023757355356\n",
      "1562. tend: 12.890395585393088\n",
      "1563. outstanding: 12.887788347354523\n",
      "1564. cheaply made: 12.878270879764518\n",
      "1565. kinda: 12.877305153171724\n",
      "1566. record: 12.8697675001511\n",
      "1567. breaking: 12.849090998541598\n",
      "1568. arm: 12.847892149860021\n",
      "1569. receiver: 12.84251936347173\n",
      "1570. solar: 12.816525980607148\n",
      "1571. would work: 12.815544277484062\n",
      "1572. tube: 12.813630920362396\n",
      "1573. pot: 12.795180610331716\n",
      "1574. seal: 12.791223808842643\n",
      "1575. budget: 12.786151260186402\n",
      "1576. searching: 12.783973194336342\n",
      "1577. many years: 12.778323897841801\n",
      "1578. product really: 12.765706920449013\n",
      "1579. last long time: 12.760160762216989\n",
      "1580. cell: 12.754901581377329\n",
      "1581. 11: 12.73233058255089\n",
      "1582. corner: 12.716886392419859\n",
      "1583. safety: 12.715970713292316\n",
      "1584. post: 12.714583652773829\n",
      "1585. quality price: 12.697435916255559\n",
      "1586. 16: 12.674847705621799\n",
      "1587. great deal: 12.652463991061152\n",
      "1588. weird: 12.651887634669553\n",
      "1589. word: 12.642933088129764\n",
      "1590. also like: 12.621719188884365\n",
      "1591. tips: 12.617530892999651\n",
      "1592. output: 12.611980534402033\n",
      "1593. container: 12.608451641358492\n",
      "1594. bought gift: 12.604328086189891\n",
      "1595. thrilled: 12.604175752853324\n",
      "1596. health: 12.604111921289459\n",
      "1597. fitting: 12.60057133395149\n",
      "1598. waist: 12.598856619595168\n",
      "1599. several times: 12.598639579002066\n",
      "1600. pass: 12.598542513009086\n",
      "1601. wheel: 12.597712697070662\n",
      "1602. hardware: 12.594086710965122\n",
      "1603. page: 12.588917648708708\n",
      "1604. assembly: 12.588822739780083\n",
      "1605. dirt: 12.584212829875401\n",
      "1606. offers: 12.570438113415989\n",
      "1607. exactly looking: 12.556286949230994\n",
      "1608. compare: 12.551233144825636\n",
      "1609. covered: 12.546501508547957\n",
      "1610. gonna: 12.544837206882216\n",
      "1611. share: 12.534221543343875\n",
      "1612. hanging: 12.532976200846687\n",
      "1613. boot: 12.529717582612042\n",
      "1614. shoot: 12.527659115719466\n",
      "1615. hopefully: 12.522113420333246\n",
      "1616. packs: 12.515416778027783\n",
      "1617. appreciate: 12.508230609699769\n",
      "1618. pre: 12.507468014765621\n",
      "1619. compartment: 12.506197178817633\n",
      "1620. apps: 12.502432211864104\n",
      "1621. efficient: 12.478613079503507\n",
      "1622. spots: 12.452785013357435\n",
      "1623. stable: 12.451510325937464\n",
      "1624. cast: 12.450174372909125\n",
      "1625. offered: 12.443319344151844\n",
      "1626. operate: 12.442913964521209\n",
      "1627. resistant: 12.438833724935284\n",
      "1628. bent: 12.437774162291461\n",
      "1629. noticeable: 12.420360872364464\n",
      "1630. changes: 12.41167802294696\n",
      "1631. catch: 12.394796340872727\n",
      "1632. doubt: 12.385190068838169\n",
      "1633. upgrade: 12.370791636754316\n",
      "1634. must say: 12.362617777106234\n",
      "1635. content: 12.353944700403735\n",
      "1636. one thing: 12.352085109073748\n",
      "1637. hard find: 12.341893745885\n",
      "1638. velcro: 12.336601661101119\n",
      "1639. think would: 12.333329372822005\n",
      "1640. cross: 12.33276323690423\n",
      "1641. stopped working: 12.332238125505683\n",
      "1642. necessary: 12.329053735576458\n",
      "1643. knows: 12.296261411039982\n",
      "1644. electric: 12.26690811039232\n",
      "1645. improvement: 12.259735679547648\n",
      "1646. wine: 12.258387853535792\n",
      "1647. nose: 12.257926940965168\n",
      "1648. skeptical: 12.239813323378579\n",
      "1649. really easy: 12.216136057248114\n",
      "1650. treat: 12.20963653740517\n",
      "1651. mention: 12.19554931070525\n",
      "1652. bubbles: 12.19453741726211\n",
      "1653. low price: 12.17849876035866\n",
      "1654. impossible: 12.177984659538879\n",
      "1655. multi: 12.175941372555378\n",
      "1656. pleased purchase: 12.171336963266208\n",
      "1657. nice looking: 12.16258396893487\n",
      "1658. pulled: 12.159576169376491\n",
      "1659. ride: 12.157520079699596\n",
      "1660. list: 12.148919373253678\n",
      "1661. label: 12.148747609359068\n",
      "1662. picture quality: 12.141338946790603\n",
      "1663. cartridge: 12.13320079858421\n",
      "1664. amp: 12.133170628947257\n",
      "1665. create: 12.13030678467419\n",
      "1666. late: 12.127076127974448\n",
      "1667. anyone looking: 12.126757140753133\n",
      "1668. rich: 12.117878421233243\n",
      "1669. reasonable price: 12.099246610502263\n",
      "1670. read reviews: 12.097818274640849\n",
      "1671. practical: 12.097451975658284\n",
      "1672. go wrong: 12.095616688117957\n",
      "1673. serum: 12.083650414962214\n",
      "1674. happen: 12.079751361014653\n",
      "1675. vibrant: 12.077924981917832\n",
      "1676. credit: 12.076810891131018\n",
      "1677. milk: 12.072727164900378\n",
      "1678. gear: 12.06769644953541\n",
      "1679. via: 12.066329473514063\n",
      "1680. effort: 12.060965915400722\n",
      "1681. jeans: 12.046667119044354\n",
      "1682. tray: 12.039341174914842\n",
      "1683. fold: 12.03064323431052\n",
      "1684. became: 12.027908127134914\n",
      "1685. ability: 12.02502893965692\n",
      "1686. rock: 12.02480521521183\n",
      "1687. realize: 12.013311030503065\n",
      "1688. compatible: 12.01085597002398\n",
      "1689. cleaner: 12.009759042644195\n",
      "1690. fish: 12.009029912496059\n",
      "1691. pc: 12.002415789875274\n",
      "1692. really liked: 12.000290474403217\n",
      "1693. stretch: 11.969499831464475\n",
      "1694. neat: 11.952012623705865\n",
      "1695. shade: 11.948737219891328\n",
      "1696. info: 11.943332619352413\n",
      "1697. compartments: 11.939381041320317\n",
      "1698. impressive: 11.932268624343536\n",
      "1699. reasonably: 11.931249128441515\n",
      "1700. period: 11.923854860681491\n",
      "1701. brighter: 11.921160035497788\n",
      "1702. despite: 11.92043530790879\n",
      "1703. driving: 11.918063260740592\n",
      "1704. cords: 11.916147679463899\n",
      "1705. load: 11.90559677339772\n",
      "1706. clips: 11.905419144461504\n",
      "1707. distance: 11.870585982828677\n",
      "1708. showed: 11.866070746537474\n",
      "1709. 00: 11.856892902608985\n",
      "1710. alternative: 11.850642300375249\n",
      "1711. avoid: 11.842648055792537\n",
      "1712. listed: 11.841127042309221\n",
      "1713. grade: 11.830495181548631\n",
      "1714. recommend everyone: 11.82272982942233\n",
      "1715. happens: 11.8202185662928\n",
      "1716. router: 11.814216942168276\n",
      "1717. likely: 11.808532772554315\n",
      "1718. fans: 11.80794517820024\n",
      "1719. yoga: 11.805153777197459\n",
      "1720. flexible: 11.788969467497138\n",
      "1721. nice quality: 11.78571477277688\n",
      "1722. search: 11.785389596352218\n",
      "1723. loving: 11.782430400188115\n",
      "1724. proof: 11.759463009275624\n",
      "1725. good job: 11.752413330580714\n",
      "1726. suppose: 11.73520848929377\n",
      "1727. track: 11.72373969747845\n",
      "1728. teeth: 11.705033763630304\n",
      "1729. look good: 11.703269629011315\n",
      "1730. miss: 11.698138783591673\n",
      "1731. great little: 11.686011880743502\n",
      "1732. versatile: 11.682731949762792\n",
      "1733. law: 11.6708694382334\n",
      "1734. supply: 11.660046237790336\n",
      "1735. hard time: 11.65878764953161\n",
      "1736. babies: 11.651317319530765\n",
      "1737. even better: 11.649542693506607\n",
      "1738. chips: 11.639466257772742\n",
      "1739. grow: 11.635255810828742\n",
      "1740. anti: 11.633745561516086\n",
      "1741. directly: 11.633121625458694\n",
      "1742. limited: 11.630031180629983\n",
      "1743. picks: 11.618187381035753\n",
      "1744. really cute: 11.616401975585195\n",
      "1745. screen protector: 11.601153741163698\n",
      "1746. forget: 11.593925461092836\n",
      "1747. lead: 11.58901074849105\n",
      "1748. hardly: 11.58890191551225\n",
      "1749. br love: 11.580879923370027\n",
      "1750. big enough: 11.573942841378322\n",
      "1751. factory: 11.568910444337439\n",
      "1752. sports: 11.568316864613948\n",
      "1753. typical: 11.560340739059189\n",
      "1754. meant: 11.557085663460471\n",
      "1755. costume: 11.550197457356868\n",
      "1756. oz: 11.549982023558664\n",
      "1757. nails: 11.547069620768111\n",
      "1758. silicone: 11.545455070583154\n",
      "1759. battery life: 11.538719576585933\n",
      "1760. threw: 11.537744622518726\n",
      "1761. washed: 11.526156464459344\n",
      "1762. great easy: 11.516674459628895\n",
      "1763. product would: 11.51503727654386\n",
      "1764. bill: 11.5108773638584\n",
      "1765. terrific: 11.506646028692781\n",
      "1766. wonder: 11.498751699043094\n",
      "1767. comfortable wear: 11.493924299175202\n",
      "1768. tons: 11.492556192901304\n",
      "1769. shiny: 11.491732544565085\n",
      "1770. antenna: 11.486439376331841\n",
      "1771. dial: 11.482929630890526\n",
      "1772. works like: 11.473994637984756\n",
      "1773. tent: 11.464259877917433\n",
      "1774. mistake: 11.459120041052216\n",
      "1775. meet: 11.424939093361372\n",
      "1776. controls: 11.424673803830942\n",
      "1777. bars: 11.417682800499897\n",
      "1778. group: 11.409572820280635\n",
      "1779. counter: 11.40490808278264\n",
      "1780. superb: 11.396555084232165\n",
      "1781. cabinet: 11.395517738126136\n",
      "1782. wii: 11.385473979951911\n",
      "1783. pairs: 11.381245583933797\n",
      "1784. general: 11.377139997884512\n",
      "1785. failed: 11.374789979325527\n",
      "1786. oem: 11.374114506013576\n",
      "1787. cars: 11.364602134251463\n",
      "1788. really happy: 11.36426180095763\n",
      "1789. bath: 11.363076659734926\n",
      "1790. easy carry: 11.361319557009622\n",
      "1791. application: 11.35917399899766\n",
      "1792. boys: 11.344488669353481\n",
      "1793. bowl: 11.338532903054517\n",
      "1794. could find: 11.335121705268147\n",
      "1795. stainless steel: 11.332486160351436\n",
      "1796. carrier: 11.330706069896353\n",
      "1797. years old: 11.329020578556612\n",
      "1798. ends: 11.325932535470274\n",
      "1799. turning: 11.3117170646606\n",
      "1800. investment: 11.311215435435884\n",
      "1801. caught: 11.309426954814139\n",
      "1802. balance: 11.296351761642816\n",
      "1803. glue: 11.287128297256608\n",
      "1804. appear: 11.281783821376184\n",
      "1805. sticky: 11.272365425822022\n",
      "1806. leg: 11.271948809955266\n",
      "1807. port: 11.266666068522644\n",
      "1808. price right: 11.263713461181258\n",
      "1809. vacation: 11.263091412295843\n",
      "1810. woman: 11.26255980250192\n",
      "1811. sad: 11.262531754945071\n",
      "1812. include: 11.24927656087642\n",
      "1813. 99: 11.245877296590335\n",
      "1814. constructed: 11.240719434170526\n",
      "1815. shipment: 11.234579820861931\n",
      "1816. printing: 11.233523962875996\n",
      "1817. butter: 11.233096795195761\n",
      "1818. jacket: 11.219410548800326\n",
      "1819. one day: 11.213935255349291\n",
      "1820. tech: 11.2098748757987\n",
      "1821. love color: 11.187435124327259\n",
      "1822. christmas gift: 11.186012097071071\n",
      "1823. better quality: 11.17459642902973\n",
      "1824. rid: 11.172793102646223\n",
      "1825. glove: 11.165458653929466\n",
      "1826. pricey: 11.157263267451437\n",
      "1827. fit well: 11.155157461266038\n",
      "1828. repair: 11.135834196734253\n",
      "1829. dishwasher: 11.128601546486411\n",
      "1830. focus: 11.12179628099604\n",
      "1831. usage: 11.121203214521733\n",
      "1832. durability: 11.110399365794006\n",
      "1833. concept: 11.109527656432377\n",
      "1834. improved: 11.103271562362961\n",
      "1835. moves: 11.10132172830814\n",
      "1836. protected: 11.096858470721115\n",
      "1837. college: 11.092939311285143\n",
      "1838. rip: 11.091877998943996\n",
      "1839. finding: 11.085195377409262\n",
      "1840. knowing: 11.080487407013496\n",
      "1841. scale: 11.076802735353589\n",
      "1842. details: 11.075489569418522\n",
      "1843. moment: 11.075221539184234\n",
      "1844. would say: 11.050702643571062\n",
      "1845. sink: 11.047725732372715\n",
      "1846. wanting: 11.042118809762316\n",
      "1847. hiking: 11.029096781599417\n",
      "1848. wires: 10.998237542495412\n",
      "1849. images: 10.98797018020272\n",
      "1850. carpet: 10.97344461021694\n",
      "1851. fault: 10.958183553840662\n",
      "1852. aside: 10.956281690341497\n",
      "1853. bend: 10.954283282175554\n",
      "1854. admit: 10.953531740814372\n",
      "1855. drinking: 10.953151445828919\n",
      "1856. brilliant: 10.947204935828685\n",
      "1857. separate: 10.94125424902795\n",
      "1858. stones: 10.939099672606421\n",
      "1859. hooks: 10.938353888280735\n",
      "1860. plot: 10.93808978979345\n",
      "1861. really works: 10.929628038239516\n",
      "1862. one one: 10.923003896811425\n",
      "1863. helping: 10.90927147081679\n",
      "1864. fixed: 10.900528747036097\n",
      "1865. coat: 10.892959618792375\n",
      "1866. filled: 10.891172505598306\n",
      "1867. corners: 10.885108779933583\n",
      "1868. ship: 10.881643556295971\n",
      "1869. started using: 10.87981736763125\n",
      "1870. cons: 10.877343053119052\n",
      "1871. stories: 10.87664859677013\n",
      "1872. 200: 10.869634977277274\n",
      "1873. netflix: 10.868412479197298\n",
      "1874. files: 10.860331228648153\n",
      "1875. therefore: 10.85932158337256\n",
      "1876. six: 10.854134329600896\n",
      "1877. pros: 10.8475484599728\n",
      "1878. casual: 10.830425352073688\n",
      "1879. clarity: 10.827721878758004\n",
      "1880. trick: 10.81048557518307\n",
      "1881. aluminum: 10.805349754826091\n",
      "1882. brand new: 10.801802399099483\n",
      "1883. beauty: 10.790071589213923\n",
      "1884. crystal: 10.789102038661264\n",
      "1885. strongly: 10.786725236018144\n",
      "1886. two weeks: 10.782049680690832\n",
      "1887. lack: 10.781613048154284\n",
      "1888. works really: 10.780748682028465\n",
      "1889. includes: 10.775279710757422\n",
      "1890. would love: 10.771233768637513\n",
      "1891. lifetime: 10.764879995901364\n",
      "1892. kids love: 10.76367453965797\n",
      "1893. narrow: 10.756784702028563\n",
      "1894. current: 10.75632327178709\n",
      "1895. plain: 10.75542268842037\n",
      "1896. zoom: 10.75358092337151\n",
      "1897. pouch: 10.749654578037456\n",
      "1898. smoothly: 10.746158475182039\n",
      "1899. outfit: 10.745961552860702\n",
      "1900. begin: 10.725145209940841\n",
      "1901. www amazon: 10.71671055918403\n",
      "1902. active: 10.707472224545624\n",
      "1903. thus: 10.70446824153118\n",
      "1904. require: 10.703462373615265\n",
      "1905. roku: 10.701861183256295\n",
      "1906. firm: 10.699342662653446\n",
      "1907. penny: 10.695964712349618\n",
      "1908. bits: 10.695271098151832\n",
      "1909. recommend friends: 10.67747853572514\n",
      "1910. one side: 10.673546771830516\n",
      "1911. http www amazon: 10.665891928627902\n",
      "1912. closed: 10.651960357212735\n",
      "1913. value money: 10.642885269037947\n",
      "1914. american: 10.637282585966549\n",
      "1915. fancy: 10.627740930331885\n",
      "1916. also great: 10.625157002880751\n",
      "1917. look nice: 10.615832677057583\n",
      "1918. deck: 10.611925544357096\n",
      "1919. protects: 10.597995887539165\n",
      "1920. motion: 10.59389835462846\n",
      "1921. plants: 10.590720350164208\n",
      "1922. animals: 10.588256824617867\n",
      "1923. easy put together: 10.583222887510951\n",
      "1924. detailed: 10.575903390137935\n",
      "1925. snow: 10.563292846366853\n",
      "1926. spending: 10.559430480159866\n",
      "1927. worth every: 10.55838756021417\n",
      "1928. besides: 10.556622939972527\n",
      "1929. gp: 10.555088745245232\n",
      "1930. cd: 10.550552812806528\n",
      "1931. pure: 10.548745104294865\n",
      "1932. particularly: 10.545264151432917\n",
      "1933. mounting: 10.544937812436086\n",
      "1934. contact: 10.54471489017742\n",
      "1935. brings: 10.54389909981987\n",
      "1936. meat: 10.537348406134413\n",
      "1937. cracked: 10.536176149887693\n",
      "1938. previously: 10.531985075771816\n",
      "1939. great gift: 10.53015071785555\n",
      "1940. give try: 10.527920366776026\n",
      "1941. br like: 10.526597851301295\n",
      "1942. pretty well: 10.52260718441932\n",
      "1943. instruction: 10.515390857176346\n",
      "1944. stitching: 10.498456862950936\n",
      "1945. stone: 10.497696915235792\n",
      "1946. chose: 10.495088000357187\n",
      "1947. points: 10.49247355633087\n",
      "1948. starts: 10.490578633310022\n",
      "1949. flow: 10.490335007858597\n",
      "1950. second one: 10.48539154012715\n",
      "1951. media: 10.47289378639517\n",
      "1952. 90: 10.467894430878077\n",
      "1953. everything need: 10.465451012885897\n",
      "1954. tie: 10.464532014400783\n",
      "1955. something like: 10.462347422808223\n",
      "1956. streaming: 10.456443911206211\n",
      "1957. galaxy: 10.456279099675296\n",
      "1958. pills: 10.453458772328945\n",
      "1959. year old son: 10.45267727625284\n",
      "1960. colorful: 10.450355685699266\n",
      "1961. web: 10.436203743068745\n",
      "1962. stated: 10.434069754011091\n",
      "1963. vitamin: 10.433592202679181\n",
      "1964. prime: 10.430175635147359\n",
      "1965. marks: 10.429465626096407\n",
      "1966. gp product: 10.426585149906096\n",
      "1967. interest: 10.4252136084409\n",
      "1968. boring: 10.4208240448294\n",
      "1969. super easy: 10.416795821314349\n",
      "1970. use product: 10.410133317075818\n",
      "1971. met: 10.403819762237411\n",
      "1972. doll: 10.403497679036299\n",
      "1973. prices: 10.391369628894612\n",
      "1974. product works: 10.375604154400735\n",
      "1975. relatively: 10.37064597213415\n",
      "1976. crap: 10.370108493775772\n",
      "1977. wise: 10.365553173316115\n",
      "1978. followed: 10.36370370786129\n",
      "1979. max: 10.360727302222621\n",
      "1980. sits: 10.35889328652269\n",
      "1981. functionality: 10.358287549773921\n",
      "1982. plate: 10.355740369610233\n",
      "1983. listening: 10.352772197247353\n",
      "1984. also love: 10.348071587670194\n",
      "1985. mark: 10.343284055744567\n",
      "1986. next time: 10.342705839571739\n",
      "1987. realized: 10.341754388391314\n",
      "1988. channel: 10.341412530288485\n",
      "1989. copy: 10.337072807409815\n",
      "1990. initially: 10.335559643383851\n",
      "1991. ahead: 10.320984058050282\n",
      "1992. shock: 10.315029726349778\n",
      "1993. rear: 10.305801380316879\n",
      "1994. saves: 10.305503194829472\n",
      "1995. kick: 10.294791778515878\n",
      "1996. 2nd: 10.29423300284211\n",
      "1997. seats: 10.270621946415838\n",
      "1998. far best: 10.269754971393516\n",
      "1999. growing: 10.24311035199401\n",
      "2000. agree: 10.240507324143126\n"
     ]
    }
   ],
   "source": [
    "#feature 1 of the dffnn model, top 2000 n grams according to their tfidf weights\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokenized reviews back to strings\n",
    "cleaned_reviews = [\" \".join(review) for review in content_review_tokens]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "# Step 1: Tokenize into unigrams, bigrams, and trigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_reviews)\n",
    "\n",
    "\n",
    "# Step 2: Calculate tf-idf weights for each n-gram\n",
    "tfidf_weights = tfidf_matrix.toarray()  # Convert the sparse matrix to dense array\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()  # Get the feature names (n-grams)\n",
    "\n",
    "# Now you can access the TF-IDF weights for each n-gram\n",
    "# for i, review in enumerate(cleaned_reviews):\n",
    "#     print(f\"TF-IDF weights for review {i+1}:\")\n",
    "#     for j, feature in enumerate(feature_names):\n",
    "#         weight = tfidf_weights[i][j]\n",
    "#         if weight > 0:  # To print only non-zero weights\n",
    "#             print(f\"{feature}: {weight}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "\n",
    "# Step 3: Select top 2000 n-grams based on their tf-idf scores\n",
    "total_tfidf_scores = np.sum(tfidf_weights, axis=0)  # Sum TF-IDF scores across all documents\n",
    "top_indices = total_tfidf_scores.argsort()[-2000:][::-1]  # Get indices of top 2000 scores in descending order\n",
    "\n",
    "# Get the top 2000 n-grams and their corresponding TF-IDF scores\n",
    "top_ngrams = [feature_names[i] for i in top_indices]\n",
    "top_tfidf_scores = [total_tfidf_scores[i] for i in top_indices]\n",
    "\n",
    "# Print the top 2000 n-grams and their TF-IDF scores\n",
    "for i, (ngram, score) in enumerate(zip(top_ngrams, top_tfidf_scores), start=1):\n",
    "    print(f\"{i}. {ngram}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "127a13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix creation for 21000 reviews for top 2000 ngrams\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def review_to_vector(review, top_ngrams, top_tfidf_scores):\n",
    "    review_vector = np.zeros(len(top_ngrams))  # Initialize a vector for the review\n",
    "\n",
    "    for ngram, weight in zip(top_ngrams, top_tfidf_scores):\n",
    "        if ngram in review:  # Check if the n-gram is present in the review\n",
    "            index = top_ngrams.index(ngram)  # Get the index of the n-gram in the top_ngrams list\n",
    "            review_vector[index] = weight  # Assign the TF-IDF weight to the corresponding index in the review vector\n",
    "\n",
    "    return review_vector\n",
    "\n",
    "# Example: Convert each review to a vector representation\n",
    "review_vectors = []\n",
    "for review in cleaned_reviews:\n",
    "    review_vector = review_to_vector(review, top_ngrams, top_tfidf_scores)\n",
    "    review_vectors.append(review_vector)\n",
    "\n",
    "# Convert the list of review vectors to a numpy array\n",
    "X = np.array(review_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb3f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing Dimensionality from 2000 to 200 per review\n",
    "\n",
    "#df['target'] = pd.factorize(df['LABEL'])[0]\n",
    "y = df['TARGET']\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Step 1: Correlation-based Feature Selection (CBFS)\n",
    "def correlation_based_feature_selection(X, y, k):\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    return X_selected\n",
    "\n",
    "# Step 2: Latent Semantic Analysis (LSA)\n",
    "def latent_semantic_analysis(X, n_components):\n",
    "    lsa = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    return X_lsa\n",
    "\n",
    "# Feature selection and dimensionality reduction for ngrams\n",
    "X_ngrams_selected = correlation_based_feature_selection(X, y, 500)\n",
    "X_ngrams_lsa = latent_semantic_analysis(X_ngrams_selected, 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "088bb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 200)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngrams_lsa.shape\n",
    "#print(type(X_ngrams_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c892979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.73868151e+01  1.02768647e+02  7.26610315e+01  3.03809716e+02\n",
      " -8.27915142e+00  4.66456015e+01  2.66001320e+01 -4.86280742e+01\n",
      "  7.44254124e+00 -1.97831231e+01  1.55038016e+01  2.38028925e+01\n",
      "  2.64646972e+00 -2.47231922e+01  1.52000920e+01  5.19867970e+01\n",
      " -4.51044493e+01 -1.09908445e+02  4.22393243e+01  1.38180121e+01\n",
      " -1.48242113e+01  3.87889349e+00 -1.54361337e+00 -2.17958403e+00\n",
      "  8.38948383e+00 -2.32563276e+00 -1.37217838e+01  2.30804660e+00\n",
      " -2.08863990e+00 -5.34636382e+00  3.80086828e+00  3.32903230e-01\n",
      "  2.12670472e+00 -7.02054175e-01 -1.23659241e+01  5.00477979e+00\n",
      "  1.28588303e+00  5.14642744e+00 -1.62744446e+01  3.97170481e+01\n",
      " -1.18965627e+00  3.78013943e+01  9.05320202e+01  3.77076532e+01\n",
      " -1.16015049e+01  1.87516440e+00  4.68109218e+01  7.55332573e+01\n",
      "  4.51990124e+00  2.00977422e+01 -3.00485867e+01 -9.00303398e+00\n",
      " -2.15325590e+01 -4.50268879e+00 -3.01781108e+00 -1.65283608e+01\n",
      " -1.35584343e+00 -9.03690852e+00  2.74770153e+00  3.69696636e+00\n",
      " -3.87429242e+00 -1.51951608e+00 -7.13139022e+00 -3.42499361e+00\n",
      " -2.47681812e+00 -1.12956125e+01 -6.20990499e+00 -1.11305734e+01\n",
      "  4.09897173e+00  2.60306229e+00 -2.18520436e+01 -2.97200272e+00\n",
      "  1.27995593e+01 -4.47687602e+00 -9.03247220e+00 -2.40677426e+00\n",
      "  2.36788722e+00 -2.50964614e+00  9.17383656e-01 -2.32613967e+00\n",
      " -3.18871536e+00  1.83476534e+00  1.49416557e+00  3.37314264e-01\n",
      " -2.03648194e+00  1.88528160e-01 -8.71002733e-01  2.63410808e+00\n",
      " -4.14011248e-02 -1.27416700e+00 -4.40343348e-01 -1.47589773e+00\n",
      "  1.10400936e+00 -1.35018182e+00  1.07580608e+00  7.46274715e-01\n",
      "  1.10141274e+00  1.07621216e+00  1.95304394e+00 -6.02793807e-01\n",
      " -1.42697899e+00 -3.64241291e-02  8.49929827e-01 -4.30167198e-01\n",
      " -3.03752482e+00 -2.42482058e-01 -4.02861496e-01  8.16019688e-01\n",
      " -6.74373141e-01 -1.77497578e-01 -9.26610464e-01 -1.27392255e+00\n",
      "  9.70393185e-01 -1.28256203e+00 -9.94344725e-01  1.43271419e+00\n",
      "  1.45614894e+00  8.20570325e-01  2.02535348e-01  1.53291161e-01\n",
      "  3.40700366e+00 -4.28434286e-01 -1.48826107e+00 -1.74863577e+00\n",
      "  3.50673421e-05  1.43414760e+00  3.28314647e+00  8.32676752e-02\n",
      " -1.02975225e+00 -4.59341667e-01 -1.10775543e+00  8.95711620e-01\n",
      "  3.64974033e-01 -5.04886234e-01  1.11016495e+00  3.15454124e+00\n",
      " -4.58897391e+00  4.52560705e+00  8.95966980e-01  2.04290975e-01\n",
      " -8.14559763e+00 -7.69766371e+00 -5.84912807e+00  4.59779959e+00\n",
      " -4.22407456e-01  5.57287262e+00 -6.20703991e+00 -2.28489152e+00\n",
      "  1.53489436e+01 -4.97320966e+00  5.83549326e+00  1.54585788e+01\n",
      "  1.37950055e+00  2.39384611e+01 -2.02712020e+01  9.08697750e+00\n",
      " -4.97790973e+00 -2.13818251e-01  8.50043394e+00  7.86139309e+00\n",
      "  2.75839468e+00 -4.43934396e+00 -1.93600367e+00 -1.79566404e+00\n",
      " -1.96828668e+00 -2.54198918e+00  1.25282211e+00 -1.07826006e-01\n",
      " -1.58510605e+00  1.60469516e+00  1.32626502e-01 -5.66515812e-01\n",
      " -1.43355311e+00 -1.19796614e-01  1.69700995e+00  7.13025805e-01\n",
      "  1.67847030e+00  2.82742711e-01 -7.86587780e-03  2.12628545e+00\n",
      "  2.07601909e+00  2.76078415e+00  8.25025143e-01  1.81969624e+00\n",
      " -1.18027728e+00  2.62689866e-01 -9.92889367e-03 -1.02594118e+00\n",
      "  2.68619359e-01 -2.57399954e+00  8.15901929e-01 -2.03651668e-01\n",
      " -1.67983033e+00 -1.24996425e+00 -6.24853460e-01  2.46305422e-01\n",
      " -8.93041096e-01 -9.22636141e-03 -1.26037819e+00 -2.12477277e-01]\n"
     ]
    }
   ],
   "source": [
    "print(X_ngrams_lsa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "P0XO5LqymuoJ",
   "metadata": {
    "id": "P0XO5LqymuoJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from gensim.models import Word2Vec\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# # Train the Skip-Gram model\n",
    "# vector_size = 100  # Dimensionality of word embeddings\n",
    "# window_size = 5    # Maximum distance between the current and predicted word within a sentence\n",
    "# min_count = 1      # Minimum frequency count of words to consider when training the model\n",
    "# workers = 4        # Number of threads to use while training\n",
    "\n",
    "# # Train the Skip-Gram model\n",
    "# skipgram_model = Word2Vec(sentences=content_review_tokens,\n",
    "#                           vector_size=vector_size,\n",
    "#                           window=window_size,\n",
    "#                           min_count=min_count,\n",
    "#                           workers=workers,\n",
    "#                           sg=1)  # sg=1 specifies Skip-Gram model\n",
    "\n",
    "# # Save the trained word embeddings\n",
    "# # skipgram_model.save('skipgram_word_embeddings.model')\n",
    "\n",
    "# #skipgram_model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d41a01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the Skip-Gram model is trained and saved as 'skipgram_word_embeddings.model'\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming your preprocessed tokens are in 'content_review_tokens'\n",
    "skipgram_model = Word2Vec(sentences=content_review_tokens,\n",
    "                          vector_size=100,\n",
    "                          window=5,\n",
    "                          min_count=1,\n",
    "                          sg=1,\n",
    "                          hs=1)  # Set hs=1 for hierarchical softmax (optional)\n",
    "#skipgram_model.save('skipgram_word_embeddings.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6b0c0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1791616   0.0524897  -0.05231478 -0.10162009 -0.04017087 -0.56161845\n",
      "  0.15999307  0.2609156  -0.35660228 -0.13327965 -0.03976102 -0.32671579\n",
      "  0.16690384  0.01003078  0.00340737 -0.05915942  0.01489905 -0.05257911\n",
      " -0.13975781 -0.35406293  0.25526112  0.10253918  0.18216395 -0.14401178\n",
      " -0.14487525  0.21098809  0.05421619  0.03644725  0.19357607 -0.05955293\n",
      "  0.18420431 -0.00749496 -0.10461058  0.20404501 -0.12647992  0.32785753\n",
      "  0.09219551 -0.27940511  0.01703775 -0.08704697  0.06907846 -0.09821813\n",
      " -0.01793839 -0.20797611  0.33020155  0.08121419 -0.05311894  0.01601348\n",
      "  0.05914304 -0.12217449  0.21691705 -0.10393532 -0.15969188 -0.05581789\n",
      " -0.32458127  0.02629361  0.19610698  0.03211286  0.06557599 -0.07393019\n",
      "  0.0892282  -0.14572393 -0.17350443  0.00917811 -0.30878576  0.23749294\n",
      "  0.15808294  0.2100823  -0.32689123  0.07468973 -0.16534622 -0.19946373\n",
      "  0.26375073 -0.21035649 -0.01392641  0.1810166   0.18161253  0.12565595\n",
      " -0.28371659 -0.0355084   0.01597047 -0.06202562 -0.33585126  0.3528524\n",
      "  0.00557474  0.11280663 -0.11857484 -0.0227786   0.0019892  -0.03719734\n",
      " -0.13583585 -0.02549884 -0.19697137 -0.06809483  0.40237431  0.25994354\n",
      "  0.40275299 -0.10691525  0.27306578 -0.03663217]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate average word embedding for a review\n",
    "def average_word_embedding(review, skipgram_model):\n",
    "  num_words = 0  # Count of valid words in the review\n",
    "\n",
    "  review_vec = np.zeros(skipgram_model.vector_size)  # Zero vector to accumulate word embeddings\n",
    "\n",
    "  for token in review:\n",
    "    try:\n",
    "      # Get the word embedding vector from the model\n",
    "      word_vec = skipgram_model.wv[token]\n",
    "      review_vec += word_vec  # Add the word vector to the accumulator\n",
    "      num_words += 1\n",
    "    except KeyError:\n",
    "      # Skip words not found in the vocabulary (might be rare or out-of-vocabulary words)\n",
    "      pass\n",
    "\n",
    "# If no valid words were found, return zero vector\n",
    "  if num_words == 0:\n",
    "    return review_vec\n",
    "  else:\n",
    "    # Calculate the average word embedding\n",
    "    return review_vec / num_words\n",
    "\n",
    "# Example usage: Calculate average embedding for a sample review\n",
    "sample_review = content_review_tokens[0]  # Assuming content_review_tokens contains tokenized reviews\n",
    "average_embedding = average_word_embedding(sample_review, skipgram_model)\n",
    "print(average_embedding)\n",
    "print(len(average_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "862f3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n"
     ]
    }
   ],
   "source": [
    "avg_embedding_reviews = []\n",
    "for i in range(0,len(content_review_tokens)):\n",
    "  sample_review = content_review_tokens[i]\n",
    "  average_embedding = average_word_embedding(sample_review, skipgram_model)\n",
    "  avg_embedding_reviews.append(average_embedding)\n",
    "print(len(avg_embedding_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3f3bd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns\n",
    "\n",
    "emotion_df = df[['VERIFIED_PURCHASE', 'OPI_FIN_POS', 'OPI_FIN_NEG', 'BL_POS', 'BL_NEG', 'AFINN_POS',\n",
    "       'AFINN_NEG', 'S140_POS', 'S140_NEG', 'SWN_POS', 'SWN_NEG',\n",
    "       'NRC_HASH_POS', 'NRC_HASH_NEG', 'EMOTICON_POS', 'EMOTICON_NEG',\n",
    "       'NRC_ANGER', 'NRC_ANTICIPATION', 'NRC_DISGUST', 'NRC_FEAR', 'NRC_JOY',\n",
    "       'NRC_SADNESS', 'NRC_SURPRISE', 'NRC_TRUST', 'NRC_EXP_ANGER',\n",
    "       'NRC_EXP_ANTICIPATION', 'NRC_EXP_DISGUST', 'NRC_EXP_FEAR',\n",
    "       'NRC_EXP_JOY', 'NRC_EXP_SADNESS', 'NRC_EXP_SURPRISE', 'NRC_EXP_TRUST']]\n",
    "\n",
    "emotion_X = emotion_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fbb0e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n",
      "331\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for i in range(21000):\n",
    "    X.append(list(X_ngrams_lsa[i]) + avg_embedding_reviews[i].tolist() + emotion_X[i])\n",
    "    \n",
    "print(len(X))\n",
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e199c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(21000, 331)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f13363e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16800, 331) (4200, 331) (16800,) (4200,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate((X[:8400], X[10500:18900]))\n",
    "X_test = np.concatenate((X[8400:10500], X[18900:]))\n",
    "y_train = np.concatenate((df['TARGET'].values[:8400], df['TARGET'].values[10500:18900]))\n",
    "y_test = np.concatenate((df['TARGET'].values[8400:10500], df['TARGET'].values[18900:]))\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aa8f06e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [2100 2100]\n"
     ]
    }
   ],
   "source": [
    "# unique_values, counts = np.unique(y_test, return_counts=True)\n",
    "# print(unique_values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "466ec3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16800, 331) (4200, 331) (16800,) (4200,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# X_train = X[:16800]\n",
    "# X_test = X[16800:]\n",
    "# y_train = df['TARGET'].values[:16800]\n",
    "# y_test = df['TARGET'].values[16800:]\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vLMVB3huWNQp",
   "metadata": {
    "id": "vLMVB3huWNQp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Model Architecture Design:\n",
    "\n",
    "# DFFNN Model:\n",
    "# Design a multilayer perceptron neural network with two hidden layers.\n",
    "# Determine the input layer size based on the features extracted in data preprocessing (e.g., 2000 n-grams, 30 emotion features, and word embeddings).\n",
    "# Define the number of neurons in each hidden layer based on a grid search procedure.\n",
    "# Choose rectified linear units as the activation function for the hidden layers.\n",
    "# Implement dropout regularization to prevent overfitting.\n",
    "# Utilize softmax activation in the output layer for binary classification (fake/truthful).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20a698",
   "metadata": {},
   "source": [
    "# DFFNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "xRQWjt37WP5T",
   "metadata": {
    "id": "xRQWjt37WP5T",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 100)               33200     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40851 (159.57 KB)\n",
      "Trainable params: 40851 (159.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DFFNN Model:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming you have the features extracted in data preprocessing stored in X\n",
    "# and the binary labels in y (0 for fake, 1 for truthful)\n",
    "\n",
    "# Input layer size based on features\n",
    "input_layer_size = len(X[0])  # Adjust based on the actual number of features\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_layer1_neurons = 100\n",
    "hidden_layer2_neurons = 50\n",
    "dropout_rate = 0.5  # Adjust as needed\n",
    "\n",
    "# Define the DFFNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(hidden_layer1_neurons, input_dim=input_layer_size, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(hidden_layer2_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(hidden_layer2_neurons, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer (binary classification with softmax activation)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "48ba1903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "525/525 [==============================] - 1s 2ms/step - loss: 0.6949 - accuracy: 0.5035\n",
      "Epoch 2/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.4990\n",
      "Epoch 3/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6953 - accuracy: 0.4958\n",
      "Epoch 4/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.5024\n",
      "Epoch 5/50\n",
      "525/525 [==============================] - 1s 2ms/step - loss: 0.6957 - accuracy: 0.4979\n",
      "Epoch 6/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.5015\n",
      "Epoch 7/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6947 - accuracy: 0.4955\n",
      "Epoch 8/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6960 - accuracy: 0.4961\n",
      "Epoch 9/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6960 - accuracy: 0.4996\n",
      "Epoch 10/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.5019\n",
      "Epoch 11/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6958 - accuracy: 0.4956\n",
      "Epoch 12/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.4994\n",
      "Epoch 13/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6948 - accuracy: 0.4981\n",
      "Epoch 14/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.4998\n",
      "Epoch 15/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.5027\n",
      "Epoch 16/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6956 - accuracy: 0.5036\n",
      "Epoch 17/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.5020\n",
      "Epoch 18/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.4957\n",
      "Epoch 19/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6955 - accuracy: 0.5018\n",
      "Epoch 20/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6955 - accuracy: 0.4964\n",
      "Epoch 21/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.4993\n",
      "Epoch 22/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6954 - accuracy: 0.5026\n",
      "Epoch 23/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6955 - accuracy: 0.4967\n",
      "Epoch 24/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6954 - accuracy: 0.5010\n",
      "Epoch 25/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.4955\n",
      "Epoch 26/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.4999\n",
      "Epoch 27/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.4951\n",
      "Epoch 28/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.4964\n",
      "Epoch 29/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6948 - accuracy: 0.5026\n",
      "Epoch 30/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.4992\n",
      "Epoch 31/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6952 - accuracy: 0.4968\n",
      "Epoch 32/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6949 - accuracy: 0.5065\n",
      "Epoch 33/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6944 - accuracy: 0.5067\n",
      "Epoch 34/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.4985\n",
      "Epoch 35/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6950 - accuracy: 0.4990\n",
      "Epoch 36/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6952 - accuracy: 0.4957\n",
      "Epoch 37/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6955 - accuracy: 0.4946\n",
      "Epoch 38/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6953 - accuracy: 0.4993\n",
      "Epoch 39/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6961 - accuracy: 0.4995\n",
      "Epoch 40/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.5085\n",
      "Epoch 41/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6968 - accuracy: 0.4943\n",
      "Epoch 42/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6946 - accuracy: 0.5027\n",
      "Epoch 43/50\n",
      "525/525 [==============================] - 1s 2ms/step - loss: 0.6953 - accuracy: 0.5023\n",
      "Epoch 44/50\n",
      "525/525 [==============================] - 1s 2ms/step - loss: 0.6957 - accuracy: 0.5001\n",
      "Epoch 45/50\n",
      "525/525 [==============================] - 1s 2ms/step - loss: 0.6948 - accuracy: 0.5054\n",
      "Epoch 46/50\n",
      "525/525 [==============================] - 1s 2ms/step - loss: 0.6952 - accuracy: 0.4985\n",
      "Epoch 47/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6965 - accuracy: 0.4910\n",
      "Epoch 48/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.5020\n",
      "Epoch 49/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6952 - accuracy: 0.5026\n",
      "Epoch 50/50\n",
      "525/525 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.4938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c2850badc0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "85142cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 0s 891us/step\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "#y_pred = (int(y_pred>0.25)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] > 0.4:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "        \n",
    "        \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8e5656c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred.shape\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(y_test.tolist().count(1))\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "# unique_values, counts = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "# print(X_est[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04a6ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the current CUDA Device:  NVIDIA GeForce MX450\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# X = torch.tensor(X)\n",
    "\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(\"Name of the current CUDA Device: \", torch.cuda.get_device_name(cuda_id))\n",
    "print(cuda_id)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae1cd1",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2UhxrWTaWS5L",
   "metadata": {
    "id": "2UhxrWTaWS5L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# CNN Model:\n",
    "# Design a convolutional neural network architecture.\n",
    "# Convert each sentence into a k-dimensional word representation using pre-trained word embeddings.\n",
    "# Concatenate word representations to obtain fixed-size input.\n",
    "# Define the number of filters in the convolutional layer and the size of the filter.\n",
    "# Utilize rectified linear units as the activation function for the convolutional layer.\n",
    "# Implement max pooling to downsample the feature maps.\n",
    "# Use softmax activation in the output layer for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LR1mmAZvWnUE",
   "metadata": {
    "id": "LR1mmAZvWnUE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100  # Dimensionality of word embeddings\n",
    "max_len = 100  # Maximum sequence length (number of words in a review)\n",
    "num_filters = 128  # Number of filters in the convolutional layer\n",
    "filter_size = 5  # Size of the filter window\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "vocab_size=1000\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_shape=(max_len,)))\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu'))\n",
    "\n",
    "# Max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Global max pooling layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GWKarpEAKPZh",
   "metadata": {
    "id": "GWKarpEAKPZh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. Model Training:\n",
    "# Split the dataset into training, validation, and testing sets.\n",
    "# Use mini-batch gradient descent for training the DFFNN model.\n",
    "# Apply stochastic gradient descent for training the CNN model.\n",
    "# Tune hyperparameters such as learning rate, dropout rate, and number of iterations using validation set performance.\n",
    "# Monitor training progress and adjust hyperparameters as needed to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_yzlSZLFKSiV",
   "metadata": {
    "id": "_yzlSZLFKSiV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I'm trying idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frIkzvgAKkLt",
   "metadata": {
    "id": "frIkzvgAKkLt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Evaluation:\n",
    "# Evaluate the trained models on the test set to measure their performance.\n",
    "# Compute metrics such as accuracy, precision, recall, and F1-score to assess the models' effectiveness in detecting fake reviews.\n",
    "# Compare the performance of the DFFNN and CNN models with baseline methods and state-of-the-art approaches mentioned in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ui2dnQRUKmuI",
   "metadata": {
    "id": "Ui2dnQRUKmuI",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc91d9",
   "metadata": {
    "id": "7dfc91d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. Optimization and Fine-tuning:\n",
    "# Experiment with different model architectures, hyperparameters, and training strategies to improve performance.\n",
    "# Consider techniques such as ensemble learning or transfer learning to further enhance model accuracy.\n",
    "# Fine-tune the models based on insights gained from initial evaluations and analyses.\n",
    "# By following these steps, you can implement the proposed DFFNN and CNN models for fake review detection based on the ideas presented in the paper. Remember to document your process thoroughly and validate your results to ensure the reliability and reproducibility of your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KVgNU8UBWIcq",
   "metadata": {
    "id": "KVgNU8UBWIcq",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WhfKRIng2i7Z",
   "metadata": {
    "id": "WhfKRIng2i7Z",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
